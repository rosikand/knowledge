[
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Stanford Notes",
    "section": "",
    "text": "Disclaimer: for purposes of efficiency, lots of content in these notes were directly taken from or adapted from the course content/documents (e.g., lecture slides). Specifically, most of the images and lots of text content is generally directly adapted from the course content.\n\n\nAutumn 2022\n\nCS 330: Deep Multi-Task and Meta Learning\nCS 221: Artificial Intelligence: Principles and Techniques\nINTLPOL 268: Hack Lab"
  },
  {
    "objectID": "concepts.html",
    "href": "concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Lists\n\nBasic Math\nBiology"
  },
  {
    "objectID": "machine-learning.html",
    "href": "machine-learning.html",
    "title": "Machine Learning Notes",
    "section": "",
    "text": "Lists\n\nPapers"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Knowledge",
    "section": "",
    "text": "Welcome!"
  },
  {
    "objectID": "notes/basic-math/second-derivative-test.html",
    "href": "notes/basic-math/second-derivative-test.html",
    "title": "Knowledge",
    "section": "",
    "text": "Footnotes\n\n\nhttps://mathstat.slu.edu/~may/ExcelCalculus/sec-4-5-SecondDerivativeConcavity.html↩︎"
  },
  {
    "objectID": "notes/papers/protein-diffusion.html",
    "href": "notes/papers/protein-diffusion.html",
    "title": "Knowledge",
    "section": "",
    "text": "Paper that applies denoising diffusion models for protein structure generation.\nSo I guess they aren’t quite “predicting” the protein structure… just merely generating it."
  },
  {
    "objectID": "notes/papers/spelke-segmentation.html",
    "href": "notes/papers/spelke-segmentation.html",
    "title": "Knowledge",
    "section": "",
    "text": "link\n\nIntroduces fully-unsupervised method for segmenting objects in an image.\nMethod inspired by Spelke objects from cognitive science.\nFrom the NeuroAI lab at Stanford lead by Dr. Yamins."
  },
  {
    "objectID": "notes/cs109/12-inference-part-1.html",
    "href": "notes/cs109/12-inference-part-1.html",
    "title": "Knowledge",
    "section": "",
    "text": "We can then apply Bayes’ theorem to a multinomial. See Federalist papers example.\nWhat you need to know for this unit:\n\n\n\n|500\n\n\n\n\n\nDefinition (inference): An updated belief about a random variable (or multiple) based on conditional knowledge regarding another random variable (or multiple) in a probabilistic model. TLDR: conditional probability with random variables.\n\nThis makes sense. We want to use that probabilistic graphical model to infer different things regarding conditionalities.\n\nExample: say you have a joint table with dorm type on one axis and year in uni on the other axis. Conditional probability would be like “what is the probability that you have a single given that you are a senior”. It is important to note the difference between joint and conditional probability. One is \\(P(X=x, Y=x)\\) and the other is \\(P(X=x | Y=y)\\)… very different equations! In the last example, joint would be “what is the probability that you have a single and that you are a senior”. Marginal is then a random variable (not a probability!): \\(P(X=x \\mid Y=2)\\).\n\n\n\n\nAt its heart, conditional probability with multiple random variables heavily relies on Bayes’ theorem (because we’d often be given or be able to derive the reverse conditional). There are many Bayes’ variants when it comes to working with two random variables (because some RVs are continious whereas others are discrete). Remember that Bayes’ theorem and probabiliteis in general works with events (i.e. the RV taking on a value) not just random variables (which are objects) by themselves.\n\n\nPractically the og Bayes’: two discrete random variables taking on a specific value where one depends on the other.\n\nDiscrete/discrete Bayes’: Let \\(M\\) and \\(N\\) be discrete random variables. Then, by Bayes’ theorem: \\[P(M=m \\mid N=n)=\\frac{P(N=n \\mid M=m) P(M=m)}{P(N=n)}.\\] Law of total probability way: \\[P(M=m \\mid N=n)=\\frac{P(N=n \\mid M=m) P(M=m)}{P(N=n \\mid M=m) P(M=m) + P(N=n \\mid M=m^{C}) P(M=m^{C})}.\\]\n\nWe are referring to the probability of \\(M\\) taking on a specific value (event) given that \\(N\\) has already taken on a value. Example:\n\\[P(M=2 \\mid N=3)=\\frac{P(N=3 \\mid M=2) P(M=2)}{P(N=3)}.\\]\nOften, we’ll use the shorthand notation:\n\\[P(m \\mid n)=\\frac{P(n \\mid m) P(m)}{P(n)}.\\]\nBayes’ theorem with discrete RV’s is easy: it is just a table with a bunch of lookups. The real complexity arises when you have a continious random variable in the mix because now you are dealing with densities instead of probabilities. We will explore that in the next lecture."
  },
  {
    "objectID": "notes/cs109/18-central-limit-theorem.html",
    "href": "notes/cs109/18-central-limit-theorem.html",
    "title": "Knowledge",
    "section": "",
    "text": "We now begin study of the “fundamental theorem of probability”… one so important that we dedicate an entire lecture to it. The main premise that the summation of iid random variables forms a normal distribution no matter what the original distribution was for the random variables. Specifically, the sum of many iid random variables is itself a random variable that can take on different values at different probabilities (i.e. it is a random variable). Plotting this specific distribution will look normal. Astounding!\n\nApplication example: say we are given a problem where it involves summing/multiple iterations of something and we are asked the probability of something about that something. This should immediatly scream CLT (when you see a sum of iid random variables) no matter what the individual iid RVs were.\n\nExample: see slide 11 on lecture 18 slides\nIt allows us to answer questions when we are summing up many different random variables.\n\nNote that the CLT is extended to average: that is, the averages (expectation) of many random variables will follow a normal distribution.\n\nOne day I plan to study the proof of this. Because it is quite an amazing result when you think of it.\nNote (Inverse \\(\\phi\\)): often times you’ll be asked to calculate the probability of things–fairly standard. But some times you might be asked to calculate a value for something that equates to a certain probability. This is just an equation where you must rearrange the PMF/PDF around. Specifically, it should be a system of one unknown and one equation. So why discuss this? Well for the CDF of a normal, you’ll be left with the \\(\\phi\\) function. So we need to invert it somehow. This is given to us but I thought I’d make note that this is indeed a possible thing to do.\n\n\nWhen we talk about sampling, what we mean is that we “instantiate” a random variable: we draw a sample from the dsitribution that is based on the probabilities of drawing each value the random variable can take on.\nDifferent types of sampling:\n\n\nData science is the science of making rigouros conclusions from dsitrubtions of samples."
  },
  {
    "objectID": "notes/cs109/23-naïve-bayes.html",
    "href": "notes/cs109/23-naïve-bayes.html",
    "title": "Knowledge",
    "section": "",
    "text": "Yea yea… we have talked about estimating parameters for a parametric probability distribution. But now let’s actually do real ML: making predictions! We will do this in the context of three datasets: - Hearts - Ancestry - Netflix\nAll three problems on the pset where you implement classifiers for all of them using both Logistic regression and Bayes’ nets!\nWe will be working with supervised classification specifically.\n\nClassification:\n\nNote: regression is same thing except numbers aren’t binary!\n\nSo how do we program an algorithm for classification? Our first insight might be to brute force Bayes’."
  },
  {
    "objectID": "notes/cs109/21-maximum-likelihood-estimation.html",
    "href": "notes/cs109/21-maximum-likelihood-estimation.html",
    "title": "Knowledge",
    "section": "",
    "text": "https://www.cs.cmu.edu/~tom/mlbook/\nWe now turn to machine learning and we begin with MLE where we try to estimate the parameters for a parametric distribution based on some observed data. But to understand this, we first need a thorough understanding of what exactly is likelihood and some other probabiltiy basics.\n\n\n\nWe aim to understand the differences between probability and likelihood. We begin with reviewing how we came about random variables and probability. Say that there exists some random process such as coin flipping (e.g. outcomes of tossing a coin 10 times). In such cases, to gain a deeper intuition for the process, we assign a random variable to the process and calculate probabilities for how probable it is that the RV takes on a certain value. In such cases, we can calculate the probability of observing a particular set of outcomes by making suitable assumptions about the underlying stochastic process. For example, we might be given that the coin comes up heads with probability \\(\\theta = p = 0.4\\) per one flip. Thus, we can calculate \\(P(X=x \\mid \\theta)\\) where \\(\\theta\\) is that probability parameter. Now you might be wondering, where do we get \\(\\theta\\) from? Do we just pull it out of our ass (excuse the French)? See, in the real world, we often do not know what the \\(\\theta\\) is and it isn’t just given to us. Instead, what we do have is the ability to flip the coin some \\(n\\) times and from that, we can calculate the \\(\\theta\\). That is, we simply observe the process that \\(X\\) is representing and the goal then is to arrive at an estimate for \\(\\theta\\) that would be a plausible choice given the observed outcomes from \\(X\\). To do this in a structured way, we represent this as a function of \\(\\theta\\) and it is defined as \\(L(\\theta \\mid X)\\): the notation makes sense, we want to know \\(\\theta\\) given the outcomes we observe from \\(X\\). Such a function is called the likelihood.\n\nLet us probe this further: \\(L(\\theta \\mid X)\\) means “how likely are we to see a certain distribution given the obersevations?”. For example, in the normal case, \nHere we ask, how likely is it that the normal distriubtion follows this exact curve given the observed data (that the mouse weighs 34 grams)? So in some ways, we are composing the distribution curve based on the data! How do we compose such a curve? We first make an assumption for the parametric form it fits. Then, we estimate the parameters. This forms the curve. So likelihood, defined as \\(L(\\theta \\mid X)\\), is simply how likely is it that the curve is shaped like this given the observed data. That is the semantic meaning and it is quite different than probability (i.e. doesn’t have to be between 0 and 1). In this lecture, we show how to estimate such parameters so that the curve is most likely given the data. The reasoning for this is that the experiment is a full circle: we can then use the constructed distriubution to make future predictions on probabilities!\nHere is a nice math result: \\(L(\\theta \\mid X)=P(X \\mid \\theta)\\): it turns out that the likelihood of a distribution given the data is the same as the probability density of the outcome given the distribution (see here). First answer here is also good and of course StatQuest. Notice in the StatQuest video that he explains likelihood by shifting around the normal curve until the likelihood is maxed out (i.e. it fits the data observed well).\n\nA note: it is vital to understand the realtion \\(L(\\theta \\mid X)=P(X \\mid \\theta)\\). Specifically, \\(P(X \\mid \\theta)\\) means the probability that we would observe our data \\(X\\) given the parametric distribution \\(\\theta\\). This is the same thing as saying how likely is it that our distribution is constructed with parameters \\(\\theta\\) given the data we observed. See why?\n\n\nNotice how this is a full circle: we have some underlying random process to which we want a whole distribution of probabilities for what the process outcome may be. However, we have no intuition yet so to derive the parameters for the distribution, we need to observe some data in real life. From this, we get the parameters we need to form our probabilistic intuitions.\n\nIn my own words,\n\nLikelihood: the likelihood is defined as how likely is it that the given parametric distribution curve (i.e. the parameters for the assumed parametric distribution family) is the correct probability distribution for the the data that has been observed.\n\nMLE: so MLE is the process of calculating the best curve for the distribution to fit the data: that is, maximizing \\(L(\\theta \\mid X)\\).\n\nAnother mental model for likelihood: say we have some data and a dsitribution curve. Now let us draw a few samples from the distribution. Likelihood measures how likely it is that the observed data matches the drawn samples. i.e. how probable is it that we’d draw samples matching the observed data? This should help you understand \\(L(\\theta \\mid X)=f(X \\mid \\theta)\\).\n\n The probability that we’d observe\n\nI mean this makes sense: what is the probability of observing your data given the parameters for the distribution (this is how likelihood is defined in course textbook: \\(L(\\theta)=\\prod_{i=1}^{n} f\\left(X_{i} \\mid \\theta\\right)\\). We want this probability to be as high as possible so that the resultant curve is fit well to the actual data. If we take a bunch of different values for \\(\\theta\\) (x-axis) and plot the results (y-axis) of the above equation on a graph, we get the likelihood function. We aim to find the maximum value of this function.\n\nOk… actual lecture notes now.\n\n\nBasically, likelihood is defined as taking the probability densities for each data point for a certain \\(\\theta\\) and multiplying them together. Now do this for a bunch of different \\(\\theta\\) values. This is the likelihood function. See 49:00 from lecture.\n\nFrom this, it is easy to see that \\[L(\\theta)=\\prod_{i=1}^{n} f\\left(X_{i} \\mid \\theta\\right).\\]\n\nWe don’t lile products and it gets small so we just use log of likelihood.\n\nOk so we need to maximize LL function. How to do? Gradient ascent!\n\n\nA note: discrete to continious. We have things in terms of densities so we will work with discrete for everything. The good news is that you can fit a function (curve) to a histogram (discrete-valued histogram).\n\\[X \\sim \\operatorname{Ber}(p) \\rightarrow f(X=x \\mid p)=p^{x}(1-p)^{1-x}\\]\n\nA new way to choose parameters… next lecture."
  },
  {
    "objectID": "notes/cs109/15-general-inference.html",
    "href": "notes/cs109/15-general-inference.html",
    "title": "Knowledge",
    "section": "",
    "text": "We started out by specifying the joint distribution which gave us all of the information we would ever need about jointly distributed multiple random variables.\nWe then developed intuition for conditional probability between a whole mixture of random variables (e.g. Bayes’ rules).\nWe then came up with questions we’d like to answer related to conditional probability. We call these questions “inference”.\nHowever, as our system of random variables grew larger, the joint becomes computationally intractable to specify.\nSo instead, we developed probablistic graphical models (Bayesian networks) which is a graphical model of causalities (nodes are RVs and edges imply causality/conditionallity). Because of dependence between the RVs, this greatly reduces the number of calculations we need for general inference questions… to get the probability of a particular node/RV taking on a value, we only need its parent probabilities to get this value (i.e. conditioned on the parents).\nBut… we still don’t know how to exactly get these values from the graphs (i.e. generally infer from the graph). Today, we learn that.\n\n\n\nDon’t know how to solve probability questions from Bayesian networks.\nMany techniques for doing so but main one today that we’ll learn is called “sampling”.\nWe will sample from this hypothetical joint distribution that we are modelling using the Bayesian network to answer general probability questions.\nCovariance: how much they differ from the mean\nCorrelation does not equal causation\nCovariance looks for linear relationship.\n\nSo covariance/correlation doesn’t tell us everything: some RV’s might be nonlinearlly correlated.\n\n\n\nSmall note example: take the following Bayesian network:\n\n\n\n|300\n\n\nNotice that\n\\[\nP\\left(F_{e}=0 \\mid U=1, F_{L}=0\\right) = P\\left(F_{e}=0 \\mid F_{L} = 0 \\right)\n\\]\nbecause fever has no dependence on undergrad. Recall definition of independence: \\(P(E|F) = P(E)\\). This is called the “independence assumption”.\n\n\n\nSo how we actually compute inference questions from Bayesian networks?\nWe can start with \\[\nP\\left(F_{l u}=1 \\mid U=1, T=1\\right)\n\\]\nwhich is chain rule of conditional probability. However, the denominator would require a marganlization of tons of RV’s potentially. Let us come up with a more intuitive method for answering any inference question we may come upon in a systematic way. We call it rejection sampling because it works by sampling and rejecting the faulty samples.\nSteps: - Step 0: Have a fully specified Bayesian network layed out. - Step 1: sample a ton! Remember sampling: given some random variable distribution, if we were to randomly take one value from that distribution, we get a sample (of course the value we get is dependent on the probabilities of that random variable taking on that value: it is inverse). So we essentially do this for all nodes (RVs) in the network. This gives us a joint sample. - Step 2: so how do we compute conditional probability questions? Well, now that we have a bunch of samples, we reject the faulty ones. What do we mean? You can imagine we have a bunch of samples:\n# [Flu, Undergrad, Fever, Tired] \n[0,1,1,1]\n[0,1,0,0]\n[1,0,1,0]\n[0,0,0,0]\n.\n.\n.\n[1,1,1,1]\nsay we want \\(P(Flu = 1 | U = 1)\\). We can loop through all samples, then throw out every sample not consistent with the observation (that is, \\(U=1\\)). So if we loop through all our samples and it doesn’t have a \\(1\\) in the index corresponding to \\(U\\), we throw (reject) it out. This will give you a subset of the samples that are all consistent with the observation. Now loop through this subset and count the number of times the index corresponding to \\(Flu\\) has a \\(1\\). Divide this count by the length of this sample subset (the subset whose elements are samples consistent with the observation).\n\n\n\n|500\n\n\nPseudcode: - Step 0: Have a fully specified Bayesian network layed out. - Step 1: sample a ton! - Step 2: collect subset of samples consistent with observation (thing you are conditioning) - Step 3: loop through this subset and count number of times the thing you want the probability of is present in the sample. - Step 4: Divide this count by the length of this sample subset (the subset whose elements are samples consistent with the observation).\n\n\nAhh… rejection sampling seems to good! However, we only worked with discrete RV’s. Sometimes we might have a continious RV in the mix and rejecting such a sample with a continious RV is a different task than if the numbers jut match. This is because the probability of a CRV taking on an exact value is 0. Thus, no samples will match exactly and you’d reject all of them leading to division by 0.\nSo what is the solution? Another algorithm we can use is called Markov Chain Monte Carlo (MCMC)!\nWelp… we learn that in a different class."
  },
  {
    "objectID": "notes/cs109/19-bootstrapping.html",
    "href": "notes/cs109/19-bootstrapping.html",
    "title": "Knowledge",
    "section": "",
    "text": "Before we start, one note on sampling: when we sample and plot a distribution, the distribution is not a plot of number occurrences for each sample value. No. It is probabilities for each sample value. Take below:\n\n\n\n|500\n\n\nNotice on the y-axis has range of \\([0,1]\\).\nNow let us get into sampling.\n\nSay we want to measure the happiness of people in Bhutan. The most accurate way to do so is to take the entire population and survey them for a score of hapiness. This will give you a complete distribution. But of course, we cannot feasibly do this. So instead, we randomly sample a small subset of the population and get their scores. Example sample: \\([24,98,23,98,42,93,94,91,97,99]\\).\n\nNow we can calculate relevant and wanted statistics about such a sample (this is called data science!) such as mean and variance. The goal is that such calculations would generalize for the entire population. Specifically, we know that the entire population would make up a true distribution if sampled entirely. So our goal here is to estimate such statistics by taking the sample’s statistics instead. We start with sample mean (denoted \\(\\bar{X}\\)) and sample variance (denoted \\(S^2\\)) which are both derived on how these terms are defined for any set of data points. \\[\\bar{X}=\\sum_{i=1}^{n} \\frac{X_{i}}{n}, \\quad S^{2}=\\sum_{i=1}^{n} \\frac{\\left(X_{i}-\\bar{X}\\right)^{2}}{n-1}.\\]\nNote: each element from the sampling set is actually just a random variable. When we draw such samples to make calculations, we of course replace them with numbers. But to work out things mathematically, we first define variables and since these take on different values at different probabilities, we just let them be random variables.\nOk… so how well does such a sample set approximate the total population? We start by asking if our sample is biased. It isn’t and this is proved in the course notes.\nNext we ask how much my sample mean might vary relative to the true mean? Vary? sounds like “variance”. Your right. We want to calculate the variance of the mean (not the variance of the variance itself). Derivation:\n\n\\[\n\\begin{array}{rlr}\n\\operatorname{Var}(\\bar{X}) & =\\operatorname{Var}\\left(\\sum_{i=1}^{n} \\frac{X_{i}}{n}\\right)=\\left(\\frac{1}{n}\\right)^{2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) & \\\\\n& =\\left(\\frac{1}{n}\\right)^{2} \\sum_{i=1}^{n} \\operatorname{Var}\\left(X_{i}\\right)=\\left(\\frac{1}{n}\\right)^{2} \\sum_{i=1}^{n} \\sigma^{2}=\\left(\\frac{1}{n}\\right)^{2} n \\sigma^{2}=\\frac{\\sigma^{2}}{n} & \\\\\n& \\approx \\frac{S^{2}}{n} & \\text { Since S is an unbiased estimate } \\\\\n\\operatorname{Std}(\\bar{X}) & \\approx \\sqrt{\\frac{S^{2}}{n}} & \\text { Since Std is the square root of }\n\\end{array}\n\\] \\[\\begin{array}{rlr}\\operatorname{Var}(\\bar{X}) & =\\operatorname{Var}\\left(\\sum_{i=1}^{n} \\frac{X_{i}}{n}\\right)=\\left(\\frac{1}{n}\\right)^{2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) & \\\\ & =\\left(\\frac{1}{n}\\right)^{2} \\sum_{i=1}^{n} \\operatorname{Var}\\left(X_{i}\\right)=\\left(\\frac{1}{n}\\right)^{2} \\sum_{i=1}^{n} \\sigma^{2}=\\left(\\frac{1}{n}\\right)^{2} n \\sigma^{2}=\\frac{\\sigma^{2}}{n} & \\\\ & \\approx \\frac{S^{2}}{n} & \\text { Since S is an unbiased estimate } \\\\ \\operatorname{Std}(\\bar{X}) & \\approx \\sqrt{\\frac{S^{2}}{n}} & \\text { Since Std is the square root of Var }\\end{array}.\\]\n\nThis last part (\\(\\operatorname{Std}(\\bar{X}) \\approx \\sqrt{\\frac{S^{2}}{n}}\\)) has a special name: standard error: and its how you report uncertainty of estimates of means.\nApplied example: Let’s say we calculate the our sample of happiness has \\(n = 200\\) people. The sample mean is \\(\\bar{X}=83\\) and the sample variance is \\(S^2 = 450\\). We can now calculate the standard error of our estimate of the mean to be \\(1.5\\). When we report our results we will say that the average happiness score in Bhutan is \\(83 ± 1.5\\) with variance \\(450\\). And done!\n\nSee how we give a “confidence interval” in how well our approximation was for the true distribution (i.e. the distribution you’d get if you sampled the entire population). And so in this light, the standard error allows us to quantify our belief bounds in the sample mean representing an approximation of the true distribution.\n\n\n\n\nWe now discuss an important algorithm (invented at Stanford!). It is a newly invented statistical technique that allows us to understand distributions better and also allows us to calculate p-values: the probability that a scientific claim is incorrect.\n\nIt was invented as a consequence of using computing to capture insights in probability theory. Specifically, the key insight here is that if we had access to the underlying distribution of our samples (i.e. its PDF/PMF), then we can answer any question we might have regarding the accuracy of our statistics we calculated from our original sample (e.g. how close our sample mean is to the true mean). But why so? Well… it goes like this:\n\nWhat if we want to know the probability that the true variance is within a certain range of the number we calculated? If you knew the underlying distribution, \\(F\\), you could simply repeat the experiment of drawing a sample of size \\(n\\) from \\(F\\) (i.e. a sample set consisting of \\(n\\) items), calculate the sample variance from our new sample and test what portion fell within a certain range.\n\nSide note: when we “sample”… we are not referring to a singular number (unless \\(n=1\\)). But rather we refer to a set of size \\(n\\) consisting of \\(n\\) drawings from the distribution.\n\n\nBut of course we aren’t really going to be given \\(F\\), the distribution function. Instead, we’d need to estimate that as well which we can do in a similar fashion: the best estimate that we can get for \\(F\\) is from our sample itself! The simplest way to estimate \\(F\\) (and the one we will use in this class) is to assume that the \\(P(X = k)\\) is simply the fraction of times that k showed up in the sample. And this defines the PMF of our estimate \\(\\hat{F}\\) of \\(F\\).\n\nPseudocode:\n\nHere is the idea:\n\nWe draw our initial sample from the population. For example, get the hapiness scores of 200 people from Bhutan.\nPlot these scores as a histogram with probabilities for each value. This is our initial sample distribution. But we want to make it more accurate. Bootstrapping gives us a way to do this (specifically to estimate the true distribution). Here is what we do next.\nWe loop a certain amount of times. Each time in the loop, we draw \\(n\\) samples from the histogram distrubution we just created (we are sampling from this distribution). This gives you a set of \\(n\\) elements. Now we can calculate the relavant statistic (e.g. mean, variance) we want from this set and append this to a list.\nAfter the loop, we should have a new set of statistics. We can plot these and this forms our new distribution.\n\nSide note: we don’t actually end up with a whole new distribution in the same unit as the true distribution. Instead, we calculate this in the context of a relavant statistic we want for each new statistic. For example, take variance. Bootstrapping leaves us with a whole new distribution for the variance which aims to estimate the true underlying distribution’s variance.\n\nKey insight: say we want to estimate the true variance of happiness scores in Bootan (pun intended here). We take 200 people from the population and plot out a distribution of hapiness scores. This gives us a “fake”/estimate of the true underlying distribution (let’s call it \\(\\hat{f}\\)). Now we can go and calculate the variance directly outright from here by drawing \\(n\\) elements from \\(\\hat{f}\\) but that wouldn’t be that accurate. Instead, what we do is run a loop and get tons of samples of size \\(n\\) and calculate the sample variance for each of these sets. Now these calculations itself form a set. And now to get what we wanted, we simply take the variance of this master set which accruately measures the true variance. Voila!\n\nSide note: in Python, to sample from a histogram (i.e. the initial 200 people you observed), use code from the official lecture slide deck.\nThe key point about bootstrapping is that it allows us to calculate the accuracy of such statistics.\nAlso note: we just keep on resampling from the data we are given (not just some \\(n\\) subset).\n\nSummary: say we want to measure a statistic about the general population. We of course cannot survey the entire population so instead we take a sample of size \\(n\\). Then, bootstrapping allows us to estimate what the true distribution (i.e. if the whole population had indeed been sampled) is by resampling. Specifically, the algorithm goes like this: 1. take your initial sample from the population of size \\(n\\). 2. Create a loop of 10,000 or greater times. At each iteration, draw a sample, of size \\(n\\) from the initial sample. 3. Calculate your statistic you want on this subset sample. Append it to a master list. 4. After the loop, calculate the statistic on this master list itself. This is you answer."
  },
  {
    "objectID": "notes/cs109/16-beta.html",
    "href": "notes/cs109/16-beta.html",
    "title": "Knowledge",
    "section": "",
    "text": "Date of when lecture was given: 02/11/22.\nWe now begin discussion of an entire new unit: uncertainty theory. We will have a meta discussion about probabilities: what happens when there is uncertainty in our probabilities themselves?! We need some sort of measure to quantify uncertainty. Unless you have been sleeping under a rock this entire quarter, this should scream: “probability is the measure of uncertainty”. Yes. Yes, that is right. We will use probabilities to measure the uncertainty of other probabilities (via random variables). This is the most theoretical and therefore most mathematical part of the course.\n\n\n\nExample: You ask about the probability of rain tomorrow. Person A: My leg itches when it rains and its kind of itchy…. Uh, \\(p = .80\\). Person B: I have done precise, complex calculations and have seen 10,451 days like tomorrow… \\(p = 0.80\\). What is the difference between the two estimates?\nExample: you are buying an item on Amazon and are deciding between two different models. The first model has all 5-star ratings! Woohoo! And the second model has an average of 4.3 stars. Easy decision right? Wrong. It turns out the 5-star rating has only 5 reviewers whereas the second model has over thousands of reviewers. Which one do you choose now? This type of uncertainty in our belief for each item model is what we will discuss today.\nRemark: Until now probabilities have just been numbers in the range 0 to 1. However, if we have uncertainty about our probability, it would make sense to represent our probabilities as random variables (and thus articulate the relative likelihood of our belief).\n\nWe introduce the beta random variable in this light via annotated example.\n\n\n\nSay we have some coin and we want to know the probability, \\(p\\), that it will turn up as heads (the age old question). We flip the coin a total of \\(n+m\\) times where \\(n\\) is the number of heads and \\(m\\) is the number of tails (so \\(n+m\\) is all flips). And so we have\n\\[p=\\frac{n}{n+m}.\\]\nBut is this really true? What if we only flip the coin 3 times total? We might get something like \\(p \\approx 0.33333\\). Are we really certainly about our belief in this probability? Can you guess what I am hinting it (uncertainty in our belief!)? This number (\\(p=\\frac{n}{n+m}\\)) doesn’t capture our uncertainty about \\(p\\) itself. So just like anything with uncertainty, we propose to capture such semantics via a distribution for the values that \\(p\\) can take on.\nRecall that we defined a random variable as something that can take on different values at different probabilities. To formalize the idea that we want a distribution for \\(p\\), we are going to use a random variable \\(X\\) to represent the probability of the coin coming up heads. And so we can define probabilities that \\(X\\) takes on a certain probability. Probability is a continious measure and has a range (support) between 0 and 1.\nAnd so without any prior belief in what \\(X\\) can be, we assume that each value that \\(X\\) can take on is equally likely (that is, we assume \\(X\\) can take on any probability at an equally likely rate). For this reason, we start by saying\n\\[X \\sim U n i(0,1).\\]\nNow how do we “update our beliefs about a random variable/probability”?! You got it. Bayes’ theorem. So we wil condition on another random variable that represents the new observations. For the Beta distribution, we condition on Binomials (take a moment to understand this: you are conditioning on an entirely separate distribution… how you obtain that distribution can be through frequentist experiment trials or be given the distribution predeterminedly).\nSay we start to flip the coin a few times now and we want to update our beliefs (probabilities) for \\(X\\). This is actually a whole experiment in it of itself. And so let \\(N\\) be a random variable that represents the number of heads that can come up, given that the coin flips are independent. So we define our binomial. But of course, we need to plug in a parameter \\(p\\) for our binomial so we actually condition on \\(X\\) itself first. That is,\n\\[(N \\mid X) \\sim \\operatorname{Bin}(n+m, x).\\]\nRecall that the little \\(x\\) is a probability. So we now apply Bayes’ theorem to update our belief in \\(X\\) given the new Binomial distribution for \\(X\\) (specifically we want to calculate the PDF distribution for \\(X\\) given \\(N\\)):\n\nNow notice one specific thing. Remember that each component of Bayes’ theorem has a name:\n\n\n\n\nSo in summary, we stated that we might have uncertainty in our beliefs (probabilities) themselves. Thus, we decided to represent probability as a random variable, \\(X\\). Now we aimed to find a parametric distribution for the random variable. Initially, we started with a uniform distribution: \\(X \\sim U n i(0,1)\\) where the value that \\(X\\) can take on is equally likely. But of course, we want to update such a belief for \\(X\\) given some observed experimental trials (or expert knowledge). So we use conditional probability in the form of Bayes’ theorem. In this case, we condition on a Binomial RV, \\(N\\) (i.e. coin flips experiment).\n\\[\nf_{X \\mid N}(x \\mid n)=\\frac{P(N=n \\mid X=x) f_{X}(x)}{P(N=n)}\n\\]\nBut realize that we have a prior! A lot of times we can just keep this as a uniform because we may not know any previous knowledge beforehand. But sometimes we do. For example, we may know that a certain drug is likely to work to a certain degree based on expert knowledge. And to incorporate such knowledge of this in the calculation, we can define the prior as such. Notice that the prior is also an RV PDF (i.e. \\(f_{X}(x)\\)) which represents the initial beliefs in our probabilities (so in some way this is recursion but it really isn’t because we are usually given such information). That is, we incorporate such domain knowledge via a probability density function which represents the beliefs in the probabilitied for \\(X\\) initially (“prior information”). This is the thing we are updating! To this in it of itself can be any RV that has range \\([0,1]\\) which leaves us with either a beta RV and uniform RV. And so if we have some prior non-uniform belief, we must use a beta to represent that belief! So yes, We are using a beta RV to derive the PDF for the beta. But don’t worry, this isn’t recursion because you’d usually be given such a beta or at least the information needed to describe and therefore denote it (i.e. through expert domain knowledge or repeated experimentation). Note that such a property is called a conjugacy: we are deriving the posterior from the same distribution family in the prior.\nOk… so we have the idea and intuition down: we need our prior to be a beta. But we haven’t even given the derivation for a beta yet. We mentioned that this information is gained by things like expert knowledge or repeated trials. We will talk about deriving the beta for the latter (the former is usually given).\n\nWe flip a fair coin defined as X ~ Uni(0, 1) prior over probability, and observe: \\(n\\) “successes” and \\(m\\) “failures”. By the Binomial distribution, our new belief in \\(X\\) is \\[\nf_{X}(x)=\\frac{1}{c} \\cdot x^{n}(1-x)^{m}.\n\\] where \\(c=\\int_{0}^{1} x^{n}(1-x)^{m}\\). And so plugging this in as our prior in the original Bayes’ derivation above, we have \\[\n\\begin{aligned}\nf(X=x \\mid N=n) &=\\frac{P(N=n \\mid X=x) f(X=x)}{P(N=n)} \\\\\n&=\\frac{\\left(\\begin{array}{c}\nn+m \\\\\nn\n\\end{array}\\right) x^{n}(1-x)^{m} f(X=x)}{P(N=n)} \\\\\n&=\\frac{\\left(\\begin{array}{c}\nn+m \\\\\nn\n\\end{array}\\right) x^{n}(1-x)^{m} \\frac{1}{B(a, b)} x^{a-1}(1-x)^{b-1}}{P(N=n)} \\\\\n&=K_{1} \\cdot\\left(\\begin{array}{c}\nn+m \\\\\nn\n\\end{array}\\right) x^{n}(1-x)^{m} \\frac{1}{B(a, b)} x^{a-1}(1-x)^{b-1} \\\\\n&=K_{3} \\cdot x^{n}(1-x)^{m} x^{a-1}(1-x)^{b-1} \\\\\n&=K_{3} \\cdot x^{n+a-1}(1-x)^{m+b-1} \\\\\nX \\mid N & \\sim \\operatorname{Beta}(n+a, m+b)\n\\end{aligned}.\n\\]\n\n\nPDF: \\[\nf(x)=B \\cdot x^{a-1} \\cdot(1-x)^{b-1}.\n\\]\n\nThere you have it folks: the beta random variable derived by conditioning a uniform on a Binomial! See here and here.\n\nSummary: ok… now a real short and sweet summary. We want a random variable to measure our beliefs in the some binomially distributed process (e.g. coin flips). We start by defining \\(X\\) as a uniform: \\(X ~ Uni(0, 1)\\). Now we perform some experiment (i.e. flipping the coin some number of times) which gives us a binomial RV, \\(N\\). We condition \\(X\\) on \\(N\\) to update our beliefs about \\(X\\): \\[\nf(X=x \\mid N=n)=\\frac{P(N=n \\mid X=x) f(X=x)}{P(N=n)}\n\\]\n\n\n\n\n\nRandom variable for measuring probabilities\n\nSo you have a distribution of probabilities\nSupport is 0 to 1.\n\nProbability… so it is continuous\n\n\nThe input parameters are like a Binomial experiment. For example, we flip a coin 10 times and it comes up heads 7/10 times and tails 3/10 times. So what is our distribution of probabilities that such a random variable can take on? Well, certainly given the observed data, we see that the most likely probability should be 7/10.\nSo we now have a distribution of probabilities for each of the probabilities that the coin can take on (and thus articulate the relative likelihood of our belief).\n\n\n\n\n\nTwo schools of thought and it boils down to a component of an equation.\n\nIn the coin flips example, we can simulate billions of coin flips instantly and approach the problem from a frequentist point of view (\\(\\frac{m}{m+n}\\)). However…\nWe might want to assign a Beta random variable that represents the probability of a medical drug working. We cannot simulate this on billions of people. So we have to use our expert domain prior knowledge about the world somehow. This is the Bayesian world view.\nSee 50:00 of lecture 16.\n\nTake this example:\n\n\nWhen we use beta as a prior, we don’t need to do the above derivation because the posterior is from the same distribution. So all we need to do is add the number of successes and number of failures to our prior parameters for beta and we get the newly defined posterior. The derivation works out to be \\(\\operatorname{Beta}(a+h, b+t)\\) where \\(a\\) and \\(b\\) are the original parameters and \\(h\\) and \\(t\\) are the updated number of successes and failures.\nApplication: You might be asked to calculate the updated belief in \\(X\\), already defined as a beta of the form \\(\\operatorname{Beta}(a, b)\\) given \\(h+t\\) new trials where \\(h\\) is number of new successes and \\(t\\) is number of new failures. So the new RV is just \\(\\operatorname{Beta}(a+h, b+t)\\). Derivation is given in course reader but it is just a repeated application of Bayes’ theorem.\nIn our original derivation for the beta, we conditioned \\(X \\sim Uni(0, 1)\\) on \\((N \\mid X) \\sim \\operatorname{Bin}(n+m, x)\\) which gave us the PDF for a beta: \\[f(X=x)=\\left\\{\\begin{array}{ll}\\frac{1}{B(a, b)} x^{a-1}(1-x)^{b-1} & \\text { if } 0<x<1 \\\\ 0 & \\text { otherwise }\\end{array} \\quad\\right.$ where $B(a, b)=\\int_{0}^{1} x^{a-1}(1-x)^{b-1} d x\\]\nNow we can continiously update our belief on \\(X\\) with other RVs as priors but if that new prior happens to be a beta… our update is very simple!: \\[\\operatorname{Beta}(a, b) \\rightarrow \\operatorname{Beta}(a+h, b+t). \\]\nNote: the original derivation for the beta was done by updating a uniform conditioned on a binomial. Here we condition a beta on a binomial (and so our prior is a beta and we actually end up with another beta which is our posterior).\nAnother note: if the posterior is the same as the prior (i.e. conjugacy), you should realize that all you are doing is updating the parameters.\n\n\n\n\n\n\nCan use laplace prior instead of uniform to prevent probability being 0. Can be nicer in code.\nImagine a scenario where we need to decide betwe en two drugs to give to patients. Let us say we have two beta distributions that represent the probabilities that each drug is a success. We can use a greedy approach that is referred to as Thompson sampling. It goes like this. Sample from your first beta distribution (giving a probability). Now sample from your second beta distribution (giving you another probability). You should choose the drug whose sample was a higher probability. Why? Because the confidence is higher in that drug (see around 1:15 for a more detailed explanation).\n\nThis allows us to make good decisions “under uncertainty”.\nThis has a whole host of applications such as in RL for game-playing agents (which path should I take in chess?).\n\nA lot of these algorithms though use similar, but more complex RL algorithms."
  },
  {
    "objectID": "notes/cs109/course-review.html",
    "href": "notes/cs109/course-review.html",
    "title": "Knowledge",
    "section": "",
    "text": "Use the Poisson approximation when \\(n\\) is large \\((>20)\\) and \\(p\\) is small \\((<0.05)\\). Use the Normal approximation when \\(n\\) is large \\((>20)\\), and \\(p\\) is mid-ranged. Specifically, when variance is greater then 10 , in other words: \\(n p(1-p)>10\\). If in doubt, go with normal.\n\n\n\n\n\n\nDiscrete/discrete conditional: \\(X\\) and \\(Y\\) are discrete RV’s. Conditional definition with discrete random variables. \\[\n\\mathrm{P}(X=x \\mid Y=y)=\\frac{P(X=x, Y=y)}{P(Y=y)}\n\\]\nDiscrete/discrete Bayes’: Let \\(M\\) and \\(N\\) be discrete random variables. Then, by Bayes’ theorem: \\[P(M=m \\mid N=n)=\\frac{P(N=n \\mid M=m) P(M=m)}{P(N=n)}.\\] Law of total probability way: \\[P(M=m \\mid N=n)=\\frac{P(N=n \\mid M=m) P(M=m)}{P(N=n \\mid M=m) P(M=m) + P(N=n \\mid M=m^{C}) P(M=m^{C})}.\\]\n\n\n\\[\n\\mathrm{M}, \\mathrm{N} \\text { are discrete. } \\mathrm{X}, \\mathrm{Y} \\text { are continuous }\n\\]\n\\[\n\\begin{aligned}\np_{M \\mid N}(m \\mid n) &=\\frac{P_{N \\mid M}(n \\mid m) p_{M}(m)}{p_{N}(n)} \\\\\nf_{X \\mid N}(x \\mid n) &=\\frac{P_{N \\mid X}(n \\mid x) f_{X}(x)}{P_{N}(n)} \\\\\np_{N \\mid X}(n \\mid x) &=\\frac{f_{X \\mid N}(x \\mid n) p_{N}(n)}{f_{X}(x)} \\\\\nf_{X \\mid Y}(x \\mid y) &=\\frac{f_{Y \\mid X}(y \\mid x) f_{X}(x)}{f_{Y}(y)}\n\\end{aligned}\n\\]\n\n\n\n\n\nWe started out with joints for specifying various probabilities for multiple random variable systems. However, joint becomes too big so we to do inference (conditional probability RVs: updating your belief about an RV event given a different RV event), we construct causal Bayesian networks. To compute conditional probability from Bayesian network, use rejection sampling: Pseudcode:\nStep 0: Have a fully specified Bayesian network layed out.\nStep 1: sample a ton!\nStep 2: collect subset of samples consistent with observation (thing you are conditioning)\nStep 3: loop through this subset and count number of times the thing you want the probability of is present in the sample.\nStep 4: Divide this count by the length of this sample subset (the subset whose elements are samples consistent with the observation).\n\n\n\n\nUniform prior:\n\nConjugate prior:\n\n\n\n\n\nSummary: say we want to measure a statistic about the general population. We of course cannot survey the entire population so instead we take a sample of size \\(n\\). Then, bootstrapping allows us to estimate what the true distribution (i.e. if the whole population had indeed been sampled) is by resampling. Specifically, the algorithm goes like this: 1. take your initial sample from the population of size \\(n\\). 2. Create a loop of 10,000 or greater times. At each iteration, draw a sample, of size \\(n\\) from the initial sample. 3. Calculate your statistic you want on this subset sample. Append it to a master list. 4. After the loop, calculate the statistic on this master list itself (note that the master list is literally a graph-able list of all the e.g. variances and from that distribution you want the e.g. variance). This is you answer.\n\nPseudocode:\ndef bootstrap(sample):\n   N = number of elements in sample\n   pmf = estimate the underlying pmf from the sample\n   stats = []\n   repeat 10,000 times:\n      resample = draw N new samples from the pmf\n      stat = calculate your stat on the resample\n      stats.append(stat)\n   stats can now be used to estimate the distribution of the stat\n\n\nWe have two distributions of numbers. We want to know whether or not the two distributions are fundamentally different or the same: were the numbers for each distribution drawn from the same universal underlying distribution? - The null hypothesis is that they aren’t different: there is no difference between the two groups, so everyone is drawn from the same distribution. A p-value is simply the probability (i.e. # between \\([0,1]\\)) that the null hypothesis is true. So…\n- If \\(p\\) is closer to \\(1\\), then we think that the distributions are likely to be the same (i.e. drawn from the same underlying distribution).\n- If \\(p\\) is closer to \\(0\\), then we think that the distributions are likely to be different (i.e. drawn from different underlying distributions).\nWe use bootstrapping here via repeated sampling.\nFirst, take the two distributions and jot down the initial difference between the means (or whatever relevant statistic you are calculating). 1. Concatenate together the two sample lists to make one universal distribution, \\(U\\). 2. Iterate at least \\(10,000\\) times and at each iteration, sample from \\(U\\) (appropriate lengths) twice: one for group A and one for group B. 3. Say you want to calculate the difference in means (called effect size). Take abs(diff between means). Append this to a master list which gets all the effect sizes for the iterations. 4. Also, at each iteration, calculate whether the effect size between the current sample means is greater than the initial effect size. If so, add to a counter. 5. At the end, take the counter value and divide it by the total number of iterations. This is your p-value.\n\nWe can calculate p-value using bootstrapping:\n\nIdentify what sample statistics you’re calculating, e.g. abs difference of sample means.\nState null hypothesis.\nCalculate observed sample statistics (using observed sample).\nRun a bootstrap experiment.\nResample the original sample to get a equally-sized new sample.\nCalculate new sample statistic.\nIf new sample statistic is more extreme than observed statistic, count \\(+=1\\).\np-value: percentage of more extreme samples over all bootstrap samples \\(\\left(\\frac{\\text { count }}{\\text { num trials }}\\right)\\).\n\nAlternative explanation:\n\n\n\n\n\n\n\n\n\n\n|200\n\n\nRecall we are working with \\(log_e\\) (natural log) which is \\(ln\\). \n\n\n\n|200\n\n\n\n\n\n\n\n|500\n\n\n\n\n\n|200\n\n\n\\[\n\\begin{aligned}\n&\\frac{d}{d x} \\ln (x)=\\frac{1}{x} \\\\\n&\\frac{d}{d x} \\ln [f(x)]=\\frac{1}{f(x)} f^{\\prime}(x) \\\\\n&\\frac{d}{d x} \\log _{a}(x)=\\frac{1}{x \\ln a} \\\\\n&\\frac{d}{d x} \\log _{a}[f(x)]=\\frac{1}{f(x) \\ln a} f^{\\prime}(x)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nExample: here is an intuitive example. We have two groups of people\n\nP1: 50 people, each independently infected with \\(p=0.1\\)\nP2: 100 people, each independently infected with \\(p=0.4\\)\nQuestion: Probability of more than 40 infections?\n\nSolution: your first thought is probably to define two binomial random variables and just add them. Let \\(P_1 \\sim bin(50,0.1)\\) and let \\(P_1 \\sim bin(100,0.4)\\). But we have a problem. We can’t add binomials with different probability parameters (see course reader equation definitions)! So what we do now? We approximate the binomials with normals!!!\n\nThe key point here is that you should try to rearrange your problem to get it in terms of something you know how to work with.\nThough remember to perform continuinty correction when tranforming from the discrete to continious realm.\n\n\nRemark: Say we have some random variable, \\(X\\). What is \\(2X\\)? Hold your horses. It is not like what we did above. In fact, this is an invalid expression. Can you picture why? Because \\(X\\) is not independent from itself! So instead, we must use a linear transform.\nRemark: subtraction is just addition by linear transforming the second part of the expression with -1 first. So \\(X-Y=X+(-1Y)=X+(-Y)\\)."
  },
  {
    "objectID": "notes/cs109/24-logistic-regression.html",
    "href": "notes/cs109/24-logistic-regression.html",
    "title": "Knowledge",
    "section": "",
    "text": "We now look at another powerful classification algorithm called logistic regression.\n\n“Heart and soul” of neural networks\nOptimization:\n\nWe need to understand optimization because Log reg., uses it quite heavily!\n\n\n\nThe idea behind supervised learning is, training data in form \\((\\mathbf{x}, y) \\ldots\\) we want to learn function \\(\\hat{y}=g(\\mathbf{x})\\) that maps \\(\\mathbf{x}\\) to \\(y\\). This is the same thing as \\(g(\\mathbf{x})=\\operatorname{argmax} \\hat{P}(Y=y \\mid \\mathbf{X})\\). IN Naive Bayes’ we literally worked directly with this probability and manipulated it using Bayes’. Here we take a different approach. We recognize that this is just one big function that maps \\(\\mathbf{X} \\rightarrow y\\). We are going to try to “approximate” this function!\n\n\n\nBefore we get started, we need to introduce a couple new things and notation: \n\nThe sigmoid function is a squashing function which squashes input \\(z\\) to be a number between 0 and 1 (needed for probabilities!). It looks like: \nSecond, we will be doing lots of matrix-vector multiplications here because of weighted sums. So it is important to understand the inner/dot product between parameters and inputs: \\[\\begin{array}{rlr}\n\\theta^{T} \\mathbf{x} & =\\sum_{i=1}^{n} \\theta_{i} x_{i} \\quad \\begin{array}{c}\n\\text { Weighted sum } \\\\\n\\text { (aka dot product) }\n\\end{array} \\\\\n& =\\theta_{1} x_{1}+\\theta_{2} x_{2}+\\cdots+\\theta_{n} x_{n}\n\\end{array}\\]\nAnd finally, we put these two together in the third point. The reason is what we will be doing is (2.) computing the weighted sum and then (1.) squashing it to a probability. Directly, we define this as \\(\\sigma\\left(\\theta^{T} \\mathbf{x}\\right)=\\frac{1}{1+e^{-\\theta^{T} \\mathbf{x}}}\\).\n\nAlso, some calculus background:\n\nRecall that for composition of functions, \\(f(x)=f(z(x))\\), we use chain rule: \\[\n\\frac{\\partial f(x)}{\\partial x}=\\frac{\\partial f(z)}{\\partial z} \\cdot \\frac{\\partial z}{\\partial x}.\n\\]\n\n\n\n\n\nNB made a naive assumption. That leaves room for improvement!\nCut to the chase! Avoid probabilities. Just approximate the function \\(P(Y \\mid X)\\)! In other words: \\[P(Y=1 \\mid \\mathbf{X}=\\mathbf{x})=\\sigma\\left(\\sum_{i} \\theta_{i} x_{i}\\right)\\].\nYou can think of this as a rule-based machine guided by some weighted: you stick in an input, it gets weighted by the parameters (sliders), summed up and squashed.\n\n\n\nIn other words, we are just using a direct engineering approach: no probability theory (yet)! In actuality, this is just a neural network with no hidden layers!\nBut of course, our outputs depend on those weights. So we are going to use prob. and calculus to estimate them!\nSmall note: we add an extra input that is always “turned on (i.e. in binary, the feature is always a 1)” to make it work a little better. This is called a “bias” term.\n\nMath: this equation is how we “predict” on a new test sample \\(\\mathbf{x}\\): \\[P(Y=1 \\mid \\mathbf{X}=\\mathbf{x})=\\sigma\\left(\\sum_{i} \\theta_{i} x_{i}\\right).\\]\nthat is literally it! Nothing else to it. Except of course, we need to determine what exactly these \\(\\theta_i\\)’s (parameters/weights) are. We have a method for this: parameter estimation! MLE!\n\nSo once we have these \\(\\theta_i\\)’s determined from training, we basically have this rule-based machine that we can use to make predictions with ease!\nOf course though, the hard part is determined these \\(\\theta_i\\)’s. We do this via training which is what we get into next.\n\n\nKey note: \\(P(Y=1 \\mid \\mathbf{X}=\\mathbf{x})=\\sigma\\left(\\sum_{i} \\theta_{i} x_{i}\\right)\\), is, as stated, a probability for \\(y=1\\) so to get \\(P(Y=0 \\mid \\mathbf{X})\\) we just employ the law of total probability and subtract from this.\n\nWe have discussed the forward pass now lets do the backwards pass!\n\nNote: a cool thing about Sigmoid is that it basically turns the “energy” (weighted sum) coming into the probability into a thing that functions like a probability.  See how the inflection point is at \\(y=0.5\\)? So any positive energy will be predicted positively and vise versa.\n\n\n\n\n\nOur performance in predicting new samples will be based on the values of the weight parameters. So that is where the real intelligence comes from: estimating the weight values! How do we do this? Let’s find out!\nWe are going to use MLE: we need to find the parameters that make our data look most likely!\nBut of course, we don’t have a direct distribution we are using as a model. So what are the parameters we are estimating? Well, we don’t need a defined distribution. We just want to estimate those weights–those are the these parameters.\nRecall that MLE is taking derivatives.\n\n\nWe use gradient ascent and update our gradient using the last line in this formula. We have multiple weights so we actually calculate the gradient for all of these weights and update them for each one on each iteration. Thus, we have a for loop for each iteration (double for-loop).\n\nCool (i.e. how we derived the LL function above): in binary classification, we essentially just have a Bernoulli! But that isn’t differentiable so we model it in a continious fashion: .\n\nSo we just plug in \\(P(Y=1 \\mid \\mathbf{X}=\\mathbf{x})=\\sigma\\left(\\sum_{i} \\theta_{i} x_{i}\\right)\\) for \\(p\\) in the Bernoulli continious equation."
  },
  {
    "objectID": "notes/cs109/26-deep-learning.html",
    "href": "notes/cs109/26-deep-learning.html",
    "title": "Knowledge",
    "section": "",
    "text": "DL is simply a bunch of logistic regressions stacked on top of each other.\nDL is basicaly MLE within neural networks.\n\n\\(P(Y|X=x)\\)\nHow to train: - Apply MLE essentially.\nWeights:\n\\[\n\\mathbf{h}_{j}=\\sigma\\left(\\sum_{i=0}^{m_{x}} \\mathbf{x}_{i} \\theta_{i, j}^{(h)}\\right)\n\\]\nEach individual weight calculation is simply a singular logistic regression.\nQuestion: if each hidden layer neuron gets there input from the input vector, why aren’t all the hidden layers the same? Turns out MLE naturally diverges and initial params are randomly set.\nSo what is the parametric PMF? Turns out it is a Bernoulli.\n\nProb distriubtion is literally at the end: Bernoulli for binary classification. We send in a data point and in the end it calculates the log likelihood that we see that data point.\n\nNote:\n\nEach data point (or datum) goes through forward pass and you get a probability: \\[\nP(Y=y \\mid \\mathbf{X}=\\mathbf{x})=(\\hat{y})^{y}(1-\\hat{y})^{1-y}\n\\] Now do this for all data points and take the log\n\\[\nL L(\\theta)=\\sum_{i=1}^{n} y^{(i)} \\log \\hat{y}^{(i)}+\\left(1-y^{(i)}\\right) \\log \\left[1-\\hat{y}^{(i)}\\right]\n\\]\nTrain by calculating derivative of log likelihood with respect to all parameters."
  },
  {
    "objectID": "notes/cs109/22-maximum-a-posteriori.html",
    "href": "notes/cs109/22-maximum-a-posteriori.html",
    "title": "Knowledge",
    "section": "",
    "text": "Our goal in ML is to estimate the parameters of a distribution.\nWe learned of a nice technique called MLE last lecture. But it can get quite limited in certain scenarios. So we introduce another one here called (Maximum A Posteriori) to add to our repertoire.\n\n\n\n\nTake a uniform distribution… it will just learn the min and max vals as the parameters. But that is nonsense: every point’s probabilities aren’t uniform! The problem? MLE overfits. It doesn’t generalize well to new data.\n\n\n\n\n\nThe philosophy is that we do want to incorporate prior information that we know about the world into our estimations.\n\nDifference (see 1:02): - MLE: choose the parameters (\\(\\theta\\)) that makes your data look most likely: \\(L(\\theta)=\\prod_{i=1}^{n} f\\left(X_{i} \\mid \\theta\\right).\\) \\[\\begin{aligned} \\hat{\\theta}_{M L E} &=\\underset{\\theta}{\\operatorname{argmax}} f\\left(X^{(1)}=x^{(1)}, \\ldots, X^{(n)}=x^{(n)} \\mid \\theta\\right) \\\\ &=\\underset{\\theta}{\\operatorname{argmax}}\\left(\\sum_{i} \\log f\\left(X^{(i)}=x^{(i)} \\mid \\theta\\right)\\right) \\end{aligned}.\\] - MAP: choose the most likely parameters, \\(\\theta\\), given the data \\(X_{1}, X_{2}, \\ldots, X_{n}\\). \\[\\hat{\\theta}_{M A P}=\\underset{\\theta}{\\operatorname{argmax}} f\\left(\\theta \\mid x^{(1)}, \\ldots, x^{(n)}\\right).\\]\n\n\n\nSo… we know how to do \\(\\underset{\\theta}{\\operatorname{argmax}} f(X \\mid \\theta)\\) (MLE) but now we want the opposite: \\(\\underset{\\theta}{\\operatorname{argmax}} f(\\theta \\mid X)\\). This should scream Bayes! Because it doesn’t make much sense to calculate the most likely parameters, \\(\\theta\\), given the data. But the inverse is easy so we use Bayes’:\n\\[\\hat{\\theta}_{M A P}=\\underset{\\theta}{\\operatorname{argmax}} \\frac{f\\left(x^{(1)}, x^{(2)}, \\ldots, x^{(n)} \\mid \\theta\\right) g(\\theta)}{h\\left(x^{(1)}, x^{(2)}, \\ldots x^{(n)}\\right)}. \\]\nNotice that we have the MLE part as our likelihood (quite a fitting name!) and also that a whole new prior is included. How do we get that? That is for you to choose and incorporate into your model! You see how this is more powerful now? We can input another prior! We are going to choose a \\(\\theta\\) that is informed by our prior belief.\n\nAh… look how nice that is… we literally end up with the same equation except now we can incorporate our prior beliefs as well! Yippee! It’s quite conveniant.\n\nNote: the prior is a “prior belief in the parameter \\(\\theta\\)”. So this is a probability of the parameter (not just the parameter itself). So let us say you have a Bernoulli you want to estimate the parameter \\(p\\) for. Your prior should be your prior belief in \\(p\\) which itself is a probability. So we have a probability of a probability. And therefore, we choose to represent our prior using the Beta distribution.\n\nDefinition (Laplace prior): When using a beta as a prior, we can use our expert domain knowledge to determine the parameters of that beta prior. But sometimes we don’t have expert domain knowledge so a common one to start out with as a base is to say you observed 1 success, 1 failure: so the prior is \\(beta(2,2)\\). Doing this prevents any posterior probability from being 0 or 1 (useful for programming so you don’t fuck up computation!). Recall Hamilton Federalist paper example: if there was one singular word in the document that Hamilton never wrote before…. BAM! probability for Hamilton writing that document drops all the way to 0! So use Laplace prior for cases like this (like in your project!).\n\nNote: we do NOT train the prior’s parameters. These are chosen by us (that is the whole point on “updating your prior beliefs”… these are beliefs you already hold).\n\n\n\n\nNote: see 23:00 - 27:00 and make notes on that part where he talks about which priors to use for what posteriors (i.e we are actually estimating the posterior’s parameters)."
  },
  {
    "objectID": "notes/cs109/25-ethics.html",
    "href": "notes/cs109/25-ethics.html",
    "title": "Knowledge",
    "section": "",
    "text": "“With great power comes greater responsibility”.\n\nLearning goals: - How to measure fairness - How to mitigate biases\n\n\nWe analyze different types of harms caused by algorithms:\n\nQuality-of-service harms\nDistributive harms\nExistential harms\n\n\n\n\n\nAlgos are hypersensitive to data that they are trained on. They might recognize patterns that we cannot recognize easily. See slide 40.\n\nMake this a paragraph in PWR paper.\n\nAlso include model cards initiative (i.e. in transparency section).\nThe section on Part 1: Framework of Harms can be used to show the effects biases have on society at greater.\n\n\nCase study: st. georges algorithm: i.e. NLP for ethnic names\n\n\n\n\n\nAdapting philosophical terms to ai algorithms.\n\n\n\n\nmistakes of the future"
  },
  {
    "objectID": "notes/cs109/20-algortihmic-analysis.html",
    "href": "notes/cs109/20-algortihmic-analysis.html",
    "title": "Knowledge",
    "section": "",
    "text": "We will learn how to calculate p-values first.\n\n\nSay we have two different distributions (lists of scalars). A big question is: what is the difference between the two distributions? We might try finding the difference in means. But there are obvious problems that come along if our sample size isn’t enough. For example, we may have samples from the same distribution that just happened to have different means. So we probe a little bit further.\n\n\nGiven two lists: e.g.: 1. \\([1,2,3,4,4,1,2,3,4,5]\\) 2. \\([23,224,24,2,4,2,3]\\)\nThese numbers were sampled from some distribution(s). The question is: did we get these samples by sampling from the same distribution or two different distributions? That is what the p-value aims to figure out.\nCalculate the means:\n\nmean: \\(2.9\\).\nmean: \\(40.285714285714\\).\n\np-value asks: what is the probability that 1 and 2 means were drawn from the same distribution.\n\np-values are numbers, between \\(\\mathbf{0}\\) and \\(\\mathbf{1}\\), that, in this example, quantify how confident we should be that distribution \\(\\mathbf{1}\\) was drawn from a different distribution than the one distribution \\(\\mathbf{2}\\) was drawn from just based on the observed data and statistic s(mean in this specific example).\n\n\nif \\(p\\) is close to 0, then we are more confident they are different.\n\n\nWatch this video for a more detailed explanation on the problem setup.\n\nSo hiw to calculate? We can use bootstrapping!\n\n\n\n\nWe use bootstrapping here via repeated sampling.\nFirst, take the two distributions and jot down the initial difference between the means (or whatever relevant statistic you are calculating). 1. Concatenate together the two sample lists to make one universal distribution, \\(U\\). 2. Iterate at least \\(10,000\\) times and at each iteration, sample from \\(U\\) (appropriate lengths) twice: one for group A and one for group B. 3. Say you want to calculate the difference in means (called effect size). Take abs(diff between means). Append this to a master list which gets all the effect sizes for the iterations. 4. Also, at each iteration, calculate whether the effect size between the current sample means is greater than the initial effect size. If so, add to a counter. 5. At the end, take the counter value and divide it by the total number of iterations. This is your p-value.\n\n\n\nWe now get into the heart of this lecture and the last topic in uncertainty theory. Recall that expectation is the weighted average of a rich random variable distribution. Also recall an important fact about expectation:\n\nExpectation of sums is sum of expectations: \\[E[X+Y]=E[X]+E[Y].\\]\n\n\nImportantly, the above statement holds regardless of dependency between the components."
  },
  {
    "objectID": "notes/cs109/11-probabilistic-models.html",
    "href": "notes/cs109/11-probabilistic-models.html",
    "title": "Knowledge",
    "section": "",
    "text": "Inverse CDF can be used for drawing samples from a distribution.\nPoisson and normal approximations. Use normal when \\(p\\) and \\(n\\) fall in certain range and Poisson for different values of \\(n\\) and \\(p\\).\n\nProportional: self-explanatory: use this symbol when you want to talk about a number being proportional to something (i.e. scalar multiple for some constant).\n\n\n\n\nWe know that we cannot measure the exact probability for a CRV because we work with densities not probabilities. However, there is a neat property: if we are asked about the ratio/proportion of two probabilities that are continious, we can use PDF’s directly without integrating.\nExample scenario: “How much more likely are you to complete in 10 hours than in 5?”. Let \\(X \\sim N\\left(\\mu=10, \\sigma^{2}=2\\right)\\).  Probability density is point on curve whereas probability is area. How to get area under curve? We need a bit of width to make a rectangle. We call this width value \\(\\epsilon\\) which is infinitestimly small. So we have: \\[\\begin{aligned} \\frac{P(X=10)}{P(X=5)} &=\\frac{\\varepsilon f(X=10)}{\\varepsilon f(X=5)} \\\\ &=\\frac{f(X=10)}{f(X=5)} \\end{aligned}.\\] And bam. Just like that we have a simple function calculation!\n\n\n\n\n\nWe know how to model real world phenomonom using single random variables. But the real power is when we join them together. We can model complex networks from real life and how these nodes of the network interact (e.g. how they depend on each other… conditional probability!) in a probabilistic framework. e.g. .\nExample: webmd. Takes in symptoms and gives possible illnesses. Can think about modelling each symptom/disease a random variable and seeing how they interact jointly to make inferred predictions.\nIn the single variable case, we defined a PMF which was able to give us the probability of the RV taking on a specific value (i.e. an event). In joint distributions, we care about both of them taking on particular values (“and”). JPMF gives you complete information for distribution. \\[P(X=a, Y=b)\\] means “what is the probability that \\(X=a\\) and \\(Y=b\\)?”.\nWe often represent joint distributions using “tables”: \nTheorem: Let \\(X\\) and \\(Y\\) be jointly distributed random variables. Then, by the Kolmogorov axioms: \\[\\sum_{x \\in X} \\sum_{y \\in Y} P(x, y)=1\\]\n\nAssignments in a joint distribution are mutually exclusive (i.e. \\((X=x)\\) must be distinct for different values of \\(x\\)).\nSyntax shortcut: \\(P(x, y) = P(X=x, Y=y)\\).\n\nMarganilization: we often want to know what the probability of \\(X\\) taking on some value is given that we are holding \\(Y\\) as constant. To do this, we loop over all the possible probabilities of \\(X\\) taking on some value with the corresponding constant \\(y\\). This is called marganilization.\nLimitations of the joint: a key realization that we had is that specifying the joint gives us everything we can possibly want to know about the joint model’s RV’s. So why do we have five more lectures on this concept. What is more to it? The problem is, as you add more random variables, the joint continues to increase in size and becomes too large to do anything useful computationally. So we must develop clever “tricks” for working with large joints (i.e conditional probabiliy).\n\nThink about it like this: a 2 RV joint distribution can be represented by a grid (2-dimensional). 3 RV system now requires a cube (an extra dimension). As we grow our model (as we often will do the real world), we will need insane number of dimensions for our (hyper)-“table”.\nExample: Let us say we have a system of 10 RV’s each of which can take on 5 different values. We have \\(5^10\\) different combinations. With only 10 RV’s! Staggering. We need to introduce some nice theory to work with this stuff more usefully (hint hint: conditional probability). That is, we need to introduce probabilisitc graphical models where the nodes represent random variables and the edges denote conditional dependence. Now we’d have one giant probabilistic system. And using these models we could start to infer different things about the system without needed the computationally intractable full joint.\n\n\n\n\n\nOur first “probabilistic model” (i.e. it is parametric) is the multinomial.\n\nWe defined the binomial based on number of heads in consecutive coin flips. We let \\(X\\) be a binomial random variable which represented number of heads. We could have alsod defined \\(Y\\) to be number of tails.\nIn dice rolling, the options aren’t binary. So instead, we define a random variable for each side: how many times does a 3 appear–let this be a random variable and so on.\nSo intead of there being 2 possible outcomes like in binomial, we have \\(n\\) different number of outcomes.\nExample: \nSo now we can answer questions relating to dice roll type experiments using the PDF of a multinomial.\nAt its root, a multinomial is a counting idea (the multinomial coefficient). A good application is language: count number of words etc. Lots of ties to conditional probability (inference!) which is what we will learn about next.\n\n\nDefinition (Multinomial): let’s perform \\(n\\) independent trials of an experiment where each trial results in one of \\(m\\) outcomes, with respective probabilities: \\(p_{1}, p_{2}, \\ldots, p_{m}\\) (constrained so that \\(\\sum_{i} p_{i}=1\\) ). An example of this would be a dice roll. Let \\(n=1\\) (one dice roll) and \\(m\\) is going to be \\(=6\\) because there are \\(6\\) possible outcomes. Now, a multinomial distribution is a closed form function that answers the question: What is the probability that there are \\(c_{i}\\) trials with outcome \\(i\\)? Mathematically: \\[P\\left(X_{1}=c_{1}, X_{2}=c_{2}, \\ldots, X_{m}=c_{m}\\right)=\\left(\\begin{array}{c}n \\\\ c_{1}, c_{2}, \\ldots, c_{m}\\end{array}\\right) \\cdot p_{1}^{c_{1}} \\cdot p_{2}^{c_{2}} \\ldots p_{m}^{c_{m}}. \\] That is, you can imagine assigning a random variable to each of the \\(m\\) outcomes (e.g. dice side six) that happens with probability \\(p_i\\). But we have \\(m\\) different random variables so we have a joitn distribution. And we care about all of them satisfying certain equalities. Example: Let us roll a six-sided \\(m=6\\) dice \\(n=10\\) times. We can specify a random variable \\(X_i\\) for each of the \\(m\\) outcomes (side that the dice lands on) that represents the number of times in \\(n\\) trials that we will see that outcome (e.g. \\(X_6\\) might represent “how many 6’s appear in \\(10\\) dice rolls?”“). Now we are interested in e.g. \\(X_1\\) taking on the constant \\(2\\) and maybe \\(X_4\\) taking on the constant \\(5\\) and so on. This is a joint probabiliy distribution! In the case of dice rolls, each RV has equal parameter \\(p\\) but that won’t be the case always as we will see in the document example (which leads into conditional probability for multiple random variables).\n\nWe can then apply Bayes’ theorem to a multinomial. See Federalist papers example."
  },
  {
    "objectID": "notes/cs109/17-adding-random-variables.html",
    "href": "notes/cs109/17-adding-random-variables.html",
    "title": "Knowledge",
    "section": "",
    "text": "Goals: - Learn how to add random variables from the same parametric distribution. - Derive from first principle how to add random variables more generally.\n\nWe start with a new definition:\n\n\nDefinition (i.i.d. random variables): Consider \\(n\\) random variables \\(X_{1}, X_{2}, \\ldots X_{n}\\). These random variables are considered iid if each random variable has the same probability distribution (same PMF or PDF) as the others and are all mutually independent. This implies expectation and variance are the same.\n\n\n\nSpecial note: by “same PDF/PMF”, we literally mean that the written out form must be identical. Specifically, the must all come from the same parametric distribution family and on top of that, have the exact same parameters.\nWhy is this important: because we may have some real world scenario where we need to have a ton of different RVs but they should all be the same distribution (and so they can all take on different values when the experiment is brought to life).\nMaking this assumption is vital and allows us to solve more complicated problems efficiently. Used in ML a lot!\nExample of iid: Let \\(X_{i} \\sim \\operatorname{Exp}(\\lambda)\\) where each \\(X_{i}\\) is independent of the other random variables.\nExample (1) of non-iid: Let \\(X_{i} \\sim \\operatorname{Exp}\\left(\\lambda_{i}\\right)\\) where each \\(X_{i}\\) is independent of the other random variables. Notice how the RVs have different parameters?! So they are not iid.\nExample (2) of non-iid: Let \\(X_{i} \\sim \\operatorname{Exp}(\\lambda)\\) where \\(X_{1}=X_{2}=\\cdots=X_{n}\\). Not iid because each RV is conditionally dependent on the other. So all RVs that are equal to each other are not in fact not iid.\n\nNow we begin our real study of uncertainty theory. We start out by asking “how do we add random variables”?\n\n\nNote: the variables must be independent to add them. Question: do they need to be from the same family as well then?\nLet us derive from first principles how one might add two random variables. Let \\(X\\) and \\(Y\\) be iid discrete random variables. We want to find \\(\\mathrm{P}(X+Y=n)\\). We start by realizing that this is equivalent to \\(\\mathrm{P}(X=i, Y=n-i)\\) for some \\(i \\in \\mathbb{N}\\). How so? For example, let \\(i=3\\) for \\(\\mathrm{P}(X+Y=n)\\). Plugging in, we obtain this: \\(3+Y=n \\rightarrow Y = n - 3\\). Now we can repeat this idea for any \\(i\\) and obtain the whole PMF for \\(X+Y\\)!:\n\\[\n\\mathrm{P}(X+Y=n)=\\sum_{i=-\\infty}^{\\infty} \\mathrm{P}(X=i, Y=n-i).\n\\]\nThis repeated process is called a convolution (see here and here for more discussion) of discrete random variables. So convolution corresponds to addition which, by extenson, allows us to form linear combinations (linear transforms).\nThe continious version of this is defined as\n\\[\nf(X+Y=n)=\\int_{i=-\\infty}^{\\infty} f(X=n-i, Y=i) d i.\n\\]\n\n\n\nWe are in luck. In practice, we generally won’t need to compute the convolution between two distributions if the RVs are from the families of distributions we know about: these families already have their sums derived!\nSee the course reader for the equations.\nI will add some of my own thoughts and examples here.\n\nExample: here is an intuitive example. We have two groups of people\n\nP1: 50 people, each independently infected with \\(p=0.1\\)\nP2: 100 people, each independently infected with \\(p=0.4\\)\nQuestion: Probability of more than 40 infections?\n\nSolution: your first thought is probably to define two binomial random variables and just add them. Let \\(P_1 \\sim bin(50,0.1)\\) and let \\(P_1 \\sim bin(100,0.4)\\). But we have a problem. We can’t add binomials with different probability parameters (see course reader equation definitions)! So what we do now? We approximate the binomials with normals!!!\n\nThe key point here is that you should try to rearrange your problem to get it in terms of something you know how to work with.\nThough remember to perform continuinty correction when tranforming from the discrete to continious realm.\n\n\nI want to stress that adding RVs is an important problem solving strategy. When given questions of the form above, you can try to break it down and solve it from first principles. But if you just realize that you can split it up into two RVs and sum them up, it becomes easier. This gives us some nostolgia from when we first discussed RVs; why did we introduce them to begin with? Because it made it significantly easier to work with some core probability problems!\nRemark: Say we have some random variable, \\(X\\). What is \\(2X\\)? Hold your horses. It is not like what we did above. In fact, this is an invalid expression. Can you picture why? Because \\(X\\) is not independent from itself! So instead, we must use a linear transform.\nRemark: subtraction is just addition by linear transforming the second part of the expression with -1 first. So \\(X-Y=X+(-1Y)=X+(-Y)\\)."
  },
  {
    "objectID": "notes/cs109/13-inference-part-2.html",
    "href": "notes/cs109/13-inference-part-2.html",
    "title": "Knowledge",
    "section": "",
    "text": "Bayes’ theorem with discrete RV’s is easy: it is just a table with a bunch of lookups. The real complexity arises when you have a continious random variable in the mix because now you are dealing with densities instead of probabilities. We will explore that in the next lecture.\nWe will use the following as a guiding example:\n\nQuestion: At birth, girl elephant weights are distributed as a Gaussian with mean \\(160 \\mathrm{~kg}\\), and standard deviation \\(7 \\mathrm{~kg}\\). At birth, boy elephant weights are distributed as a Gaussian with mean \\(165 \\mathrm{~kg}\\), and standard deviation of \\(3 \\mathrm{~kg}\\). All you know about a newborn elephant is that it is \\(163 \\mathrm{~kg}\\). What is the probability that it is a girl?\n\nThe initial set up using Bayes’ would be: Let \\(G\\) be the event that the elephant is a girl (discrete Bernoulli indicator). Let \\(X\\) be the event that the elephant weighs \\(163\\) kg (continious Gaussian)\n\\[\nP=(G=1|X=163) = \\frac{P(X=163 | G = 1)P(G=1)}{P(X=163)}\n\\]\nNow notice: anything that involves \\(P(X=x)\\) taking on a specific value \\(x\\), we have a problem: a continious random variable cannot take on a specific value! So we must use a trick.\nWe need some width under the curve to multiply the density by and we called it \\(\\epsilon\\). Luckily, by the Bayes’ formula, were gonna be able to cancel them out:\n\\[P(N=n \\mid X=x)=\\frac{P(X=x \\mid N=n) P(N=n)}{P(X=x)},\\] \\[P(N=n \\mid X=x)=\\frac{f(X=x \\mid N=n) \\cdot \\epsilon \\cdot P(N=n)}{f(X=x) \\cdot \\epsilon},\\] \\[P(N=n \\mid X=x)=\\frac{f(X=x \\mid N=n) \\cdot P(N=n)}{f(X=x)}.\\]\nSo recall… anytime you have a continious RV in this formula (not conditioned on, but the one you actually want the probability for), you need to use density not probability.\nHere was a catch: what \\(X\\) is depends on what \\(G\\) is. We had two different gaussian random variables defined:\n\\[\n\\begin{aligned}\n&X \\mid G=1 \\text { is } N\\left(\\mu=160, \\sigma^{2}=7^{2}\\right) \\\\\n&X \\mid G=0 \\text { is } N\\left(\\mu=165, \\sigma^{2}=3^{2}\\right)\n\\end{aligned}\n\\]\nThis causal relationship can be represented graphically (probabilisitc graphical model! inference!):\n\n\n\n|340\n\n\nMuch more efficient and computationally tractable to reason about conditionallity (i.e. answer questions like “What is \\(P(G=1 \\mid X=163)\\)?”) than the entire joint!\nWe will explore this graphical idea in the next lecture. Back to Bayes’!\nCalculating normalization constant:\n\nProblem solving tip: you might be given two Bayes’ with continious RV’s and want to find “likelihood that this is greater than that”. Before deriving the actual formula, you can cancel out the normalizing constant because you are dividing a fraction by a fraction.\nOther times, you’ll actually need to derive the denominator using law of total probability.\nAlso you can get denom. by figuring out what value sums to 1 for all of the possible posteriors by the third axiom."
  },
  {
    "objectID": "notes/cs109/14-modeling.html",
    "href": "notes/cs109/14-modeling.html",
    "title": "Knowledge",
    "section": "",
    "text": "Inference is when you ask conditional probability questions about a joint distriubtion.\nWe also learned how to update our beliefs about a random variable given some result from another random variable (see eye test lecture example).\n\nPosterior belief.\n\nRemember that you cannot have a probability/posterior of a random variable itself: \\(P(X)\\) makes no sense (this is object/function). Instead, you need it to be an event: when your RV takes on a value. So if you want to update a discrete random variable distribution conditioned on another random variable taking on a particular value (i.e. the student got the answer correct), you need to update every event possible for that random variable. In other words, you need to apply Bayes’ to all of \\(P(X=x_1)...P(X=x_n)\\). These posterior beliefs make up your posterior distribution.\nNormalize: sum up all the possible beliefs (sum up \\(P(X=x_1)...P(X=x_n)\\)) to get the normalization constant for the denominator of the Bayes’ theorem formula:\n\nThis is necessary when you are updating your belief about all possible events for the random variable. \\[\\begin{aligned} P(A=a \\mid Y=0) &=\\frac{P(Y=0 \\mid A=a) P(A=a)}{P(Y=0)} \\\\ &=\\frac{P(Y=0 \\mid A=a) P(A=a)}{\\sum_{a} P(Y=0, A=a)} \\\\ &=\\frac{P(Y=0 \\mid A=a) P(A=a)}{\\sum_{a} P(Y=0 \\mid A=a) P(A=a)} \\end{aligned}\\]\nCalled normalize because makes all posterior beliefs correctly sum to 1.\n\n\nNew topics: 1. Probabilistic modeling 2. Correlation 3. Independence with random variables\n\n\n\n\nInference is when you ask conditional probability questions about a joint distriubtion.\nIn WebMD, we might want to know what \\(P(X=1 | U=1)\\) is where \\(X=1\\) means you have a virus and \\(U=1\\) means you are an undergraduate student (or could be a symptom etc.). Asking these questions is inference.\nIdeally, we can be given a joint distribution and answer any question we like. But as we know, we cannot do that cuz the size of the distribution increases rapidly and becomes an \\(np\\)-hard problem (computationally intractable). So we need better “models” for the joint.\nOne way to do this is via a graph: \nUsing the nodes and edges, you can start to calculate conditional probability questions.\nThese are called “Bayesian networks”.\nThe artform of making a BN is describing the hypothetical joint distribution using causality. You might have a node such as “undergrad bernoulli” and another “tired ernoulli”. We know being an undergrad causes you to be tired. This is how we’d form our model.\nOnce we have the directed graph, we can answer inference (conditional probability questions). If you think about it, it makes to condition a random variable event on its parents (e.g. prob. tired conditioned on being an undergrad). \\[\n\\prod P\\left(x_{i} \\mid \\text { Values of parents of } X_{i}\\right).\n\\]\nSo to estimate the full joint, we need to:\n\nConstruct directed graph where each node is a random variable and each edge represents causality (conditioning).\nThen, to approximate the joint, instead of filling in a table of all possible combinations, we’d just need to calculate the values for each node (but this is easier because each node/RV is conditioned on its parents!).\n\nNotes:\n\nOne constraint is that the graphs must be acyclic meaning you cannot have a directed edge pointing in both ways. This makes sense because you cannot have a conditional going in both directions (only one thing can cause another thing). This is an assumption we make to make the math fit into the probabilistic framework. Helpful slides:\n\n\n \n\n\n\n\n\nFor RV’s to be independent, all possible events between the two must be independent.\n\nHow do we measure independence? Covariance!\n\nCovariance: how do we measure how related (how much the vary together) two random variables are? We do it with a metric called covariance: \\[\n\\begin{aligned}\n\\operatorname{Cov}(X, Y) &=E[X Y-E[X] Y-X E[Y]+E[Y] E[X]] \\\\\n&=E[X Y]-E[X] E[Y]-E[X] E[Y]+E[X] E[Y] \\\\\n&=E[X Y]-E[X] E[Y]\n\\end{aligned}\n\\]\n\n\n\n\n|500\n\n\n\nIf \\(X\\) and \\(Y\\) are independent, then \\(\\operatorname{Cov}(X, Y)=0\\). But \\(\\operatorname{Cov}(X, Y)=0\\) doesn’t necessarily imply independence.\nPositive covariance (positive linearly correlated) \nZero covariance\n\n\n\n\n|500\n\n\n\nNegative covariance\n\n\n\n\n|500\n\n\n\n\nYou might be thinking… hmm covariance doesn’t really have a “unit” per say. What does “covariance” even mean besides being just a metric? So people came up with correlation:\nMeasures “linearity”\nCovariance is a quantitative measurement of the relationship between two variables. Correlation between 2 RVs, \\(\\rho(X, Y)\\) is covariance of the two variables normalized by the variance of each variable. This normalization cancels units out and normalizes the measure so that it is always in the range \\([0,1]\\) : \\[\n\\rho(X, Y)=\\frac{\\operatorname{Cov}(X, Y)}{\\sqrt{\\operatorname{Var}(X) \\operatorname{Var}(Y)}}\n\\] Correlation measure linearity between \\(X\\) and \\(Y\\). \\[\n\\begin{array}{ll}\n\\rho(X, Y)=1 & Y=a X+b \\text { where } a=\\sigma_{y} / \\sigma_{x} \\\\\n\\rho(X, Y)=-1 & Y=a X+b \\text { where } a=-\\sigma_{y} / \\sigma_{x} \\\\\n\\rho(X, Y)=0 & \\text { absence of linear relationship }\n\\end{array}\n\\] If \\(\\rho(X, Y)=0\\) we say that \\(X\\) and \\(Y\\) are “uncorrelated.” If two varaibles are independent, then their correlation will be 0 . However, it doesn’t go the other way. A correlation of 0 does not imply independence.\nGiven historical data, we can use correlation and covariance to figure out how to construct our Bayesian networks!"
  },
  {
    "objectID": "notes/cs221/1.html",
    "href": "notes/cs221/1.html",
    "title": "Knowledge",
    "section": "",
    "text": "Date: 9/26/22\n\nCourse content delivered in modules and in lecture (which go over the content from modules in an interactive manner).\nSo during lecture, Percy and Dorsa will walk us through the lecture slides from the module page.\n\n\n\n\n\nHow can we go from code –> self-driving cars? We need a plan of attack. Here we introduce an AI paradigm consisting of three components:\n\nModeling\nInference\nLearning\n\n\n\n\n\n\nModeling: i.e., how we can go from a complex problem (e.g., lane shifting) to a nice, mathematical representation from which we can work with.\n\n\n\n\n\nHowever, modeling is inherently lossy: we lose information due to the complexities of the world. not all of the richness of the real world can be captured, and therefore there is an art of modeling: what does one keep versus ignore?\nInference: answer questions with respect to the model. Example given city model: what is the shortest path? Essentially a mathematical excersize where we use algorithms to solve the problem.\n\n\n\n\n\nLearning: simplified but real world is complex. Not feasible to write full model. But building a model is very hard to accurately represent the real world. So instead, we can show the computer data and the model is learned implicity through such data.\n\ni.e., we’ll write down a model architecture (model “family”), you find the edge values by learning.\n\n\n\n\n\n\nNote: you probably think of neural networks when we talk about modeling. But in actuality, it is much more broad; The idea of learning is not tied to a particular model family (e.g., neural networks). Rather, it is more of a philosophy of how to produce models.\nRest of course: use this paradigm to understand modeling strategies: from low-level to high-level.\nMachine learning: data –> model;\nState-based model: proceduraly: take next step into next state to “search” for a solution.\nVariable-based: assignment: assign bayesian edges in a graph (essentially a PGM).\n\n\n\n\n\n\n\n\n\n\n\nNext module.\n\nTopics in this course have long and complex history.\nTuring test: define intelligence as a machine that can fool a human.\nThree stories of the birth of AI.\nBirth of AI (1956): John Mcarthy organizes Dartmouth conference during the Summer.\n\nProposed challenge to build artificial intelligence with computers. Idea: “Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.”\nProduced some good results like Samuel’s chess engine.\nAI summer: expected to be solved in like 10 years.\nBut of course: underwelmed… no AGI produced!\nProblems:\n\nNot enough compute\nDidn’t understand the complexity of the problem (no np-completeness yet!)\n\n\n1. Symbolic AI: Knowledge-based systems (1970s+):\n\nIdea is to encode explicit domain knowledge in the form of rules:\n\n\nif [premises] then [conclusion]\n\nCommercial success: medical prediction systems\n\nSolved limited compute problem\nBut world is too complex to model completely with this system.\n\n\n2. Neural AI: Neural models (1943, 1980’s):\n\nMcullugh and Pitts: made mathematical model of neurons. What can it learn?\n\nNote: these two were Psychologists well before the age of computing: no intention to build AGI.\n\nLots of advances until in 1969 Minsky proved that XOR was not solvable by linear NN’s. Killed NN’s for the timing being.\nHowever, field picked back up again with the rise of connectionism (explaining cognitive phenominoms using neural networks).\n\n1980: Fukushima’s neocognition CNN architecture.\n1986: Hinton’s backprop.\n1989: Lecun applies backprop learning to Fukushima’s CNN architecture.\n\nDidn’t really pick up speed until 2000s and 2010s when compute (GPUs) came into fruition and networks could be made “deep”er.\n\nAlexNet moment!\n\n\n3. Statistical modeling:\n\nBayesian networks, SVM’s, etc.\nProvide theory\nGoes back to Gauss (before AI)\n\n\nNext module…\n\n\n\n\n\nHigh-level principle: ensure AI is developed to benefit and not harm society.\nHowever, it is tough to operationalize these principles in the real world.\nTwo axes: intent vs. impact.\n\n\n\n\n\nLevels of abstraction so it can get quite complex to see what is right/wrong.\n\nDownstream impact.\n\nAccidents:\n\nGood intent, bad results\n\nComes from gap between model and real-world. We lossy it down to the model and then deploy lossy version in real-world. Lossed information can cause problems!\n“Misalignment between real-world objective and system’s objective.”\n\n\nOptimize wrong objective function\n\ne.g., facebook echo chamber: want good user feeedback but can cause political isolation.\n\nFairness: performance disparities between different groups.\nRobustness: spurious correlations\n\nExample: train classifier for collapsed lung. But all collapsed lung x-ray usually has some chest drain device so the model is really just learning to find the chest-drain rather than a pathological identifier.\n\nSecurity: adversaries (sticker on stop sign for autonomous driving).\n\nThink about the task setup: e.g., don’t make gender classification algorithm.\nFeedback loops: bad algorithm paradigms (as explained above), bad data make for a bad loop.\nData is of course controversial. Dataset biases, imbalanced classes, legal battles over copyright ownership. Labour problems for labeling data.\n\nSo what do we do? - Transparency: document uses, model, etc. - e.g., model cards - Choosing right problems\nSummary:"
  },
  {
    "objectID": "notes/cs221/4.html",
    "href": "notes/cs221/4.html",
    "title": "Knowledge",
    "section": "",
    "text": "Footnotes\n\n\nlingering question: do we define how many clusters there are and where they lie explicity? Or is this learned automatically?↩︎\nrealize that this component must be of the same dimension and space of the input feature vectors. See the visual to understand why.↩︎\ndo we do steps 1 and 2 for each iteration or alternate?↩︎"
  },
  {
    "objectID": "notes/cs221/3.html",
    "href": "notes/cs221/3.html",
    "title": "Knowledge",
    "section": "",
    "text": "Footnotes\n\n\nI don’t think I comprehend this well enough.↩︎"
  },
  {
    "objectID": "notes/cs221/2.html",
    "href": "notes/cs221/2.html",
    "title": "Knowledge",
    "section": "",
    "text": "Footnotes\n\n\nQuestion: when we say “learning” algorithm, what are we referring to? The whole framework? Backprop?↩︎\nis this always the case though?↩︎\nQuestion: I am still confused on the difference between score and margin. Is margin measures correctness, what is the difference between margin and loss then?↩︎\nsee how plotting the loss against the margin allows us to discover the following pitfalls? (motivation for introducing margin).↩︎"
  },
  {
    "objectID": "notes/intlpol268/1.html",
    "href": "notes/intlpol268/1.html",
    "title": "Knowledge",
    "section": "",
    "text": "Date: 9/26/22\nQuestions: - Textbook? Readings? Last year recordings?\n\n\n\nNot too technical\nCybersecurity from offensive pov.\nAim to get working understanding of common attacks (mostly from policy side)\nLectures + sections\n\nMonday: tech lecture (Alex), weekly assignment available\nAlso one discussion section on one of Tuesday, Wednesday, Thursday, Friday\n\nLab (work together)\n\nWednesday: law lecture (Riana)\n\nVirtual machines reset weekly so no late work I guess\nGrading:\n\n10%: discussion attendance\n… (see slides)\n\n\n\n\n\n\nSnowden: exposed the capabilities of the US.\nKey to global competition for power in this day and age.\nWhat can hackers do?\n\nProper taxonomy is tier-ed (not based on who they work for since those lines are fluid), but based on skill level and capabilities.\n{Paste chart}.\n\nLayers of abstraction hide things away\n\ni.e., a hacker hacking upstream for a government would have less chance of getting caught if their position is far away from the actor.\n\nZero-day exploit\n\n\n\nNow a less fine-grained taxonomy… - Superpowers: large orgs, countries, etc. - Examples: five i’s: US, UK, AUS, NZ, CAN - Rapid risers: north korea (lazarus group), vietnam, south korea… - Rapid improving, learn quickly from superpowers - Peleton: lots of people doing this work but mostly private contractors, (e.g., India). - Ambitious …: poor, not many resources\n\n\n\nWhat control does the government have over bad actors? - Full control: In USA: non-authorized hacking is prosecuted. - Less control: e.g., Korea - No control: no laws, e.g., Nigeria\n\n\n\n\nSide-channel attacks:\n\nNot used much but lots of Phd students study this"
  },
  {
    "objectID": "notes/intlpol268/3.html",
    "href": "notes/intlpol268/3.html",
    "title": "Knowledge",
    "section": "",
    "text": "Technical lecture.\n\n\n\nPacket switching in early days –> copper wire connects all the way to the receiver to transmit the signal. Electromechanical.\n\nIdea is to connect your phone wire to a packet center, then the packet center employees connect you to the next packet center and so on. Eventually, you have a wired connection going across the country.\n\n\n\n\n\n\n\n\n🍎 Internet: A network of networks."
  },
  {
    "objectID": "notes/intlpol268/2.html",
    "href": "notes/intlpol268/2.html",
    "title": "Knowledge",
    "section": "",
    "text": "Law lecture, weak 1.\n\n\n\nHow the law & policy side of class is organized:\n\nWe talk about (organized by the week):\n\nWhen somebody hacks somebody else\n\nWeeks 2-5, laws such as CFAA, DMCA; purposes.\n\nWho gets attacked\n\nWeek 6\nPurposes: profit\nLaws such as data security\n\nWhen government hacks their own people\n\nWeek 7-8\nPurpose: law enforcement\nECPA law etc.\n\nWhen countries hack each other\n\nWeek 9\nPurposes: military action\nLaws: international law of conflict\n\n\n\n\n\n\n\n\nSelf-regulation\nTech companies can enforce their own standards\n\nPolice as they wish\n\nInsurance\n\n\n\n\n…\n\n\n\n\nConstitution\ninternational\nstatutory law\nadministrative law\ncase law"
  },
  {
    "objectID": "notes/cs330/1.html",
    "href": "notes/cs330/1.html",
    "title": "Knowledge",
    "section": "",
    "text": "Note: MEL = meta-learning.\n\n\n\nModern DL methods for learning across tasks\nImplementing these methods (MT, TL) in PyTorch\nGlimpse of building new algorithms\n\nlow-level descriptions:\n\nMT, TL\nMeta learning algos\nAdvanced meta learning topics\nUnsupervised pre-training\n\nFS learning\n\nDomain adaption\nLifelong learning\nOpen problems\n\nFocus on DL, with case studies in things like NLP. - No RL! (see CS 224R)\n\n\n\n\nLectures are live-streamed and recorded\ntwo guest lectures\nPrereqs:\n\nSufficient background in ML (229)\n\n\n\n\n50% of grade.\n\n0: multi-task basics\n1: multi-task data processing and BB-ML\n2: gradient-based ML\n3: fine-tuning pre-trained language models\n4 (optional): Bayesian ML and meta overfitting\n\nReplace 15% of hw/project\nNot coding, all math\n\n6 late days\n\n\n\n\n\nPoster session, 50% of grade.\nIdea: …\n\nNow technical content…\n\n\n\n\n\nHow can we enable agents to learn a breadth of skills in the real world?\n\nBecause each time we have to train a supervised signal\n\nSo the goal is to learn representations across tasks\n\n\nAside (common paradigm to learn representations): initialize well (not randomly) –> fine-tune on new task.\n\nThis is harder for RL than NLP because NLP has the entire wikipedia to use but robotic common sense representations are not as straightforward (maybe we need a common robot embedding?)\n\n\nEvolution:\n\nEarly in CV: hand-design features, train SVM on-top\nModern CV: end-to-end training, no hand-engineering\n\nAllows us to handle unstructured inputs without understanding it\n\nNow why meta-learning? Three reasons…\n\nDon’t have large dataset at the outset to pre-train on or use in end-to-end SL manner (med imaging, robotics, etc.)\n\nEven more so: long-tail data samples (e.g., self-driving won’t catch all edge cases)\n\nMEL techniques can help with this (kinda… not the main focus tho)\n\n\nQuickly learn something new (few-shot learning)\n\nLots of open problems\n\n\n\n\n\nWhat is a task?\n\nDataset + loss objective –> model\nObjects as “tasks”\nCritical assumption: different tasks need to share some base structure (goal is to exploit shared structure)\n\nBut lots of tasks share structure (even as upstream as sharing the laws of physics!)\nQuestion: can we learn a shared embedding space for e.g., text + images in one?\n\n\nDoes MT learning reduce to single-task SL learning?\n\nSomewhat (tho not for every problem)\nIdea: sum loss and data:\n\n\n\\[\n\\mathcal{D}=\\bigcup \\mathcal{D}_i \\quad \\mathcal{L}=\\sum \\mathcal{L}_i\n\\]\nNext up: a technical dive into the multi-task learning framework."
  },
  {
    "objectID": "notes/cs330/4.html",
    "href": "notes/cs330/4.html",
    "title": "Knowledge",
    "section": "",
    "text": "(to-do…)"
  },
  {
    "objectID": "notes/cs330/3.html",
    "href": "notes/cs330/3.html",
    "title": "Knowledge",
    "section": "",
    "text": "🍎 Transfer learning: Solve target task 𝒯 after solving source task(s) b 𝒯a by transferring knowledge learned from 𝒯a.\n\n\n\n\nMT: solve multiple tasks (\\(\\mathscr{T}_1, \\cdots, \\mathscr{T}_T\\)) at once: \\(\\min _\\theta \\sum_{i=1}^T \\mathscr{L}_i\\left(\\theta, \\mathscr{D}_i\\right)\\).\nTL: Solve target task 𝒯 after solving source task(s) b 𝒯a by transferring knowledge learned from 𝒯a.\n\nAssumption: cannot access data 𝒟 during transfer. (data is separate).\n\nGood to use TL over MT when\n\nMore data in source task than target task\nDon’t care about solving tasks at the same time\nPre-trained weights for source task are available (e.g., pre-training on imagenet)\n\nIn TL, we don’t need to solve tasks at the same time… just adaption.\nIn this sense: transfer learning is a valid solution to multi-task learning (but not vice versa).\n\nWe can do multiple tasks in TL but MT doesn’t transfer represenations over.\n\n\n\n\n\nMost common way to do TL: pre-training the weights for init purposes. Then use traditional gradient steps to update the parameters.\n\n\n\nIdea is that you can pre-learn just general image features for example and then use this features when training on a new task. This applies for example, even when you pre-train using ImageNet for a medical image task.\n\nQuestion: criterion for when TL will work/helpful?\n\nDifficult, open question, some general common wisdom though (i.e., two medical tasks that are related… not hard rule though).\n\nWhere to get pre-trained params?\n\nImageNet\nLarge language corpora\n\nMathematical perspective: we have non-convex function we need to optimize. By pre-training, we can start in a better “basin” in the loss landscape.\n\n\n\nAbove, we specified the general framework for pre-training/fine-tuning. But there exists many variants to this framework. The general idea is that we want to preserve a good balance of the old data while learning from the new data. In this sense, we can choose, for example, how many layers to freeze when training. Here are some common params:\n\nMotivation: typically you need to adjust the last layer for your task. This means we need to randomly init the last layer. However, during backprop, these random weights will “hit” the earlier layer weights and embed unwanted randomness into it. And for that reason, it might make sense to freeze the earlier layer weights so that we don’t override the valuable general features that we learned earlier.\n\n\n\n\nQuestion: what is the difference between the above paradigm and self-supervised pre-training (e.g., using things like DINO and things like image puzzle occlusion)?\nQuestion: do you need labels to pre-train for a target task that is supervised?\n\n\n\n\n(Note: by common wisdom, we mean the things generally people do when TL’ing: e.g., freeze all layers but last, need diverse source dataset, etc.)\nHere are some recent results:\n\nFrom the paper: “Unsupervised pre-training objectives may not require diverse data for pre-training.”\n\nResult was that you don’t actually need a diverse data set for pre-training (breaks common wisdom).\n\nYoonho’s (TA) recent experiment\n\nWe know that fine-tuning works well for only last layer. But why? last layer is really just like any other layer.\nMaybe for low-level distribution shifts, it might actually be better to only train a first layer and freeze the rest.\nResult: Fine-tuning the first or middle layers can work better than the last layers.\nPaper was released just last week! (Lee, Chen, Tajwar, Kumar, Yao, Liang, Finn. Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. Openreview 09/28/22.).\n\n\nOk… wow lots of design choices then. Despite this, Chelsea recommends:\n\nTrain last layer then fine-tune entire network\n\n\n\n\n\nAnd the idea is that we don’t mess up the earlier learned features by init the last layer to be within scope of the previous ones first before actually fine-tuning.\n\n\nMajor problem: Fine-tuning doesn’t work well with very small target task datasets. This is where meta-learning can help.\n\n\n\n\n\nWe will cover: - Problem formulation - General recipe of meta-learning algorithms\nTwo views: mechanistic and probabilisitc.\n\n\n\nBayesian networks review: many random variables as nodes in a direct graph. \\(P(y | x)\\) (distribution) means \\(y\\) is dependent on \\(x\\) (not vise versa).\nLet’s now draw the graphical model for classic single-task learning.\n\n\n\n\n\nNow multi-task learning:\n\n\n\n\n…"
  },
  {
    "objectID": "notes/cs330/archived/4.html",
    "href": "notes/cs330/archived/4.html",
    "title": "Knowledge",
    "section": "",
    "text": "It is important to note the difference between memory vs. disk. Memory is storage which the CPU accesses and interacts with immediatly. Disk is where things like files store long term. Virtual memory simulates the memory slot but is actually stored in disk so it can benefit from the larger size of the disk.\nThis link has some more information: Memory and disk storage both refer to internal storage space in a computer. Each is used for a different purpose.The term “memory” usually means RAM (Random Access Memory); RAM is hardware that allows the computer to efficiently perform more than one task at a time (i.e., multi-task). The terms “disk space” and “storage” usually refer to hard drive storage. Hard drive storage is typically used for long-term storage of various types of files. Higher capacity hard drives can store larger amounts and sizes of files, such as videos, music, pictures, and documents. “Virtual memory” is hard disk space that has been designated to act like RAM. It assists the computer with multi-tasking when there is not a sufficient amount of RAM for the tasks.\nHere is \\(4+4+5\\) mathematical content:\n\\[\n4+5=9\n\\]\nwow!\n\nIn the last unit, we learned about concurrency: how computer systems can execute multiple programs (processes) at once on one processor. Now we turn to memory management: how computer systems can manage the execution of multiple programs at once with only one main memory. Because of course, this is necessary to have multiple processes running at once.\nIn this section, we aim to look at algorithms and schemes that solve this problem.\n- CPU can only directly access data from registers or main memory. If something from disk needs to be accessed, it must first be moved to main memory.\n\n\nSo what does MM actually look like? Remember, it just a long contigious array of bytes where each byte has an address. It looks like this:\nEach process needs to have its own contigious block of memory that it can access. To help manage this (that is, to protect processes from interfering with each other), there are two registers which are called the base and limit registers which determine the legal range of addresses that a process may access (question: is this because processes might move around and are not static?). The above figure shows an example of the base and the limit ranges (remember MM grows downwards so the base is actually higher up in the figure): process can access bytes addressed between (inclusive) base: 300040 and limit: 120900. This scheme is visually depicted as follows:\n\nImportantly, only OS kernel mode can change the values in these registers: not user programs. So the OS really provides the lubricating jelly here: makes it work for the higher-level processes.\nOS will continiously boot out user processes in concurrency to replace it with another process to execute.\n\n\n\n\nA “program” resides in disk as an executable. Upon execution, program is brought into memory and becomes process. So processes waiting to be brought into MM for execution are stored in an input queue.\nIn standard old-school single-tasking, OS would select one process from the queue and load it into memory. CPU that can access code instructions and data from the memory. Then, process terminates and the memory space is now available for the next process to run.\n\nI need to understand address binding a bit better tbh.\n\n\n\n\nAs we know, a process must be in MM to be executed. However, a process can be swapped temporarily out of memory to something called a backing store (when it pauses execution) and then brought back into MM for continued execution.\nSwapping makes it possible for the aggregate space of all the running processes (logical space) to exceed that of the physical address space of main memory: thus increasing the degree of multiprogramming. This is all visually depicted like so:\nWe will discuss some swapping techniques here.\n\n\n\n\nMost intuitive way (first thing you’d think of):\n\nStandard swapping involves moving processes (their memory images that is: see here) between MM and a backing store (BS). BS is commonly some fast disk. Must be large enough to accomodate copies of all memory images for all users.\nSystem maintains a ready queue consisting of all processes whose memory images are on the abcking store or in memory and are ready to run.\nWhenever CPU scheduler decides to execute a process, it calls the dispatcher. From here, the dispatcher checks to see whether the next process in the queue is in memory. If not, it goes to retrieve it from BS and loads into memory. If there was some previous process in memory, it just does a simple swap.\nReloads registers and transfers control to this new process.\n\nProblems: - Standard swapping is a good conceptual introduction but it is not currently used that often because it requires too much swapping time rather than execution time to be an efficient memory management solution. We will talk about some other solutions below but to get you thinking, what happens if we swap only part of each process rather than the entire thing? This is kind of the basis of the virtual memory which we will cover next.\n\n\n\n\nWe discussed swapping above to control execution of processes during run time. But we can also study how we can effectively actually allocate the process images into main memory. To be more specific, when we swap a process from backing store (disk) to main memory, where should we place the process memory block? How can we do this most efficiently in conjuction with all of the previously placed blocks of memory? This is the idea of contiguous memory allocation which we discuss here. This is a direct mirror of heap allocation techniques discussed in CS 107.\n\nMM must hold both the OS and all other user processes. So we need to be as efficient as possible.\nWe usually place OS in the low memory (the first thing) because of proximity to the interrupt vector which is placed in low-memory.\n\n\n\n\nBefore we even begin to discuss allocation, we should understand the idea of memory protection: that is, how can we prevent a process from accessing memory it does not own?\nVirtualization: we can do this with the relocation (base) and limit registers.\n\nRelocation register: specifies the base physical address of the process.\nLimit register: specifies the range of logical addresses (those functioning per the CPU and user program).\nExample: relocation = 100040 and limit = 746000.\nSo each logical address must now fall within the range specified by the limit register. The MMu does the mapping from the logical addresses dynamically by adding the value to the value in the relocation register.\n\nCPU scheduler selects a process for exeuction and dispatcher loads the correct relocation and limit registers.\nSo when the CPU generates new logical addresses during run time, we do a quick check to ensure that the addresses fall between these ranges set by registers.\nThis helps make the OS (really?) dynamically sized.\n\n\n\n\nOk… now let us get into allocation now that we have an idea for how process memory is protected.\nThe first idea is that of fixed-partitions.\n\nDivide memory into several fixed size “partitions”.\nEach partition may only contain one process.\nWhen a partition if free, a process is selected from the queue and is loaded into this free partition.\nWhen the process terminates, the partition becomes available for the next process.\n\nPretty easy to see limitations and why it is no longer used commonly: fixed size, can only have as many programs running = number of partitions.\nThe next scheme is just about identical to heap allocation techniques from 107: variable-partitioning. We don’t divy up the memory into partitions designated for each process (discretize). Instead, we view the memory as a continious region. At first, the entire free region is one big hole. OS takes processes from input queue according to scheduling algorithm. We try to find region in the holes where we can allocate next process. We do this until we no longer have enough free space for the next process. In which case, the OS can decide to stall until there is enough free memory or jump to the next process small enough in the queue (depending on the scheduling algorithm at play). Here is how the textbook describing this allocation scheme:\n\nIn general, as mentioned, the memory blocks available comprise asetofholes of various sizes scattered throughout memory. When a process arrivesand needs memory, the system searches the set for a hole that is large enoughfor this process. If the hole is too large, it is split into two parts. One part isallocated to the arriving process; the other is returned to the set of holes. Whena process terminates, it releases its block of memory, which is then placed backin the set of holes. If the new hole is adjacent to other holes, these adjacent holesare merged to form one larger hole. At this point, the system may need to checkwhether there are processes waiting for memory and whether this newly freedand recombined memory could satisfy the demands of any of these waitingprocesses.\n\nSound familiar? This is dynamic storage allocation: how can we satisfy a request of size \\(n\\) from a list of free holes?\nIn it of itself, there are many solutions to this scheme which we all know about from 107. Common strategies include:\n\nFirst-fit: traverse list of holes. Allocate the process to the first hole that is big enough. Search can start from beginning of list of holes or where it left off from previous search.\nBest-fit: designate the process to the smallest hole that is big enough for the process to fit. For this, we must search the entire list (unless we order the list by size). Produces smallest leftover hole.\nWorst-fit: traverse list of holes. Allocate the process to the largest hole. Again, must traverse entire list. Produces largest leftover hole. Reasoning is that the leftover hole migfht be more useful for future processes than best-fit small leftover holes.\n\n\n\n\nPitfalls of memory allocation that is inherently bound to happen based on the nature of the problem (John thinks that you can never have a perfect solution that avoids memory fragmentation).\nTwo types (see these notes for more):\n\nInternal fragmentation: an allocated block is larger than what is needed for the process (example: we may only need four bytes but we must start the next sequence of bytes 4 bytes further since we align everything via 8 byte widths). “Caused by the fact that memory is allocated in blocks of a fixed size, whereas the actual memory needed will rarely be that exact size.”.\nExternal fragmentation: no single available block is large enough to satisfy the request, but there is, in aggregate, enough free memory available (e.g. what we described above).\n\nSome stats: - 50-percent rule: one-third of memory may be unusable. Statistical analysis of first fit states that given \\(N\\) allocated blocks, another \\(0.5\\) \\(N\\) blocks will be lost due to fragmentation. - On average, for every request, only \\(1/2\\) of the block will be filled up by the process. So \\(1/2\\) of every block is wasted due to internal fragmentation on average.\nHow to solve these problems? Can never perfectly solve but some ideas we will discuss include segmentation and paging. Basically: - Coalescing: If the programs in memory are relocatable, (using execution-time address binding as previously discussed), then the external fragmentation problem can be reduced via compaction (coalescing at runtime!), i.e. moving all processes down to one end of physical memory. This only involves updating the relocation register for each process, as all internal work is done using logical addresses (not the physical address space). - Non-contigious blocks of physical memory through logical address mapping: Another solution as we will see in upcoming sections is to allow processes to use non-contiguous blocks of physical memory (but “virtually” contigious in logical address space?), with a separate relocation register for each block. - Segmentation and paging (the topic of the next two sections acheive this).\n\n\n\n\nIt is vital that you understand address binding and the differences between logical addresses and physical addresses before moving on (as the next sections begin an introduction to virtual memory which involves mapping logical addresses to physical addresses… everything to this point has just been absolute mappings). There was a section above on it, but I think it makes more sense to discuss it here. So before moving on, I will write some notes on it here. These notes are based on this document.\nAside: os-step book states that the “address space” of a process is an abstraction of physical memory that the program sees. It is the e.g. stack, heap, code, etc. It is not an image of the physical main memory itself.\nThe program has no privalege to view main memory physically. The OS does and handles this (and ensures things like protection so that no process can access another processes memory). The main point I want to stress here is that the program’s view of memory is isolated to its address space which itself is a virtualization of physical memory (not the entire image of physical memory, but rather that of the process block). The memory management unit is then responsible for translating the addresses from the virtual address space to the physical memory slab. So stress this more, in the above figure of a process’s virtual address space, notice that the addresses start at \\(0\\) but this isn’t the actual address of the code segment in memory. The MMU translates this address. Thus, we consider this a virtualization of the memory. Again, what I want to stress is that even basic techniques like base-bound translation are virtual memory because the address space is itself virtual. So we need some form of address translation even in this technique.\n\nAddress uniquely identifies a location in the memory (byte-indexed).\nLogical address: a virtual address and can be viewed by the user.\n\nUsed like a reference, by the user, to access the physical address.\n\nThe physical address, computed by the MMU, is where all the dirty, behind-the-scenes stuff happens.\n\nCannot be accessed by the user.\n\nDifference between logical and physical address is that logical address is generated by CPU during a program execution whereas, the physical address refers to a location in the memory unit.\n\nSee chart here for this differences:\nPause: you might be wondering, wny do we need this whole virtualized mapping scheme? Why not give user processes direct access to the main memory unit? The answer is security. We don’t want applications to be able to overwrite other processes for example. So we let the operating system take this role and it does this by creating a virtual address space for the user and then performs the mapping to the physical address space. To see more about the motivation, see this Stack Overflow thread.\n\n\nLet us take a deeper dive on logical addresses.\n\nIt is an address generated by the CPU for a program: e.g. program local variables such as int x get a virtual address.\nThis address is not a physical memory address. It is used to locate a physical memory address.\nWe call the set of all logical addresses the logical address space.\nThe Memory-Management Unit (a piece of hardware) maps the logical addresses to the corresponding physical addresses.\nDuring compile and load time, the logical addresses are identical to the physical addresses. The real power comes into play during execution time. At run time, the address-binding methods by the MMU generate different logical and physical address.\n\n\n\n\nLet us now discuss physical addresses in more depth.\n\nPhysical Address identifies a physical location in a memory. It is an address to some byte-index.\nThe MMU computes the corresponding physical address for each logical address.\nUser cannot access. Kernel mode OS can though.\nThe set of all physical addresses corresponding to the logical addresses in a Logical address space is called Physical Address Space.\n\nDiagram:\n\n\n\n\n\n\nWe again need to review the beginning parts of this chapter to solidify understanding.\n\nCPU needs to access main memory to execute a process. Process is stored in main memory and its image includes its code instructions, data, etc.\nCPU can only access direct main memory and not hard drive disk. So all programs must be loaded in main memory to execute.\nOnly the OS has access to the physical address space. Logical addresses (for things like int x variables) are generated by the CPU for each program.\nUser processes can only access memory (via logical addresses) that belongs to it. What designates the memory a process can access (again not physically but by means of virtual logical addresses) is based on two registers which define a range of addresses that the process can access: a base register and a limit register.\nEvery memory access made by a user process is checked against these two registers, and if a memory access is attempted outside the valid range, then a fatal error is generated.\nOS has privaleged access to all memory locations as they perform job of swapping. Changing of base and limit registers (dynamically changing size of process?) is only allowed by the OS as well.\n\nNow let us talk about address binding (see this link and this one too) a bit:\n\nThe process of associating program instructions and data to physical memory addresses is called address binding. That is, how can we find the addresses for which the program should live during execution?\n\n\nUser programs typically refer to memory addresses with symbolic names such as “i”, “count”, and “averageTemperature”. These symbolic names must be mapped or ‘bound’ to physical memory addresses. This occurs in several stages during the living time of the program:\n\n\nCompile time: can generate address for the process to live (load address) during compile time: if it is known at compile time where a program will reside in physical memory, then absolute code can be generated by the compiler, containing actual physical addresses. However if the load address changes at some later time, then the program will have to be recompiled (boo!).\nLoad time: say that the location at which a program will be loaded is not known at compile time. In this case, he compiler must generate relocatable code. In this case, all of the addresses are simply in reference to the start of the program (so that when the load address becomes known, the physical addresses can be calculated by adding the referenced addresses to the load address). So in this case, if that starting address changes, then the program must be reloaded but not recompiled.\nExecution time: sometimes we may actually want to move around a program/process in memory during the course of its execution. In this case, binding must be delayed until execution time. This requires special hardware (i.e. memory management unit), and is the method implemented by most modern OSes.\n\nSo we now understand that address binding can occur in one of the three stages above. Addresses bound at compile time or load time have identical logical and physical addresses. Addresses created at execution time, however, have different logical and physical addresses. This mapping is performed by the MMU at run-time (how amazing is that! on the fly!). How exactly does the MMU do this? That is what we will discuss next chapter but for now:\n\nThe MMU can take on many forms. One of the simplest is just to modify the base-register scheme described earlier. i.e.:\nThe base register is now termed a relocation register, whose value is added to every memory request at the hardware level. We can modify the location of all the addresses by changing the relocation (base) address on the fly and then computing the new addresses by adding the relocation address to it (because everything is in reference the base address (I think?)).\nNote that user programs never see physical addresses. User programs work entirely in logical address space, and any memory references or manipulations are done using purely logical addresses. Only when the address gets sent to the physical memory chips is the physical memory address generated.\n\nAlright… back to the action.\n\n\n\n\nWe left off with how we can mitigate the effects of fragmentation in contigious memory allocation. We mentioned that we can coalesce blocks of free memory if the addresses are relocatable. However, another solution is to allow processes to use non-contiguous blocks of physical memory (but “virtually” contigious in logical address space?), with a separate relocation register for each block. There are two approaches to this and segmentation is the first one we will discuss. This begins the real intro to “virtual memory”.\n(The abstraction)\nThe basic idea behind segmentation is to model how a programmer would think. We programmers tend to view memory split up between things like the stack, heap, code, global variables, and libraries. But main memory is just a contigious sequence of bytes. But remember, user’s only work with a logical/virtual interpretation of this. So the idea of segementation is that the logical address space can be split up into segments** where each segment corresponds to some semantic meaning (e.g. heap)**.\n\nActually, I don’t think this is the case. You should view segmentation as an extension/follow-up to the base-and-bounds approach. Instead of giving each process a base and a bound. We are going to divide the processes memories up into segments and then these itself are like base and bounds (so we have an array of base and bounds). The idea is that we can move around these smaller blocks more coherently, reduces fragmentation, easier to compact, but the main conveniance is the ability to allow the process memory to be physically discontinious. A note: internal fragmentation cannot occur because the segments are allocated based on how the logical memory is divided up. However, external fragmentation can still occur if for example, the blocks don’t fit into a space.\n\nSo now, the logical address is a two tuple:\n\\[\n<\\text { segment-number, offset }>\\text { }\n\\]\nwhere offset is distance in bytes from base address of the segment.\n\n\n(The physical realization)\nAlthough the programmer can now refer to objects in the program by a two-dimensional address, the actual physical memory is still, of course, a one dimensional sequence of bytes. So what is being done on the physical side of things?\n\nWe must define an implementation to map two-dimensional programmer-defined addresses into one-dimensional physical addresses. Going from 2d to 1d!\nHow does this mapping work?\nDone by the segment table.\nEach entry (one entry per segment that is) in the segment table has a segment base and segment limit.\nSegment base: contains the starting physical address where the segment resides in memory (physical memory).\nSegment limit: specifies the length of the segment\n\nThe use of a segment table is illustrated in the above figure. - A logical address consists of two parts: a segment number,\\(s\\),and an offset into that segment,\\(d\\). - The segment number is used as an index to the segment table (like an array index, to get the correct segment). - The offset \\(d\\) of the logical address must be between 0 and the segment limit. We check for this, and if it is not, we trap (send error) to the operating system (logical addressing attempt beyond end of segment… segfault!). When an offset is legal, it is added to the segment base to produce the address in physical memory of the desired byte. The segment table is thus essentially an array of base–limit register pairs. - So I think… from my understanding, that this means multiple processes can use these sections in a shared fashion and so the process block is not contigious. Is this correct? Like each new process comes in and divides itself by the semantic segments (e.g. puts some of its memory in the heap, has its functions in the stack area, etc.). New process does the exact same thing. The segments are shared between processes.\nPictoral example:\nSee how we now have physical “segments” in the physical memory rather than process blocks?\nSome auxillary notes from the course lecture notes:\n\nEach segment also has a protection bit for each segment that denotes whether it is read-write versus read-only.\nSegmentation table (map as its called in the course notes) is itself stored in the MMU.\nTable is modified during context switches\n\nSee Ed question here. Basically, not all segments are shared between processes. Some might have function segments unique to a process so it takes up its own segment. In these cases, some of the segments are swapped out of the table and physical memory during context switching.\n\n\nIn practice, we the two-tuple referencing scheme can be stored within bitwise semantics: Top bits of address select segment, low bits indiciate the offset.\n\n\n\n\nPros: Flexibility. - Permits the physical address space of a process to be non-contiguous (main motivation) - Each segment is managed independently. - e.g. stack doesn’t interfere with text segment (unless you get a pesky segfault). - Grow/shrink independently - Swap to disk independently when needed - Can share segments between processes - e.g. helpful if you want to do something like share code - Can compact/coalesce memory to reduce fragmentation\nCons: - Limited to fixed number of segments - Fragmentation - But how? - Interestingly, fragmentation can occur on the backing store during swaps (because colaselcing isn’t possible on backing store since CPU cannot access it) - Maybe internal fragmentation (like some processes won’t have enough code to fill up the text segment) - Virtual address space is rigidly divided\n\n\n\n\nWe now introduce the most used method for memory management in today’s systems: paging. It is so good because it is virtual and it solves the problems of the other virtual memory manangement solution (segmentation).\n\nWe saw that segmentation permits the physical address space of a process to be non-contiguous. Paging also offers this pro.\nThe difference, is that paging avoids external fragmentation and also avoids the need to compact/coalesce.\nAnother pro is that it solves the considerable problem of fitting memory chunks of varying sizes onto the backing store. Most other schemes cause fragmentation on the backing store itself (when needed to swap, it is essentially the same problem (allocation) but in a different direction lol).\nSo… what is paging, how do we accomplish and implement it? Well, we are going to need some support from the hardware and it is implemented in the OS.\n\n\n\n\nPaging is a memory management scheme that allows process’s physical memory to be discontinuous, and which eliminates problems with fragmentation by allocating memory in equal sized blocks known as pages.\nIdea is to break up physical memory into fixed-size blocks called frames and break logical memory into blocks of the same size called pages.\nSo the idea is that when a process is to be executed, during the loading process (I think?) its pages fill in the available memory frames from their source (backing store or physical file).\nNot only that, the backing store itself is divided into the same fixed-sized blocks that are the same size of the frames.\nIt is easy to see that the logical address space (pages) is now completely separate from the physical address space. Because of this, we can simulate to the user that there is more memory than there might be physically available.\n\nIn summary from above: - Paging involves breaking physical memory into equal fixed-sized blocks called frames. - Then, the logical address space (virtual memory displayed to the programmer) is itself broken up into the same fixed-sized blocks called pages. - When process executed, load pages into available frames. - Advantages: allows process’s physical memory to be discontinuous, more virtual memory than physical memory, avoids fragmentation\nHardware support: - When CPU generates a logical address, it is divided into two parts: - Page number (p) - Page offset (d) - We use the page number to index into a page table. - The page table is table containing the base address of each page in physical memory - This base address is combined with the page offset to define the physical memory address that is sent to the memory unit.\n\n\nLet us talk about the size of the pages. Dependent on hardware. Typically by powers of 2 because then translation from logical –> physical is easy because we can user high-order bits.\n\nSay that size of logical address space is \\(2^m\\) bytes and we have page size as \\(2^n\\) bytes. This means that the offset itself will only take up \\(n\\) bits. So we can use the remaining \\(m-n\\) bits to store the page number and use those \\(n\\) bits to store the offset into that page.\n\nSee textbook for example of how to index into a page table to get the physical address. The general idea is conveyed through the following visual:\n\n\n\n\nImportantly, there is no external fragmentation: any free frame can be allocated to some process that may need it.\nThough internal fragmentation can still occur: but this only ahappens on the last page possible because the memory will be divided into the fixed-size page blocks. For example: > Notice that frames are allocated as units. If the memory requirements of a process do not happen to coincide with page boundaries, the last frame allocated may not be completely full. For example, if page size is 2,048 bytes, a process of 72,766 bytes will need 35 pages plus 1,086 bytes. It will be allocated 36 frames, resulting in internal fragmentation of 2,048 − 1,086 = 962 bytes. In the worst case, a process would need n pages plus 1 byte. It would be allocated n + 1 frames, resulting in internal fragmentation of almost an entire frame.\nBecause of this, on avg, expect 1/2 of page of internal fragmentation per process.\n\nSo we should maybe try to make pages as small as possible then? Maybe but maybe not: overhead of keeping that many pages increases. For example, disk i/o (backing store swapping) is faster with larger pages because less pages have to be transferred over.\n\nIn today OS, we generally use 4 KB page sizes.\nI think the general idea is that once a process requests memory (i.e. it begins its execution and the code is loaded into memory), it builds a page table for that process, necessary free frames are allocated and hold the processes memory (the frames that are free are stored in a free-frame list) –> then these frames are inserted into the initialized page table corresponding to that process.\nThese notes say\n\n“Processes are blocked from accessing anyone else’s memory because all of their memory requests are mapped through their page table. There is no way for them to generate an address that maps into any other process’s memory space.”\nBut I am still confused on how this provides protection? Why can’t a program/cpu generate a bit address whose page number bits go to a page beyond what is stored in the page table?\n\nThe operating system must keep track of each individual process’s page table, updating it whenever the process’s pages get moved in and out of memory, and applying the correct page table when processing system calls for a particular process (?). This all increases the overhead involved when swapping processes in and out of the CPU. (The currently active page table must be updated to reflect the process that is currently running.)\n\n\n\n\n\nOk… so we know that each process has a page table. And we know that the OS is responsible for all of this handling. But how does it actually know what frames are available at what time and when to give some of the frames to pages of processes? It must manage all of this and it does so through a data structure called the frame table.\nFT has entries where each one corresponds to a physical frame in memory and indicates:\n\nWhether the frame is free or not\nIf not free, which page of which process has the spot.\n\n\n\n\n\n\n\nPage lookups must be done for every memory reference, and whenever a process gets swapped in or out of the CPU, its page table must be swapped in and out too (in the MMU?), along with the instruction registers, etc. It is therefore appropriate to provide hardware support for this operation, in order to make it as fast as possible and to make process switches as fast as possible also.\n\nWait: note that the page map itself is stored in continuous physical memory. The PTBR (see below) determines which process page table we would point to.\n\nSome OS have one page table per process and might for example, store it in the PCB. Others might share page tables across processes to reduce the overhead.\nHow can we implement page table in hardware though?\nOne idea is to dedicate registers for page tables. But this of course is only possible when we have a small number of pages as registers are limited.\nBut of course, today’s page tables are massive (> 1 million entries). So the register approach is infeasible.\nSo another idea is: keep the page table itself in main memory and have a special register called the page-table base register (PTBR), point (i.e. store a pointer) to the page table in memory. So whenever we need to change the page table, we just have to change the pointer in this one singular register! Context switching time is reduced.\nBut… timing is still inefficient: because every memory access now requires two memory accesses - One to fetch the frame number from memory (i.e. go to page table) and then another one to access the desired memory location given by the table. So time efficiency is reduced by power of 2 (not going to work in high circumstances).\nSo we got a problem and how we gonna fix it?\nPresent day solution is to use a special, small, fast-lookup hardware cache called a translation look-aside buffer (TLB).\n\n\n\n\nBasically, we create a small hardware cache (buffer array/dictionary) of recent translations.\nEach cache entry stores two things:\n\nThe page number portion from a virtual address (i.e. the page that the CPU wants) is stored as a key and the corresponding physical page/frame number from physical memory is stored as the value.\n\nSo on each memory reference, compare the page number from the virtual address with the virtual page numbers in every TLB entry (which is neatly done in parallel so way more efficient than just a straight memory reference). If the page number is found, its frame number is immediately available and is used to access memory. No memory access needed!\nBut… if the page number is not in the TLB (known as a TLB miss), a memory reference to the page table must be made :(.\n\nAt this point, we swap one entry from the TLB with these new translation so that it will be found quickly on the next reference. (optional) Different replacement policies can be used at this step (i.e. least recently used is the one that is swapped out). Also we can “wire down” certain TLB entries that cannot be swapped out such as those belonging to the OS.\nYou might be asking how a memory reference to a page table is actually made if we don’t just swap the page table in and out for each process like we had before this sub section. Basically, each page table for each process is stored in the PCB which itself is stored in the OS’s memory. The PTBR register holds a pointer to this place. So on a TLB miss, we need to use the pointer stored in the PTBR and access the page table their (1 memory reference) and then actually get the corresponding page/frame wanting to be addressed (another 1 memory reference).\n\nSo on a miss, the process is still the same amount of time (two memory references: PTBR ptr –> page table –> frame). But on a hit, only one memory reference needed!\nBut luckily, computers usually have a > 95% hit ratio. This makes sense. In programming, we define some things like an array that we continue to access throughout the course of the program. So it is best if we keep this memory translation in a cache. The TLB supplies such functionality.\nIf you are paying close attention, you might notice on problem with protection: if the TLB remains the same on context switching, then doesn’t this mean we give the opportunity to a program to maliciously get some random page from the cache buffer and use it as if it was their own? To protect against this, some TLBs store address-space identifiers, ASIDs, to keep track of which process “owns” a particular entry in the TLB. This enables entries from multiple processes to be stored simultaneously in the TLB without granting one process access to some other process’s memory location. Without this feature the TLB has to be flushed clean with every process switch if protection is wanted (which it is).\nSide note: we aren’t actually learning about ASIDs. Instead, for every context switch, we invalidate the entire TLB by setting each entry of the TLB to be invalid (cannot be accessed).\n\nQuestion: So for every context switch, we invalidate the entire TLB? How do we share pages between processes then from the TLB? Also, isn’t this inefficient?\n\n\n\nVisual:\nSome various notes: - Lots of math involved with the bit calculations. To determine how mnay bits we need to address a page table of size \\(2^n\\) bytes, we need \\(n\\) bits.\n\n\n\n\n\nWe can also provide extra protection via the page table. We can allocate a few more bits in the page table for each entry.\nWe can use these protection bits to specify whether the page can e.g. be read-only, write-only, execute-only, or any combination.\n\nCause OS to issue trap is CPU trying to reference a page to which it shouldn’t (i.e. trying to write to a read-only page).\n\n\nImportantly, we can also provide a valid–invalid bit.\nWhen this bit is set to valid,the associated page is in th eprocess’s logical address space and is thus a legal (or valid) page. When the bit is set to invalid, the page is not in the process’s logical address space. Illegal addresses are trapped by use of the valid–invalid bit. The operating system sets this bit for each page to allow or disallow access to the page.\n\nExample (figure 7.15):\n\nNotice that process doesn’t have access to pages 6, 7. So if the CPU somehow generates some address corresponding to that page, OS will issue a trap due to the valid-invalid bit at those entries for this process’s page table.\n\n\n\n\n\n\nMain difference is that paging is fixed sized blocks and segmentation is variable sized. Paging has only internal fragmentation whereas segmentation has only external fragmentation.\nNote: make pros and cons list.\nSegmentation pros: - No internal fragmentaion\nPaging pros: - No external fragmentation - Virtual memory can be larger than physical memory"
  },
  {
    "objectID": "notes/cs330/2.html",
    "href": "notes/cs330/2.html",
    "title": "Knowledge",
    "section": "",
    "text": "Footnotes\n\n\nQuestion: can we treat the task weights as actual parameters (not hyperparameters) that we update via the optimization process (either explicitly or implicitly)?↩︎"
  },
  {
    "objectID": "notes/biology/qpi.html",
    "href": "notes/biology/qpi.html",
    "title": "Knowledge",
    "section": "",
    "text": "Footnotes\n\n\nWikipedia page.↩︎\nWikipedia page.↩︎\nWikipedia page.↩︎"
  },
  {
    "objectID": "pages/papers.html",
    "href": "pages/papers.html",
    "title": "Papers",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nOct 4, 2022\n\n\nProtein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models\n\n\n0 min\n\n\n\n\nOct 11, 2022\n\n\nUnsupervised Segmentation in Real-World Images via Spelke Object Inference\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/cs221.html",
    "href": "pages/cs221.html",
    "title": "CS 221 Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\nWeek 1, Lecture 1 - Introduction\n\n\n5 min\n\n\n\n\n\n\nWeek 1, Lecture 2 - Machine Learning I\n\n\n10 min\n\n\n\n\n\n\nWeek 2, Lecture 1 - Machine Learning II\n\n\n4 min\n\n\n\n\n\n\nWeek 2, Lecture 1 - Machine Learning III\n\n\n6 min\n\n\n\n\n\n\nWeek 3, Lecture 1 - Search I\n\n\n11 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/basic-math.html",
    "href": "pages/basic-math.html",
    "title": "Basic math",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\nSecond Derivative Test\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/intlpol268.html",
    "href": "pages/intlpol268.html",
    "title": "INTLPOL 268 Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\nWeek 1, Lecture 1 - Introduction\n\n\n1 min\n\n\n\n\n\n\nWeek 1, Lecture 2 - Law Introduction\n\n\n1 min\n\n\n\n\n\n\nWeek 2, Lecture 1 - Internet\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/biology.html",
    "href": "pages/biology.html",
    "title": "Biology",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\nQuantitative Phase Imaging\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/cs109.html",
    "href": "pages/cs109.html",
    "title": "CS 109 Notes",
    "section": "",
    "text": "Note: credit for most figures and lots of paragraphs goes to CS 109.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\nLecture 11 - Probablistic models\n\n\n5 min\n\n\n\n\n\n\nLecture 12: Inference part I\n\n\n2 min\n\n\n\n\n\n\nLecture 13: Inference part II\n\n\n2 min\n\n\n\n\n\n\nLecture 14 - Modeling\n\n\n4 min\n\n\n\n\n\n\nLecture 15: General Inference\n\n\n3 min\n\n\n\n\n\n\nLecture 16 - Beta\n\n\n10 min\n\n\n\n\n\n\nLecture 17: Adding Random Variables\n\n\n4 min\n\n\n\n\n\n\nLecture 18: Central Limit Theorem\n\n\n1 min\n\n\n\n\n\n\nLecture 19: Bootstrapping\n\n\n7 min\n\n\n\n\n\n\nLecture 20: Algorithmic Analysis\n\n\n2 min\n\n\n\n\n\n\nLecture 24: Logistic Regression\n\n\n4 min\n\n\n\n\n\n\nLecture 25: Ethics in Machine Learning\n\n\n0 min\n\n\n\n\n\n\nLecture 26: Deep Learning\n\n\n0 min\n\n\n\n\n\n\nReference\n\n\n5 min\n\n\n\n\n\n\n🌋 Lecture 21: Maximum Likelihood Estimation\n\n\n5 min\n\n\n\n\n\n\n🐻 Lecture 23: Naïve Bayes\n\n\n0 min\n\n\n\n\n\n\n🛶 Lecture 22: Maximum A Posteriori\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/cs330.html",
    "href": "pages/cs330.html",
    "title": "CS 330 Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\nWeek 1, Lecture 1 - Course Introduction\n\n\n2 min\n\n\n\n\n\n\nWeek 1, Lecture 2 - Multi-task learning\n\n\n2 min\n\n\n\n\n\n\nWeek 2, Lecture 1 - Transfer Learning + Start of Meta-Learning\n\n\n4 min\n\n\n\n\n\n\nWeek 2, Lecture 2 - Black-box meta-learning & in-context learning\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/cs221/5.html",
    "href": "notes/cs221/5.html",
    "title": "Knowledge",
    "section": "",
    "text": "Footnotes\n\n\ncan’t this vary though depending on the depth that we are at?↩︎\nthis site actually has some good info.↩︎\nnote that this is an important thing to understand as it gives rise to the motivation for \\(A^*\\). In UCS, we uniformly align the nodes in order. However, as we will see, \\(A^*\\) has a bias knowing the end goal.↩︎"
  }
]