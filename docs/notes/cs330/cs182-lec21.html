<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Knowledge – cs182-lec21</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Knowledge</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html">Stanford Notes</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../machine-learning.html">Machine Learning</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../concepts.html">Concepts</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#berkeley-cs-182-lecture-21---meta-learning" id="toc-berkeley-cs-182-lecture-21---meta-learning" class="nav-link active" data-scroll-target="#berkeley-cs-182-lecture-21---meta-learning">Berkeley CS 182, Lecture 21 - Meta Learning</a>
  <ul class="collapse">
  <li><a href="#introduction-and-black-box-meta-learning" id="toc-introduction-and-black-box-meta-learning" class="nav-link" data-scroll-target="#introduction-and-black-box-meta-learning">Introduction (and black-box meta-learning)</a>
  <ul class="collapse">
  <li><a href="#generic-learning-vs.-generic-meta-learning" id="toc-generic-learning-vs.-generic-meta-learning" class="nav-link" data-scroll-target="#generic-learning-vs.-generic-meta-learning">Generic learning vs.&nbsp;generic meta-learning</a></li>
  <li><a href="#meta-learning-methods" id="toc-meta-learning-methods" class="nav-link" data-scroll-target="#meta-learning-methods">Meta-learning methods</a></li>
  </ul></li>
  <li><a href="#non-parametric-meta-learning" id="toc-non-parametric-meta-learning" class="nav-link" data-scroll-target="#non-parametric-meta-learning">Non-parametric meta-learning</a>
  <ul class="collapse">
  <li><a href="#high-level-idea" id="toc-high-level-idea" class="nav-link" data-scroll-target="#high-level-idea">High-level idea</a></li>
  <li><a href="#matching-networks" id="toc-matching-networks" class="nav-link" data-scroll-target="#matching-networks">Matching networks</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="berkeley-cs-182-lecture-21---meta-learning" class="level1">
<h1>Berkeley CS 182, Lecture 21 - Meta Learning</h1>
<section id="introduction-and-black-box-meta-learning" class="level2">
<h2 class="anchored" data-anchor-id="introduction-and-black-box-meta-learning">Introduction (and black-box meta-learning)</h2>
<ul>
<li>meta learning is a new concept in machine learning that allows a general representation of a model to adapt to new tasks very quickly.</li>
<li>The main idea behind metal learning is to learn how to learn. And what we mean by this is that we can represent tasks that are fundamentally different as our training samples in a large dataset, so for example, we might have a cats versus dogs task and also a hot dog versus burger classification task. But if you think about this deeply, you understand that a human who is able to learn how to classify cats versus dogs might be able to use that general representation and some of that to also classify hotdogs vs.&nbsp;burgers, it’ll be able to adapt to the hotdogs and burgers example much quicker than if it had not seen the cats versus dogs example in the first place and this is important because a lot of tasks in the world share general representations that underly the actual task. But most of these tasks do not have a lot of data. So the idea is that we have a lot of images of cats and dogs but little bit of burgers and hotdogs. We can learn the general representation that underlies the task of classifying images and apply it to new datasets that are smaller.</li>
<li>We will cover three different meta-learning paradigms, but first let’s introduce the high-level concept (explained through the first paradigm: black-box meta-learning).</li>
<li>In traditional SL, we have:</li>
</ul>
<p><span class="math display">\[
f(x) \rightarrow y
\]</span></p>
<p>where <span class="math inline">\(x\)</span> is, for example, an image and <span class="math inline">\(y\)</span> is a classification label. But in meta-learning, we have</p>
<p><span class="math display">\[
f\left(\mathcal{D}^{\operatorname{tr}}, x\right) \rightarrow y
\]</span></p>
<p>where <span class="math inline">\(\mathcal{D}^{\operatorname{tr}}\)</span> is a training dataset for some task <span class="math inline">\(T_i\)</span>, where the entire dataset for <span class="math inline">\(f\)</span> is composed of tasks <span class="math inline">\(T_1, \ldots, T_n\)</span>.</p>
<p align="center">
<img alt="picture 2" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/1a33e70436396b94253295f40a0aacbad956e63e6320ae9eec5844dde710ea23.png" width="500">
</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>💡 Underlying question: how do we read in a task training set into some neural network?</p>
</div>
</div>
</div>
<ul>
<li>For traditional SL, we’d, for example, flatten the image pixels into one long sequence and pass this into the model (i.e., an MLP).</li>
<li>So we can do the same here: concatenate and flatten the entire training set as a sequence and use some sequence-based model like an LSTM.
<ul>
<li>Note: we’d concatenate the labels to the model (one-hot vector represented) as well.
<ul>
<li>Zero-out the query labels though.</li>
</ul></li>
</ul></li>
<li>Visually, we’d pass in this entire sequence for each <span class="math inline">\(\mathcal{D}^{\operatorname{tr}}\)</span>:</li>
</ul>
<p align="center">
<img alt="picture 3" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/d431e54b96994a769b2e35ffabc073d603d9d2485d36905984b1cec898fbb4e4.png" width="300">
</p>
<section id="generic-learning-vs.-generic-meta-learning" class="level3">
<h3 class="anchored" data-anchor-id="generic-learning-vs.-generic-meta-learning">Generic learning vs.&nbsp;generic meta-learning</h3>
<p>The following describes the same concept, just in two different perspectives. Above, we thought of this whole task as a traditional supervised learning problem where the inputs themselves are just tasks in the form of <span class="math inline">\(f\left(\mathcal{D}^{\operatorname{tr}}, x\right) \rightarrow y\)</span>. That is,</p>
<ul>
<li><strong>“Generic” supervised learning</strong> (in the context of meta-learning): From here, the training process is the same as with traditional supervised learning. Just train <span class="math inline">\(f\left(\mathcal{D}^{\operatorname{tr}}, x\right) \rightarrow y\)</span>!</li>
</ul>
<p>However, we can also view this entire process in a different light, which we call “<strong>Generic meta-learning</strong>”.</p>
<p>Here is a box containing the mathematical representation of the two points.</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Generic learning</strong>: <span class="math display">\[\begin{aligned} \theta^{\star} &amp;=\arg \min _\theta \mathcal{L}\left(\theta, \mathcal{D}^{\operatorname{tr}}\right) \\ &amp;=f_{\text {learn }}\left(\mathcal{D}^{\operatorname{tr}}\right) \end{aligned}\]</span></p>
<p><strong>Generic meta-learning</strong>: <span class="math display">\[
\theta^{\star}=\arg \min _\theta \sum_{i=1}^n \mathcal{L}\left(\phi_i, \mathcal{D}_i^{\mathrm{ts}}\right)
\]</span> where <span class="math inline">\(\phi_i=f_\theta\left(\mathcal{D}_i^{\mathrm{tr}}\right)\)</span>.</p>
Visually,
<p align="center">
<img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/59a1a320fbc4f34a363a9cc78737824166bec964d90be1361c3d1f90c732f1d0.png" width="500">
</p>
</div>
</div>
</div>
<p>Let’s break the second view down further.</p>
<ul>
<li>Flatten entire task into one long sequence composed of <span class="math inline">\(\mathcal{D}^{\mathrm{tr}}_i = [(x_1, y_1),...(x_n, y_n)]\)</span>.</li>
<li>Input this into some sequence model (i.e., an RNN). The RNN is notated as <span class="math inline">\(f_\theta\)</span> with parameters <span class="math inline">\(\theta^{\star}\)</span>.
<ul>
<li>You can think of the RNN as a learnable feature extractor.</li>
</ul></li>
<li>This RNN produces the last hidden state <span class="math inline">\(h_i\)</span>. <span class="math inline">\(h_i\)</span> contains the updates for <span class="math inline">\(p_{\phi_i}(y \mid x)\)</span>, which is a task-specific head (e.g., MLP classifier).</li>
<li>So we have <span class="math inline">\(\phi_i=f_\theta\left(\mathcal{D}_i^{\mathrm{tr}}\right)\)</span> and <span class="math inline">\(\phi_i=\left[h_i, \theta_p\right]\)</span>. Essentially, the RNN updates the classifier head’s parameters (which are separate from the overall RNN model parameters). We explicitly train to update the RNN model parameters, but implicity this <em>meta-learns</em> the MLP head (<span class="math inline">\(p_{\phi_i}(y \mid x)\)</span>) parameters as well!</li>
</ul>
<p>In practice, we’d set this up very mechanicalistically: just a sequence + label problem. Just to re-iterate:</p>
<ul>
<li>We pass in a flattened sequence of <span class="math inline">\(\mathcal{D}^{\mathrm{tr}}_i\)</span>, concatenated with the unlabeled test points (all of them). The desired output is the labels for those test points.</li>
</ul>
</section>
<section id="meta-learning-methods" class="level3">
<h3 class="anchored" data-anchor-id="meta-learning-methods">Meta-learning methods</h3>
<p>Can group them into the following three-categories:</p>
<ol type="1">
<li><strong>Black-box meta-learning</strong>
<ul>
<li>recasting the meta-learning problem as a meta supervised learning problem. What we just described in the above section.</li>
<li>Generally done via sequence-based NN’s that read in <span class="math inline">\(\mathcal{D}^{\mathrm{tr}}\)</span> entirely.</li>
<li>CS 330, hw 1</li>
<li>Note that the entire training set for a task can get very large, very quickly (<span class="math inline">\(n * k\)</span>). Thus, advanced sequence models have been developed for this. For example, “<em>Meta-Learning with Memory-Augmented Neural Networks</em>” (MANN), which uses external memory to keep the entire dataset in-place. Again, what we implemented in hw 1.</li>
<li>Think of it this way: optimization process is the same, but the (meta)-model is different. Can we reverse this?</li>
</ul></li>
<li><strong>Non-parametric meta-learning</strong>
<ul>
<li>Parametric in the meta aspect, but not parametric on a per-task basis.</li>
<li>“k-means for tasks”</li>
<li>Protonets</li>
</ul></li>
<li><strong>Gradient-based meta learning</strong>
<ul>
<li>MAML</li>
<li>Adapt to new task by fine-tuning your model with gradient descent. Train in such a way that optimizes the adaptation process. Learning how to learn.</li>
<li>Optimization process is different, but the model is same (hence, “model-agnostic”).</li>
<li>This is the important one that is used more commonly nowadays.</li>
</ul></li>
</ol>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p align="center">
<img alt="picture 2" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/1373965aa0e2a1a6a19272782e7ec98ddd6d34b2678ce0a81e928474ef116e9d.png" width="100%">
</p>
</div>
</div>
</section>
</section>
<section id="non-parametric-meta-learning" class="level2">
<h2 class="anchored" data-anchor-id="non-parametric-meta-learning">Non-parametric meta-learning</h2>
<ul>
<li>Note that by non-parametric, we mean we don’t use params to adapt to each new task. But the overall deep net still has params.</li>
</ul>
<section id="high-level-idea" class="level3">
<h3 class="anchored" data-anchor-id="high-level-idea">High-level idea</h3>
<p align="center">
<img alt="picture 3" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/ed19650ff0ad5793d4ee7d720b67ae6e4383ad8b0b4a1e8d400a01619606826f.png" width="50%">
</p>
<ul>
<li>Suppose we have a 1-shot, 5-way classification problem. We have 5 images in each <span class="math inline">\(\mathcal{D}^{\mathrm{tr}}_i\)</span>, each corresponding to a different class. We now have one new image in <span class="math inline">\(\mathcal{D}^{\mathrm{te}}_i\)</span>. We wish to assign one of the 5 labels to this new image. The idea is to compare this test image to the train images and take the label of the nearest neighbor (from the train images) to the test image.</li>
<li>But the pixel values aren’t that comparable/learnable. So to make this learnable, we use an <strong>extractor network</strong> <span class="math inline">\(\phi\)</span> to embed each images into some feature space. Then, in this feature space, we can find this nearest neighbor. But the idea is that we parameterize <span class="math inline">\(\phi\left(x_i\right)\)</span> and learn these params during the training process.
<ul>
<li>The idea is to learn a good feature space.</li>
<li>Note that for now, the feature space is the same for the train and test images but they can be separate (discuss this later).</li>
</ul></li>
<li>Lots of math details, but see the video for that.</li>
</ul>
<p>Here is the loss:</p>
<p><span class="math display">\[
\theta^{\star}=\arg \min _\theta \sum_{i=1}^n \mathcal{L}({f_\theta\left(\mathcal{D}_i^{\mathrm{tr}}\right.}), \mathcal{D}_i^{\mathrm{ts}})
=
-\sum_{i=1}^n \sum_{j=1}^m \log p_\theta\left(y_j^{\mathrm{ts}} \mid x_j^{\mathrm{ts}}, \mathcal{D}_i^{\mathrm{tr}}\right)
\]</span></p>
<p>where the parameters <span class="math inline">\(\theta^{\star}\)</span> are learned for the embedding network/function <span class="math inline">\(\phi(x)\)</span>.</p>
<p>Ok… we got the high-level idea down. Now let us turn to some actual implementations (which are a bit more involved).</p>
</section>
<section id="matching-networks" class="level3">
<h3 class="anchored" data-anchor-id="matching-networks">Matching networks</h3>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>