<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Knowledge – quarto-input99dc1d00</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Knowledge</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html">Stanford Notes</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#dinasour-book-chapter-7-notes---main-memory" id="toc-dinasour-book-chapter-7-notes---main-memory" class="nav-link active" data-scroll-target="#dinasour-book-chapter-7-notes---main-memory">Dinasour Book Chapter 7 Notes - Main Memory</a>
  <ul class="collapse">
  <li><a href="#hardware-of-main-memory" id="toc-hardware-of-main-memory" class="nav-link" data-scroll-target="#hardware-of-main-memory">7.1 - Hardware of main memory</a>
  <ul class="collapse">
  <li><a href="#address-binding" id="toc-address-binding" class="nav-link" data-scroll-target="#address-binding">7.1.2 - Address binding</a></li>
  <li><a href="#swapping" id="toc-swapping" class="nav-link" data-scroll-target="#swapping">7.2 - Swapping</a></li>
  <li><a href="#standard-swapping" id="toc-standard-swapping" class="nav-link" data-scroll-target="#standard-swapping">7.2.1 - Standard swapping</a></li>
  </ul></li>
  <li><a href="#contiguous-memory-allocation" id="toc-contiguous-memory-allocation" class="nav-link" data-scroll-target="#contiguous-memory-allocation">7.3 - Contiguous memory allocation</a>
  <ul class="collapse">
  <li><a href="#memory-protection" id="toc-memory-protection" class="nav-link" data-scroll-target="#memory-protection">Memory protection</a></li>
  <li><a href="#memory-allocation" id="toc-memory-allocation" class="nav-link" data-scroll-target="#memory-allocation">Memory allocation</a></li>
  <li><a href="#fragmentation" id="toc-fragmentation" class="nav-link" data-scroll-target="#fragmentation">Fragmentation</a></li>
  </ul></li>
  <li><a href="#aside-logical-vs.-physical-addresses" id="toc-aside-logical-vs.-physical-addresses" class="nav-link" data-scroll-target="#aside-logical-vs.-physical-addresses">Aside: Logical vs.&nbsp;physical addresses</a>
  <ul class="collapse">
  <li><a href="#logical-addresses" id="toc-logical-addresses" class="nav-link" data-scroll-target="#logical-addresses">Logical addresses</a></li>
  <li><a href="#physical-addresses" id="toc-physical-addresses" class="nav-link" data-scroll-target="#physical-addresses">Physical addresses</a></li>
  <li><a href="#key-differences" id="toc-key-differences" class="nav-link" data-scroll-target="#key-differences">Key differences</a></li>
  <li><a href="#more-on-the-hardware-and-address-binding" id="toc-more-on-the-hardware-and-address-binding" class="nav-link" data-scroll-target="#more-on-the-hardware-and-address-binding">More on the hardware and address binding</a></li>
  </ul></li>
  <li><a href="#segmentation" id="toc-segmentation" class="nav-link" data-scroll-target="#segmentation">7.4 - Segmentation</a>
  <ul class="collapse">
  <li><a href="#segmentation-hardware" id="toc-segmentation-hardware" class="nav-link" data-scroll-target="#segmentation-hardware">Segmentation hardware</a></li>
  <li><a href="#pros-and-cons-of-segmentation" id="toc-pros-and-cons-of-segmentation" class="nav-link" data-scroll-target="#pros-and-cons-of-segmentation">Pros and cons of segmentation</a></li>
  </ul></li>
  <li><a href="#paging" id="toc-paging" class="nav-link" data-scroll-target="#paging">7.5 - Paging</a>
  <ul class="collapse">
  <li><a href="#basic-methods" id="toc-basic-methods" class="nav-link" data-scroll-target="#basic-methods">7.5.1 - Basic methods</a></li>
  <li><a href="#hardware-support" id="toc-hardware-support" class="nav-link" data-scroll-target="#hardware-support">7.5.2 - Hardware support</a></li>
  <li><a href="#protection" id="toc-protection" class="nav-link" data-scroll-target="#protection">7.5.3 - Protection</a></li>
  </ul></li>
  <li><a href="#differences-between-paging-and-segmentation" id="toc-differences-between-paging-and-segmentation" class="nav-link" data-scroll-target="#differences-between-paging-and-segmentation">Differences between paging and segmentation</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="dinasour-book-chapter-7-notes---main-memory" class="level1">
<h1>Dinasour Book Chapter 7 Notes - Main Memory</h1>
<p>It is important to note the difference between <strong>memory vs.&nbsp;disk</strong>. Memory is storage which the CPU accesses and interacts with immediatly. Disk is where things like files store long term. Virtual memory simulates the memory slot but is actually stored in disk so it can benefit from the larger size of the disk.</p>
<p><a href="https://kb.iu.edu/d/ahtx">This link</a> has some more information: Memory and disk storage both refer to internal storage space in a computer. Each is used for a different purpose.The term “memory” usually means RAM (Random Access Memory); RAM is hardware that allows the computer to efficiently perform more than one task at a time (i.e., multi-task). The terms “disk space” and “storage” usually refer to hard drive storage. Hard drive storage is typically used for long-term storage of various types of files. Higher capacity hard drives can store larger amounts and sizes of files, such as videos, music, pictures, and documents. “Virtual memory” is hard disk space that has been designated to act like RAM. It assists the computer with multi-tasking when there is not a sufficient amount of RAM for the tasks.</p>
<p>Here is <span class="math inline">\(4+4+5\)</span> mathematical content:</p>
<p><span class="math display">\[
4+5=9
\]</span></p>
<p>wow!</p>
<hr>
<p>In the last unit, we learned about concurrency: how computer systems can execute multiple programs (processes) at once on one processor. Now we turn to memory management: how computer systems can manage the execution of multiple programs at once with only one main memory. Because of course, this is necessary to have multiple processes running at once.</p>
<p>In this section, we aim to look at algorithms and schemes that solve this problem.<br>
- CPU can only directly access data from registers or main memory. If something from disk needs to be accessed, it must first be moved to main memory.</p>
<section id="hardware-of-main-memory" class="level2">
<h2 class="anchored" data-anchor-id="hardware-of-main-memory">7.1 - Hardware of main memory</h2>
<p>So what does MM actually look like? Remember, it just a long contigious array of bytes where each byte has an address. It looks like this:</p>
<p>Each process needs to have its own contigious block of memory that it can access. To help manage this (that is, to protect processes from interfering with each other), there are two registers which are called the <strong>base and limit registers</strong> which determine the legal range of addresses that a process may access (question: is this because processes might move around and are not static?). The above figure shows an example of the base and the limit ranges (remember MM grows downwards so the base is actually higher up in the figure): process can access bytes addressed between (inclusive) base: 300040 and limit: 120900. This scheme is visually depicted as follows:</p>
<ul>
<li>Importantly, only OS kernel mode can change the values in these registers: not user programs. So the OS really provides the lubricating jelly here: makes it work for the higher-level processes.</li>
<li>OS will continiously boot out user processes in concurrency to replace it with another process to execute.</li>
</ul>
<section id="address-binding" class="level3">
<h3 class="anchored" data-anchor-id="address-binding">7.1.2 - Address binding</h3>
<ul>
<li>A “program” resides in disk as an executable. Upon execution, program is brought into memory and becomes process. So processes waiting to be brought into MM for execution are stored in an <strong>input queue</strong>.</li>
<li>In standard old-school single-tasking, OS would select one process from the queue and load it into memory. CPU that can access code instructions and data from the memory. Then, process terminates and the memory space is now available for the next process to run.</li>
</ul>
<p>I need to understand address binding a bit better tbh.</p>
</section>
<section id="swapping" class="level3">
<h3 class="anchored" data-anchor-id="swapping">7.2 - Swapping</h3>
<ul>
<li><p>As we know, a process must be in MM to be executed. However, a process can be <strong>swapped</strong> temporarily out of memory to something called a <strong>backing store</strong> (when it pauses execution) and then brought back into MM for continued execution.</p></li>
<li><p>Swapping makes it possible for the aggregate space of all the running processes (logical space) to exceed that of the physical address space of main memory: thus increasing the degree of multiprogramming. This is all visually depicted like so:</p></li>
<li><p>We will discuss some swapping techniques here.</p></li>
</ul>
</section>
<section id="standard-swapping" class="level3">
<h3 class="anchored" data-anchor-id="standard-swapping">7.2.1 - Standard swapping</h3>
<p>Most intuitive way (first thing you’d think of):</p>
<ol type="1">
<li>Standard swapping involves moving processes (their memory images that is: see <a href="https://tldp.org/LDP/LG/issue23/flower/psimage.html">here</a>) between MM and a backing store (BS). BS is commonly some fast disk. Must be large enough to accomodate copies of all memory images for all users.</li>
<li>System maintains a <strong>ready queue</strong> consisting of all processes whose memory images are on the abcking store or in memory and are ready to run.</li>
<li>Whenever CPU scheduler decides to execute a process, it calls the dispatcher. From here, the dispatcher checks to see whether the next process in the queue is in memory. If not, it goes to retrieve it from BS and loads into memory. If there was some previous process in memory, it just does a simple swap.</li>
<li>Reloads registers and transfers control to this new process.</li>
</ol>
<p>Problems: - Standard swapping is a good conceptual introduction but it is not currently used that often because it requires too much swapping time rather than execution time to be an efficient memory management solution. We will talk about some other solutions below but to get you thinking, what happens if we swap only <em>part</em> of each process rather than the entire thing? This is kind of the basis of the virtual memory which we will cover next.</p>
</section>
</section>
<section id="contiguous-memory-allocation" class="level2">
<h2 class="anchored" data-anchor-id="contiguous-memory-allocation">7.3 - Contiguous memory allocation</h2>
<p>We discussed swapping above to control execution of processes during run time. But we can also study how we can effectively actually <strong>allocate</strong> the process images into main memory. To be more specific, when we swap a process from backing store (disk) to main memory, where should we place the process memory block? How can we do this most efficiently in conjuction with all of the previously placed blocks of memory? This is the idea of contiguous memory allocation which we discuss here. This is a direct mirror of heap allocation techniques discussed in CS 107.</p>
<ul>
<li>MM must hold both the OS and all other user processes. So we need to be as efficient as possible.</li>
<li>We usually place OS in the low memory (the first thing) because of proximity to the interrupt vector which is placed in low-memory.</li>
</ul>
<section id="memory-protection" class="level3">
<h3 class="anchored" data-anchor-id="memory-protection">Memory protection</h3>
<ul>
<li>Before we even begin to discuss allocation, we should understand the idea of <strong>memory protection</strong>: that is, <strong>how can we prevent a process from accessing memory it does not own</strong>?</li>
<li>Virtualization: we can do this with the relocation (base) and limit registers.
<ul>
<li>Relocation register: specifies the base physical address of the process.</li>
<li>Limit register: specifies the range of logical addresses (those functioning per the CPU and user program).</li>
<li>Example: relocation = 100040 and limit = 746000.</li>
<li>So each logical address must now fall within the range specified by the limit register. The MMu does the mapping from the logical addresses dynamically by adding the value to the value in the relocation register.</li>
</ul></li>
<li>CPU scheduler selects a process for exeuction and dispatcher loads the correct relocation and limit registers.</li>
<li>So when the CPU generates new logical addresses during run time, we do a quick check to ensure that the addresses fall between these ranges set by registers.</li>
<li>This helps make the OS (really?) dynamically sized.</li>
</ul>
</section>
<section id="memory-allocation" class="level3">
<h3 class="anchored" data-anchor-id="memory-allocation">Memory allocation</h3>
<p>Ok… now let us get into allocation now that we have an idea for how process memory is protected.</p>
<p>The first idea is that of <strong>fixed-partitions</strong>.</p>
<ol type="1">
<li>Divide memory into several fixed size “partitions”.</li>
<li>Each partition may only contain one process.</li>
<li>When a partition if free, a process is selected from the queue and is loaded into this free partition.</li>
<li>When the process terminates, the partition becomes available for the next process.</li>
</ol>
<p>Pretty easy to see limitations and why it is no longer used commonly: fixed size, can only have as many programs running = number of partitions.</p>
<p>The next scheme is just about identical to heap allocation techniques from 107: <strong>variable-partitioning</strong>. We don’t divy up the memory into partitions designated for each process (discretize). Instead, we view the memory as a continious region. At first, the entire free region is one big <strong>hole</strong>. OS takes processes from input queue according to scheduling algorithm. We try to find region in the holes where we can allocate next process. We do this until we no longer have enough free space for the next process. In which case, the OS can decide to stall until there is enough free memory or jump to the next process small enough in the queue (depending on the scheduling algorithm at play). Here is how the textbook describing this allocation scheme:</p>
<blockquote class="blockquote">
<p>In general, as mentioned, the memory blocks available comprise asetofholes of various sizes scattered throughout memory. When a process arrivesand needs memory, the system searches the set for a hole that is large enoughfor this process. If the hole is too large, it is split into two parts. One part isallocated to the arriving process; the other is returned to the set of holes. Whena process terminates, it releases its block of memory, which is then placed backin the set of holes. If the new hole is adjacent to other holes, these adjacent holesare merged to form one larger hole. At this point, the system may need to checkwhether there are processes waiting for memory and whether this newly freedand recombined memory could satisfy the demands of any of these waitingprocesses.</p>
</blockquote>
<p>Sound familiar? This is dynamic storage allocation: <strong>how can we satisfy a request of size <span class="math inline">\(n\)</span> from a list of free holes?</strong></p>
<p>In it of itself, there are many solutions to this scheme which we all know about from 107. Common strategies include:</p>
<ul>
<li><strong>First-fit</strong>: traverse list of holes. Allocate the process to the <strong>first hole</strong> that is big enough. Search can start from beginning of list of holes or where it left off from previous search.</li>
<li><strong>Best-fit</strong>: designate the process to the <strong>smallest hole</strong> that is big enough for the process to fit. For this, we must search the entire list (unless we order the list by size). Produces smallest leftover hole.</li>
<li><strong>Worst-fit</strong>: traverse list of holes. Allocate the process to the <strong>largest hole</strong>. Again, must traverse entire list. Produces largest leftover hole. Reasoning is that the leftover hole migfht be more useful for future processes than best-fit small leftover holes.</li>
</ul>
</section>
<section id="fragmentation" class="level3">
<h3 class="anchored" data-anchor-id="fragmentation">Fragmentation</h3>
<p>Pitfalls of memory allocation that is inherently bound to happen based on the nature of the problem (John thinks that you can never have a perfect solution that avoids memory fragmentation).</p>
<p>Two types (see <a href="https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/8_MainMemory.html">these notes</a> for more):</p>
<ul>
<li><strong>Internal fragmentation</strong>: an allocated block is larger than what is needed for the process (example: we may only need four bytes but we must start the next sequence of bytes 4 bytes further since we align everything via 8 byte widths). “Caused by the fact that memory is allocated in blocks of a fixed size, whereas the actual memory needed will rarely be that exact size.”.</li>
<li><strong>External fragmentation</strong>: no single available block is large enough to satisfy the request, but there is, in aggregate, enough free memory available (e.g.&nbsp;what we described above).</li>
</ul>
<p>Some stats: - <strong>50-percent rule</strong>: one-third of memory may be unusable. Statistical analysis of <strong>first fit</strong> states that given <span class="math inline">\(N\)</span> allocated blocks, another <span class="math inline">\(0.5\)</span> <span class="math inline">\(N\)</span> blocks will be lost due to fragmentation. - On average, for every request, only <span class="math inline">\(1/2\)</span> of the block will be filled up by the process. So <span class="math inline">\(1/2\)</span> of every block is wasted due to internal fragmentation on average.</p>
<p><strong>How to solve these problems</strong>? Can never perfectly solve but some ideas we will discuss include segmentation and paging. Basically: - <strong>Coalescing</strong>: If the programs in memory are relocatable, (using execution-time address binding as previously discussed), then the external fragmentation problem can be reduced via compaction (coalescing at runtime!), i.e.&nbsp;moving all processes down to one end of physical memory. This only involves updating the relocation register for each process, as all internal work is done using logical addresses (not the physical address space). - <strong>Non-contigious blocks of physical memory through logical address mapping</strong>: Another solution as we will see in upcoming sections is to allow processes to use non-contiguous blocks of physical memory (but “virtually” contigious in logical address space?), with a separate relocation register for each block. - Segmentation and paging (the topic of the next two sections acheive this).</p>
</section>
</section>
<section id="aside-logical-vs.-physical-addresses" class="level2">
<h2 class="anchored" data-anchor-id="aside-logical-vs.-physical-addresses">Aside: Logical vs.&nbsp;physical addresses</h2>
<p>It is vital that you understand address binding and the differences between logical addresses and physical addresses before moving on (as the next sections begin an introduction to virtual memory which involves mapping logical addresses to physical addresses… everything to this point has just been absolute mappings). There was a section above on it, but I think it makes more sense to discuss it here. So before moving on, I will write some notes on it here. These notes are based on <a href="https://www.meerutcollege.org/mcm_admin/upload/1587052623.pdf">this</a> document.</p>
<p>Aside: os-step book states that the “address space” of a process is an abstraction of physical memory that the program sees. It is the e.g.&nbsp;stack, heap, code, etc. It is <em>not</em> an image of the physical main memory itself.</p>
<p>The program has no privalege to view main memory physically. The OS does and handles this (and ensures things like protection so that no process can access another processes memory). The main point I want to stress here is that the program’s view of memory is isolated to its address space which itself is a virtualization of physical memory (not the entire image of physical memory, but rather that of the process block). The memory management unit is then responsible for translating the addresses from the virtual address space to the physical memory slab. So stress this more, in the above figure of a process’s virtual address space, notice that the addresses start at <span class="math inline">\(0\)</span> but this isn’t the actual address of the code segment in memory. The MMU translates this address. Thus, we consider this a <strong>virtualization</strong> of the memory. Again, what I want to stress is that even basic techniques like base-bound translation are virtual memory because the address space is itself virtual. So we need some form of address translation even in this technique.</p>
<ul>
<li><strong>Address</strong> uniquely identifies a location in the memory (byte-indexed).</li>
<li><strong>Logical address</strong>: a <em>virtual</em> address and can be viewed by the user.
<ul>
<li>Used like a reference, by the user, to access the physical address.</li>
</ul></li>
<li>The <strong>physical address</strong>, computed by the MMU, is where all the dirty, behind-the-scenes stuff happens.
<ul>
<li>Cannot be accessed by the user.</li>
</ul></li>
<li>Difference between logical and physical address is that <strong>logical address is generated by CPU during a program execution</strong> whereas, the <strong>physical address refers to a location in the memory unit</strong>.</li>
</ul>
<p>See chart here for this differences:</p>
<p>Pause: you might be wondering, wny do we need this whole virtualized mapping scheme? Why not give user processes direct access to the main memory unit? The answer is security. We don’t want applications to be able to overwrite other processes for example. So we let the operating system take this role and it does this by creating a virtual address space for the user and then performs the mapping to the physical address space. To see more about the motivation, see <a href="https://stackoverflow.com/questions/29771977/purpose-of-logical-address#:~:text=Logical%20address%20is%20used%20to,process%20thus%20corrupting%20that%20process.">this</a> Stack Overflow thread.</p>
<section id="logical-addresses" class="level3">
<h3 class="anchored" data-anchor-id="logical-addresses">Logical addresses</h3>
<p>Let us take a deeper dive on logical addresses.</p>
<ul>
<li>It is an address generated by the CPU for a program: e.g.&nbsp;program local variables such as <code>int x</code> get a virtual address.</li>
<li>This address is not a physical memory address. It is used to locate a physical memory address.</li>
<li>We call the <strong>set of all logical addresses the logical address space</strong>.</li>
<li>The <strong>Memory-Management Unit</strong> (a piece of hardware) maps the logical addresses to the corresponding physical addresses.</li>
<li>During compile and load time, the logical addresses are identical to the physical addresses. The real power comes into play during execution time. At run time, the address-binding methods by the MMU generate different logical and physical address.</li>
</ul>
</section>
<section id="physical-addresses" class="level3">
<h3 class="anchored" data-anchor-id="physical-addresses">Physical addresses</h3>
<p>Let us now discuss physical addresses in more depth.</p>
<ul>
<li>Physical Address identifies a physical location in a memory. It is an address to some byte-index.</li>
<li>The MMU computes the corresponding physical address for each logical address.</li>
<li>User cannot access. Kernel mode OS can though.</li>
<li>The set of all physical addresses corresponding to the logical addresses in a Logical address space is called Physical Address Space.</li>
</ul>
<p>Diagram:</p>
</section>
<section id="key-differences" class="level3">
<h3 class="anchored" data-anchor-id="key-differences">Key differences</h3>
</section>
<section id="more-on-the-hardware-and-address-binding" class="level3">
<h3 class="anchored" data-anchor-id="more-on-the-hardware-and-address-binding">More on the hardware and address binding</h3>
<p>We again need to review the beginning parts of this chapter to solidify understanding.</p>
<ul>
<li>CPU needs to access main memory to execute a process. Process is stored in main memory and its image includes its code instructions, data, etc.</li>
<li>CPU can only access direct main memory and not hard drive disk. So all programs must be loaded in main memory to execute.</li>
<li>Only the OS has access to the physical address space. Logical addresses (for things like <code>int x</code> variables) are generated by the CPU for each program.</li>
<li>User processes can only access memory (via logical addresses) that belongs to it. What designates the memory a process can access (again not physically but by means of virtual logical addresses) is based on two registers which define a range of addresses that the process can access: a base register and a limit register.</li>
<li>Every memory access made by a user process is checked against these two registers, and if a memory access is attempted outside the valid range, then a fatal error is generated.</li>
<li>OS has privaleged access to all memory locations as they perform job of swapping. Changing of base and limit registers (dynamically changing size of process?) is only allowed by the OS as well.</li>
</ul>
<p>Now let us talk about address binding (see <a href="https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/8_MainMemory.html">this</a> link and <a href="https://www.techwalla.com/articles/what-is-address-binding">this</a> one too) a bit:</p>
<blockquote class="blockquote">
<p>The process of associating program instructions and data to physical memory addresses is called address binding. That is, how can we find the addresses for which the program should live during execution?</p>
</blockquote>
<ul>
<li>User programs typically refer to memory addresses with symbolic names such as “<code>i</code>”, “<code>count</code>”, and “<code>averageTemperature</code>”. These symbolic names must be mapped or ‘bound’ to physical memory addresses. This occurs in several stages during the living time of the program:</li>
</ul>
<ol type="1">
<li><strong>Compile time</strong>: can generate address for the process to live (load address) during compile time: if it is known at compile time where a program will reside in physical memory, then absolute code can be generated by the compiler, containing actual physical addresses. However if the load address changes at some later time, then the program will have to be recompiled (boo!).</li>
<li><strong>Load time</strong>: say that the location at which a program will be loaded is not known at compile time. In this case, he compiler must generate <strong>relocatable code</strong>. In this case, all of the <strong>addresses are simply in reference to the start of the program</strong> (so that when the load address becomes known, the physical addresses can be calculated by adding the referenced addresses to the load address). So in this case, if that starting address changes, then the program must be reloaded but not recompiled.</li>
<li><strong>Execution time</strong>: sometimes we may actually want to move around a program/process in memory during the course of its execution. In this case, binding must be delayed until execution time. This requires special hardware (i.e.&nbsp;memory management unit), and is the method implemented by most modern OSes.</li>
</ol>
<p>So we now understand that address binding can occur in one of the three stages above. Addresses bound at compile time or load time have identical logical and physical addresses. Addresses created at execution time, however, have different logical and physical addresses. <strong>This mapping is performed by the MMU at run-time</strong> (how amazing is that! on the fly!). How exactly does the MMU do this? That is what we will discuss next chapter but for now:</p>
<ul>
<li>The MMU can take on many forms. One of the simplest is just to modify the base-register scheme described earlier. i.e.:</li>
<li>The base register is now termed a relocation register, whose value is added to every memory request at the hardware level. We can modify the location of all the addresses by changing the relocation (base) address on the fly and then computing the new addresses by adding the relocation address to it (because everything is in reference the base address (I think?)).</li>
<li>Note that user programs never see physical addresses. User programs work entirely in logical address space, and any memory references or manipulations are done using purely logical addresses. Only when the address gets sent to the physical memory chips is the physical memory address generated.</li>
</ul>
<p>Alright… back to the action.</p>
</section>
</section>
<section id="segmentation" class="level2">
<h2 class="anchored" data-anchor-id="segmentation">7.4 - Segmentation</h2>
<p>We left off with how we can mitigate the effects of fragmentation in contigious memory allocation. We mentioned that we can coalesce blocks of free memory if the addresses are relocatable. However, another solution <strong>is to allow processes to use non-contiguous blocks of physical memory</strong> (but “virtually” contigious in logical address space?), with a separate relocation register for each block. There are two approaches to this and segmentation is the first one we will discuss. This begins the real intro to “virtual memory”.</p>
<p>(The abstraction)</p>
<p>The basic idea behind segmentation is to model how a programmer would think. We programmers tend to view memory split up between things like the stack, heap, code, global variables, and libraries. But main memory is just a contigious sequence of bytes. But remember, user’s only work with a logical/virtual interpretation of this. So t<strong>he idea of segementation is that the logical address space can be split up into </strong>segments** where each segment corresponds to some semantic meaning (e.g.&nbsp;heap)**.</p>
<blockquote class="blockquote">
<p>Actually, I don’t think this is the case. You should view segmentation as an extension/follow-up to the base-and-bounds approach. Instead of giving each process a base and a bound. We are going to divide the processes memories up into segments and then these itself are like base and bounds (so we have an array of base and bounds). The idea is that we can move around these <em>smaller</em> blocks more coherently, reduces fragmentation, easier to compact, but the main conveniance is the ability to allow the process memory to be physically discontinious. A note: internal fragmentation <em>cannot</em> occur because the segments are allocated based on how the logical memory is divided up. However, external fragmentation can still occur if for example, the blocks don’t fit into a space.</p>
</blockquote>
<p>So now, the logical address is a two tuple:</p>
<p><span class="math display">\[
&lt;\text { segment-number, offset }&gt;\text { }
\]</span></p>
<p>where offset is distance in bytes from base address of the segment.</p>
<section id="segmentation-hardware" class="level3">
<h3 class="anchored" data-anchor-id="segmentation-hardware">Segmentation hardware</h3>
<p>(The physical realization)</p>
<p>Although the programmer can now refer to objects in the program by a two-dimensional address, the actual physical memory is still, of course, a one dimensional sequence of bytes. So what is being done on the physical side of things?</p>
<ul>
<li><strong>We must define an implementation to map two-dimensional programmer-defined addresses into one-dimensional physical addresses</strong>. Going from 2d to 1d!</li>
<li>How does this mapping work?</li>
<li>Done by the <strong>segment table</strong>.</li>
<li>Each entry (one entry per segment that is) in the <strong>segment table</strong> has a <strong>segment base</strong> and <strong>segment limit</strong>.</li>
<li>Segment base: contains the starting physical address where the segment resides in memory (physical memory).</li>
<li>Segment limit: specifies the length of the segment</li>
</ul>
<p>The use of a segment table is illustrated in the above figure. - A logical address consists of two parts: a segment number,<span class="math inline">\(s\)</span>,and an offset into that segment,<span class="math inline">\(d\)</span>. - The segment number is used as an index to the segment table (like an array index, to get the correct segment). - The offset <span class="math inline">\(d\)</span> of the logical address must be between 0 and the segment limit. We check for this, and if it is not, we trap (send error) to the operating system (logical addressing attempt beyond end of segment… <em>segfault</em>!). When an offset is legal, it is added to the segment base to produce the address in physical memory of the desired byte. The segment table is thus essentially an array of base–limit register pairs. - So I think… from my understanding, that this means multiple processes can use these sections in a shared fashion and so the process block is not contigious. Is this correct? Like each new process comes in and divides itself by the semantic segments (e.g.&nbsp;puts some of its memory in the heap, has its functions in the stack area, etc.). New process does the exact same thing. <strong>The segments are shared between processes</strong>.</p>
<p>Pictoral example:</p>
<p>See how we now have physical “segments” in the physical memory rather than process blocks?</p>
<p>Some auxillary notes from the course lecture notes:</p>
<ul>
<li>Each segment also has a protection bit for each segment that denotes whether it is read-write versus read-only.</li>
<li>Segmentation table (map as its called in the course notes) is itself stored in the MMU.</li>
<li>Table is modified during context switches
<ul>
<li>See Ed question <a href="https://edstem.org/us/courses/20857/discussion/1457730">here</a>. Basically, not all segments are shared between processes. Some might have function segments unique to a process so it takes up its own segment. In these cases, some of the segments are swapped out of the table and physical memory during context switching.<br>
</li>
</ul></li>
<li>In practice, we the two-tuple referencing scheme can be stored within bitwise semantics: Top bits of address select segment, low bits indiciate the offset.</li>
</ul>
</section>
<section id="pros-and-cons-of-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="pros-and-cons-of-segmentation">Pros and cons of segmentation</h3>
<p><strong>Pros</strong>: Flexibility. - Permits the physical address space of a process to be non-contiguous (main motivation) - Each segment is managed independently. - e.g.&nbsp;stack doesn’t interfere with text segment (unless you get a pesky segfault). - Grow/shrink independently - Swap to disk independently when needed - Can share segments between processes - e.g.&nbsp;helpful if you want to do something like share code - Can compact/coalesce memory to reduce fragmentation</p>
<p><strong>Cons</strong>: - Limited to fixed number of segments - Fragmentation - But how? - Interestingly, fragmentation can occur on the backing store during swaps (because colaselcing isn’t possible on backing store since CPU cannot access it) - Maybe internal fragmentation (like some processes won’t have enough code to fill up the text segment) - Virtual address space is rigidly divided</p>
</section>
</section>
<section id="paging" class="level2">
<h2 class="anchored" data-anchor-id="paging">7.5 - Paging</h2>
<p>We now introduce the most used method for memory management in today’s systems: paging. It is so good because it is virtual and it solves the problems of the other virtual memory manangement solution (segmentation).</p>
<ul>
<li>We saw that segmentation <strong>permits the physical address space of a process to be non-contiguous</strong>. Paging also offers this pro.</li>
<li>The difference, is that <strong>paging avoids external fragmentation</strong> and also avoids the need to compact/coalesce.</li>
<li>Another pro is that it solves the considerable problem of fitting memory chunks of varying sizes onto the backing store. Most other schemes cause fragmentation on the backing store itself (when needed to swap, it is essentially the same problem (allocation) but in a different direction lol).</li>
<li>So… what is paging, how do we accomplish and implement it? Well, we are going to need some support from the hardware and it is implemented in the OS.</li>
</ul>
<section id="basic-methods" class="level3">
<h3 class="anchored" data-anchor-id="basic-methods">7.5.1 - Basic methods</h3>
<ul>
<li>Paging is a memory management scheme that <strong>allows process’s physical memory to be discontinuous</strong>, and which <strong>eliminates problems with fragmentation</strong> by allocating memory in equal sized blocks known as pages.</li>
<li>Idea is to break up physical memory into fixed-size blocks called <strong>frames</strong> and break logical memory into blocks of the same size called <strong>pages</strong>.</li>
<li>So the idea is that when a process is to be executed, during the loading process (I think?) its pages fill in the available memory frames from their source (backing store or physical file).</li>
<li>Not only that, the backing store itself is divided into the same fixed-sized blocks that are the same size of the frames.</li>
<li>It is easy to see that the logical address space (pages) is now completely separate from the physical address space. Because of this, we can simulate to the user that there is more memory than there might be physically available.</li>
</ul>
<p>In summary from above: - Paging involves breaking physical memory into equal fixed-sized blocks called <strong>frames</strong>. - Then, the logical address space (virtual memory displayed to the programmer) is itself broken up into the same fixed-sized blocks called <strong>pages</strong>. - When process executed, load pages into available frames. - Advantages: allows process’s physical memory to be discontinuous, more virtual memory than physical memory, avoids fragmentation</p>
<p>Hardware support: - When CPU generates a logical address, it is divided into two parts: - <strong>Page number (p)</strong> - <strong>Page offset (d)</strong> - We use the page number to index into a <strong>page table</strong>. - The <strong>page table</strong> is table containing the <strong>base address of each page in physical memory</strong> - This base address is combined with the page offset to define the physical memory address that is sent to the memory unit.</p>
<section id="page-size" class="level4">
<h4 class="anchored" data-anchor-id="page-size">Page size</h4>
<p>Let us talk about the size of the pages. Dependent on hardware. Typically by powers of 2 because then translation from logical –&gt; physical is easy because we can user high-order bits.</p>
<ul>
<li>Say that size of logical address space is <span class="math inline">\(2^m\)</span> bytes and we have page size as <span class="math inline">\(2^n\)</span> bytes. This means that the <em>offset</em> itself will only take up <span class="math inline">\(n\)</span> bits. So we can use the remaining <span class="math inline">\(m-n\)</span> bits to store the page number and use those <span class="math inline">\(n\)</span> bits to store the offset into that page.</li>
</ul>
<p>See textbook for example of how to index into a page table to get the physical address. The general idea is conveyed through the following visual:</p>
</section>
<section id="back-to-section" class="level4">
<h4 class="anchored" data-anchor-id="back-to-section">back to section</h4>
<ul>
<li>Importantly, there is <strong>no external fragmentation</strong>: any free frame can be allocated to some process that may need it.</li>
<li>Though <strong>internal fragmentation</strong> can still occur: but this only ahappens on the last page possible because the memory will be divided into the fixed-size page blocks. For example: &gt; Notice that frames are allocated as units. If the memory requirements of a process do not happen to coincide with page boundaries, the last frame allocated may not be completely full. For example, if page size is 2,048 bytes, a process of 72,766 bytes will need 35 pages plus 1,086 bytes. It will be allocated 36 frames, resulting in internal fragmentation of 2,048 − 1,086 = 962 bytes. In the worst case, a process would need n pages plus 1 byte. It would be allocated n + 1 frames, resulting in internal fragmentation of almost an entire frame.</li>
<li>Because of this, on avg, expect 1/2 of page of internal fragmentation per process.
<ul>
<li>So we should maybe try to make pages as small as possible then? Maybe but maybe not: overhead of keeping that many pages increases. For example, disk i/o (backing store swapping) is faster with larger pages because less pages have to be transferred over.</li>
</ul></li>
<li>In today OS, we generally use 4 KB page sizes.</li>
<li>I think the general idea is that once a process requests memory (i.e.&nbsp;it begins its execution and the code is loaded into memory), it builds a page table for that process, necessary free frames are allocated and hold the processes memory (the frames that are free are stored in a free-frame list) –&gt; then these frames are inserted into the initialized page table corresponding to that process.</li>
<li>These notes say
<ul>
<li>“Processes are blocked from accessing anyone else’s memory because all of their memory requests are mapped through their page table. There is no way for them to generate an address that maps into any other process’s memory space.”</li>
<li>But I am still confused on how this provides protection? Why can’t a program/cpu generate a bit address whose page number bits go to a page beyond what is stored in the page table?</li>
</ul></li>
<li>The <strong>operating system must keep track of each individual process’s page table</strong>, <strong>updating it whenever the process’s pages get moved in and out of memory</strong>, and applying the correct page table when processing system calls for a particular process (?). This all increases the overhead involved when swapping processes in and out of the CPU. (The currently active page table must be updated to reflect the process that is currently running.)</li>
</ul>
</section>
<section id="frame-table-optional" class="level4">
<h4 class="anchored" data-anchor-id="frame-table-optional">Frame table (optional)</h4>
<ul>
<li>Ok… so we know that each process has a page table. And we know that the OS is responsible for all of this handling. But how does it actually know what frames are available at what time and when to give some of the frames to pages of processes? It must manage all of this and it does so through a data structure called the <strong>frame table</strong>.</li>
<li>FT has entries where each one corresponds to a physical frame in memory and indicates:
<ul>
<li>Whether the frame is free or not</li>
<li>If not free, which page of which process has the spot.</li>
</ul></li>
</ul>
</section>
</section>
<section id="hardware-support" class="level3">
<h3 class="anchored" data-anchor-id="hardware-support">7.5.2 - Hardware support</h3>
<blockquote class="blockquote">
<p>Page lookups must be done for every memory reference, and whenever a process gets swapped in or out of the CPU, its page table must be swapped in and out too (in the MMU?), along with the instruction registers, etc. It is therefore appropriate to provide hardware support for this operation, in order to make it as fast as possible and to make process switches as fast as possible also.</p>
</blockquote>
<p>Wait: <strong>note that the page map itself is stored in continuous physical memory</strong>. The PTBR (see below) determines which process page table we would point to.</p>
<ul>
<li>Some OS have one page table per process and might for example, store it in the PCB. Others might share page tables across processes to reduce the overhead.</li>
<li>How can we implement page table in hardware though?</li>
<li>One idea is to dedicate registers for page tables. But this of course is only possible when we have a small number of pages as registers are limited.</li>
<li>But of course, today’s page tables are massive (&gt; 1 million entries). So the register approach is infeasible.</li>
<li>So another idea is: <strong>keep the page table itself in main memory</strong> and <strong>have a special register called the page-table base register (PTBR), point (i.e.&nbsp;store a pointer) to the page table in memory</strong>. So whenever we need to change the page table, we just have to change the pointer in this one singular register! Context switching time is reduced.</li>
<li>But… timing is still inefficient: because <strong>every memory access now requires two memory accesses</strong> - One to fetch the frame number from memory (i.e.&nbsp;go to page table) and then another one to access the desired memory location given by the table. So time efficiency is reduced by power of 2 (not going to work in high circumstances).</li>
<li>So we got a problem and how we gonna fix it?</li>
<li>Present day solution is to use a special, small, fast-lookup hardware cache called a <strong>translation look-aside buffer (TLB)</strong>.</li>
</ul>
<section id="translation-look-aside-buffer" class="level4">
<h4 class="anchored" data-anchor-id="translation-look-aside-buffer">Translation look-aside buffer</h4>
<ul>
<li>Basically, we create a small hardware cache (buffer array/dictionary) of recent translations.</li>
<li>Each cache entry stores two things:
<ul>
<li>The page number portion from a virtual address (i.e.&nbsp;the page that the CPU wants) is stored as a key and the corresponding physical page/frame number from physical memory is stored as the value.</li>
</ul></li>
<li>So on each memory reference, compare the page number from the virtual address with the virtual page numbers in every TLB entry (which is neatly done in parallel so way more efficient than just a straight memory reference). If the page number is found, its frame number is immediately available and is used to access memory. No memory access needed!</li>
<li>But… if the page number is not in the TLB (known as a <strong>TLB miss</strong>), a memory reference to the page table must be made :(.
<ul>
<li>At this point, we swap one entry from the TLB with these new translation so that it will be found quickly on the next reference. (optional) Different replacement policies can be used at this step (i.e.&nbsp;least recently used is the one that is swapped out). Also we can “wire down” certain TLB entries that cannot be swapped out such as those belonging to the OS.</li>
<li>You might be asking how a memory reference to a page table is actually made if we don’t just swap the page table in and out for each process like we had before this sub section. Basically, each page table for each process is stored in the PCB which itself is stored in the OS’s memory. The PTBR register holds a pointer to this place. So on a TLB miss, we need to use the pointer stored in the PTBR and access the page table their (1 memory reference) and then actually get the corresponding page/frame wanting to be addressed (another 1 memory reference).</li>
</ul></li>
<li>So on a miss, the process is still the same amount of time (two memory references: PTBR ptr –&gt; page table –&gt; frame). But on a hit, only one memory reference needed!</li>
<li>But luckily, computers usually have a &gt; 95% hit ratio. This makes sense. In programming, we define some things like an array that we continue to access throughout the course of the program. So it is best if we keep this memory translation in a cache. The TLB supplies such functionality.</li>
<li>If you are paying close attention, you might notice on problem with protection: if the TLB remains the same on context switching, then doesn’t this mean we give the opportunity to a program to maliciously get some random page from the cache buffer and use it as if it was their own? To protect against this, some TLBs store address-space identifiers, <strong>ASIDs</strong>, to keep track of which process “owns” a particular entry in the TLB. This enables entries from multiple processes to be stored simultaneously in the TLB without granting one process access to some other process’s memory location. Without this feature the TLB has to be flushed clean with every process switch if protection is wanted (which it is).</li>
<li><strong>Side note</strong>: we aren’t actually learning about ASIDs. Instead, for every context switch, we invalidate the entire TLB by setting each entry of the TLB to be invalid (cannot be accessed).
<ul>
<li>Question: So for every context switch, we invalidate the entire TLB? How do we share pages between processes then from the TLB? Also, isn’t this inefficient?</li>
</ul></li>
<li></li>
</ul>
<p><strong>Visual</strong>:</p>
<p>Some various notes: - Lots of math involved with the bit calculations. To determine how mnay bits we need to address a page table of size <span class="math inline">\(2^n\)</span> bytes, we need <span class="math inline">\(n\)</span> bits.</p>
</section>
</section>
<section id="protection" class="level3">
<h3 class="anchored" data-anchor-id="protection">7.5.3 - Protection</h3>
<ul>
<li>We can also provide extra protection via the page table. We can allocate a few more bits in the page table for each entry.</li>
<li>We can use these <strong>protection bits</strong> to specify whether the page can e.g.&nbsp;be read-only, write-only, execute-only, or any combination.
<ul>
<li>Cause OS to issue trap is CPU trying to reference a page to which it shouldn’t (i.e.&nbsp;trying to write to a read-only page).<br>
</li>
</ul></li>
<li>Importantly, we can also provide a <strong>valid–invalid bit</strong>.</li>
<li>When this bit is set to valid,the associated page is in th eprocess’s logical address space and is thus a legal (or valid) page. When the bit is set to invalid, the page is not in the process’s logical address space. Illegal addresses are trapped by use of the valid–invalid bit. The operating system sets this bit for each page to allow or disallow access to the page.</li>
</ul>
<p>Example (figure 7.15):</p>
<ul>
<li>Notice that process doesn’t have access to pages 6, 7. So if the CPU somehow generates some address corresponding to that page, OS will issue a trap due to the valid-invalid bit at those entries for this process’s page table.</li>
</ul>
<hr>
</section>
</section>
<section id="differences-between-paging-and-segmentation" class="level2">
<h2 class="anchored" data-anchor-id="differences-between-paging-and-segmentation">Differences between paging and segmentation</h2>
<p>Main difference is that paging is fixed sized blocks and segmentation is variable sized. Paging has only internal fragmentation whereas segmentation has only external fragmentation.</p>
<p>Note: make pros and cons list.</p>
<p>Segmentation pros: - No internal fragmentaion</p>
<p>Paging pros: - No external fragmentation - Virtual memory can be larger than physical memory</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>