<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Knowledge ‚Äì maximum-likelihood-estimation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Knowledge</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html">Stanford Notes</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#lecture-21-maximum-likelihood-estimation" id="toc-lecture-21-maximum-likelihood-estimation" class="nav-link active" data-scroll-target="#lecture-21-maximum-likelihood-estimation">üåã Lecture 21: Maximum Likelihood Estimation</a>
  <ul class="collapse">
  <li><a href="#review-what-is-likelihood" id="toc-review-what-is-likelihood" class="nav-link" data-scroll-target="#review-what-is-likelihood">Review: what is likelihood?</a>
  <ul class="collapse">
  <li><a href="#discrete-to-continious" id="toc-discrete-to-continious" class="nav-link" data-scroll-target="#discrete-to-continious">Discrete to continious</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="lecture-21-maximum-likelihood-estimation" class="level1">
<h1>üåã Lecture 21: Maximum Likelihood Estimation</h1>
<p>https://www.cs.cmu.edu/~tom/mlbook/</p>
<p>We now turn to machine learning and we begin with MLE where we try to estimate the parameters for a parametric distribution based on some observed data. But to understand this, we first need a thorough understanding of what exactly is likelihood and some other probabiltiy basics.</p>
<section id="review-what-is-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="review-what-is-likelihood">Review: what is likelihood?</h2>
<ul>
<li>We aim to understand the differences between probability and likelihood. We begin with reviewing how we came about random variables and probability. Say that there exists some random process such as coin flipping (e.g.&nbsp;outcomes of tossing a coin 10 times). In such cases, to gain a deeper intuition for the process, we assign a random variable to the process and calculate probabilities for how probable it is that the RV takes on a certain value. In such cases, we can calculate the probability of observing a particular set of outcomes by making suitable assumptions about the underlying stochastic process. For example, we might be given that the coin comes up heads with probability <span class="math inline">\(\theta = p = 0.4\)</span> per one flip. Thus, we can calculate <span class="math inline">\(P(X=x \mid \theta)\)</span> where <span class="math inline">\(\theta\)</span> is that probability parameter. Now you might be wondering, where do we get <span class="math inline">\(\theta\)</span> from? Do we just pull it out of our ass (excuse the French)? See, in the real world, we often do not know what the <span class="math inline">\(\theta\)</span> is and it isn‚Äôt just given to us. Instead, what we do have is the ability to flip the coin some <span class="math inline">\(n\)</span> times and from that, we can calculate the <span class="math inline">\(\theta\)</span>. That is, we simply observe the process that <span class="math inline">\(X\)</span> is representing and the goal then is to arrive at an estimate for <span class="math inline">\(\theta\)</span> that would be a plausible choice given the observed outcomes from <span class="math inline">\(X\)</span>. To do this in a structured way, we represent this as a function of <span class="math inline">\(\theta\)</span> and it is defined as <span class="math inline">\(L(\theta \mid X)\)</span>: the notation makes sense, we want to know <span class="math inline">\(\theta\)</span> given the outcomes we observe from <span class="math inline">\(X\)</span>. Such a function is called the <strong>likelihood</strong>.
<ul>
<li>Let us probe this further: <span class="math inline">\(L(\theta \mid X)\)</span> means ‚Äúhow likely are we to see a certain distribution given the obersevations?‚Äù. For example, in the normal case, <img src="attachments/Pasted%20image%2020220309171107.png" class="img-fluid"></li>
<li>Here we ask, how likely is it that the normal distriubtion follows this exact curve given the observed data (that the mouse weighs 34 grams)? So in some ways, we are composing the distribution curve based on the data! How do we compose such a curve? We first make an assumption for the parametric form it fits. Then, we estimate the parameters. This forms the curve. So likelihood, defined as <span class="math inline">\(L(\theta \mid X)\)</span>, is simply how likely is it that the curve is shaped like this given the observed data. That is the semantic meaning and it is quite different than probability (i.e.&nbsp;doesn‚Äôt have to be between 0 and 1). In this lecture, we show how to estimate such parameters so that the curve is most likely given the data. The reasoning for this is that the experiment is a full circle: we can then use the constructed distriubution to make future predictions on probabilities!</li>
<li>Here is a nice math result: <span class="math inline">\(L(\theta \mid X)=P(X \mid \theta)\)</span>: it turns out that the likelihood of a distribution given the data is the same as the probability density of the outcome given the distribution (see <a href="https://www.quora.com/How-come-the-likelihood-function-and-the-probability-function-are-the-same">here</a>). First answer <a href="https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability">here</a> is also good and of course <a href="https://www.youtube.com/watch?v=pYxNSUDSFH4">StatQuest</a>. Notice in the StatQuest video that he explains likelihood by shifting around the normal curve until the likelihood is maxed out (i.e.&nbsp;it fits the data observed well).
<ul>
<li>A note: it is vital to understand the realtion <span class="math inline">\(L(\theta \mid X)=P(X \mid \theta)\)</span>. Specifically, <span class="math inline">\(P(X \mid \theta)\)</span> means the probability that we would observe our data <span class="math inline">\(X\)</span> given the parametric distribution <span class="math inline">\(\theta\)</span>. This is the same thing as saying how likely is it that our distribution is constructed with parameters <span class="math inline">\(\theta\)</span> given the data we observed. See why?</li>
</ul></li>
</ul></li>
<li>Notice how this is a <strong>full circle</strong>: we have some underlying random process to which we want a whole distribution of probabilities for what the process outcome may be. However, we have no intuition yet so to derive the parameters for the distribution, we need to observe some data in real life. From this, we get the parameters we need to form our probabilistic intuitions.</li>
</ul>
<p>In my own words,</p>
<blockquote class="blockquote">
<p><strong>Likelihood</strong>: the likelihood is defined as how likely is it that the given parametric distribution curve (i.e.&nbsp;the parameters for the assumed parametric distribution family) is the correct probability distribution for the the data that has been observed.</p>
</blockquote>
<p>MLE: so MLE is the process of calculating the best curve for the distribution to fit the data: that is, maximizing <span class="math inline">\(L(\theta \mid X)\)</span>.</p>
<blockquote class="blockquote">
<p>Another mental model for likelihood: say we have some data and a dsitribution curve. Now let us draw a few samples from the distribution. Likelihood measures how likely it is that the observed data matches the drawn samples. i.e.&nbsp;how probable is it that we‚Äôd draw samples matching the observed data? This should help you understand <span class="math inline">\(L(\theta \mid X)=f(X \mid \theta)\)</span>.</p>
</blockquote>
<p><img src="attachments/Pasted%20image%2020220309174847.png" class="img-fluid"> The probability that we‚Äôd observe</p>
<blockquote class="blockquote">
<p>I mean this makes sense: what is the probability of observing your data given the parameters for the distribution (this is how likelihood is defined in course textbook: <span class="math inline">\(L(\theta)=\prod_{i=1}^{n} f\left(X_{i} \mid \theta\right)\)</span>. We want this probability to be as high as possible so that the resultant curve is fit well to the actual data. If we take a bunch of different values for <span class="math inline">\(\theta\)</span> (x-axis) and plot the results (y-axis) of the above equation on a graph, we get the likelihood function. We aim to find the maximum value of this function.</p>
</blockquote>
<p>Ok‚Ä¶ actual lecture notes now.</p>
<hr>
<ul>
<li>Basically, likelihood is defined as taking the probability densities for each data point for a certain <span class="math inline">\(\theta\)</span> and multiplying them together. Now do this for a bunch of different <span class="math inline">\(\theta\)</span> values. This is the likelihood function. See <code>49:00</code> from lecture.
<ul>
<li>From this, it is easy to see that <span class="math display">\[L(\theta)=\prod_{i=1}^{n} f\left(X_{i} \mid \theta\right).\]</span></li>
</ul></li>
<li>We don‚Äôt lile products and it gets small so we just use log of likelihood.</li>
</ul>
<p>Ok so we need to maximize LL function. How to do? Gradient ascent!</p>
<section id="discrete-to-continious" class="level3">
<h3 class="anchored" data-anchor-id="discrete-to-continious">Discrete to continious</h3>
<p>A note: discrete to continious. We have things in terms of densities so we will work with discrete for everything. The good news is that you can fit a function (curve) to a histogram (discrete-valued histogram).</p>
<p><span class="math display">\[X \sim \operatorname{Ber}(p) \rightarrow f(X=x \mid p)=p^{x}(1-p)^{1-x}\]</span></p>
<p><img src="attachments/Pasted%20image%2020220309225850.png" class="img-fluid"></p>
<p>A new way to choose parameters‚Ä¶ next lecture.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>