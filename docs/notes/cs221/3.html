<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Knowledge – quarto-inputb91f37dc</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Knowledge</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html">Stanford Notes</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../machine-learning.html">Machine Learning</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../concepts.html">Concepts</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#week-2-lecture-1---machine-learning-ii" id="toc-week-2-lecture-1---machine-learning-ii" class="nav-link active" data-scroll-target="#week-2-lecture-1---machine-learning-ii">Week 2, Lecture 1 - Machine Learning II</a>
  <ul class="collapse">
  <li><a href="#module-group-dro" id="toc-module-group-dro" class="nav-link" data-scroll-target="#module-group-dro">Module: Group DRO</a>
  <ul class="collapse">
  <li><a href="#linear-regression-the-standard-way" id="toc-linear-regression-the-standard-way" class="nav-link" data-scroll-target="#linear-regression-the-standard-way">Linear regression the standard way</a></li>
  <li><a href="#using-groups" id="toc-using-groups" class="nav-link" data-scroll-target="#using-groups">Using groups</a></li>
  </ul></li>
  <li><a href="#module-non-linear-features-offline" id="toc-module-non-linear-features-offline" class="nav-link" data-scroll-target="#module-non-linear-features-offline">Module: Non-linear features (offline)</a>
  <ul class="collapse">
  <li><a href="#quadratic-predictors" id="toc-quadratic-predictors" class="nav-link" data-scroll-target="#quadratic-predictors">Quadratic predictors</a></li>
  <li><a href="#piecewise-constant-predictors" id="toc-piecewise-constant-predictors" class="nav-link" data-scroll-target="#piecewise-constant-predictors">Piecewise constant predictors</a></li>
  <li><a href="#periodic-predictors" id="toc-periodic-predictors" class="nav-link" data-scroll-target="#periodic-predictors">Periodic predictors</a></li>
  <li><a href="#the-general-idea" id="toc-the-general-idea" class="nav-link" data-scroll-target="#the-general-idea">The general idea</a></li>
  <li><a href="#classifiers" id="toc-classifiers" class="nav-link" data-scroll-target="#classifiers">Classifiers</a></li>
  </ul></li>
  <li><a href="#module-feature-templates-offline" id="toc-module-feature-templates-offline" class="nav-link" data-scroll-target="#module-feature-templates-offline">Module: Feature templates (offline)</a></li>
  <li><a href="#module-neural-networks" id="toc-module-neural-networks" class="nav-link" data-scroll-target="#module-neural-networks">Module: Neural networks</a></li>
  <li><a href="#module-backpropagation" id="toc-module-backpropagation" class="nav-link" data-scroll-target="#module-backpropagation">Module: Backpropagation</a>
  <ul class="collapse">
  <li><a href="#functions-as-boxes" id="toc-functions-as-boxes" class="nav-link" data-scroll-target="#functions-as-boxes">Functions as boxes</a></li>
  </ul></li>
  <li><a href="#module-differentiable-programming-optional" id="toc-module-differentiable-programming-optional" class="nav-link" data-scroll-target="#module-differentiable-programming-optional">Module: Differentiable programming (optional)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">



<section id="week-2-lecture-1---machine-learning-ii" class="level1 page-columns page-full">
<h1>Week 2, Lecture 1 - Machine Learning II</h1>
<p><code>Date: 10/3/22</code></p>
<section id="module-group-dro" class="level2">
<h2 class="anchored" data-anchor-id="module-group-dro">Module: Group DRO</h2>
<blockquote class="blockquote">
<p>“How to ensure more equitable performance.”</p>
</blockquote>
<ul>
<li><strong>Motivation</strong>:
<ul>
<li>Thus far, our goal has been to minimize training loss with the hope that our model will perform well on new examples.</li>
<li>However, train loss is defined as the average of all individual losses on each sample.</li>
<li>Equally weighting every sample loss can lead to problems with equity.</li>
<li>For example, unrepresented groups in the training data will have less representation in the loss and therefore in the model.</li>
<li>group distributional robust optimization (group DRO) can help mitigate some of these inequalities.</li>
<li>tl;dr: averaging training losses can lead to inequalities across groups, group DRO can help mitigate this.</li>
</ul></li>
<li><strong>Example</strong>:
<ul>
<li>Gender Shades project by Joy Buolamwini and Timnit Gebru discovered that Microsoft, Face++, and IBM’s gender classification system performed worse on darker-skinned females, while lighter-skinned data samples got nearly 100% accuracy.
<ul>
<li>i.e., discrimination can arise from ML</li>
</ul></li>
<li>Has real life consequences: “Robert Julian-Borchak Williams was wrongly arrested due to a incorrect match with another Black man captured from a surveillance video, and this mistake was made by a facial recognition system”.</li>
</ul></li>
</ul>
<p>How can we help mitigate this algorithmically?</p>
<section id="linear-regression-the-standard-way" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression-the-standard-way">Linear regression the standard way</h3>
<p>Take the following dataset, where each sample now as an addtional meta-datum: a group (e.g., some demographic).</p>
<p align="center">
<img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/203a10115a09ff5f74c0cdc06106bcd6d5d683946084e8bfa78f6cb4862644ec.png" width="75">
</p>
<p>Let’s setup the same model we used in last lecture’s linear regression module:</p>
<p><span class="math display">\[
f_{\mathbf{w}}(x)=\mathbf{w} \cdot \phi(x) \quad \mathbf{w}=[w] \quad \phi(x)=[x]
\]</span></p>
<blockquote class="blockquote">
<p>Important note: we don’t use (i.e., the predictor <span class="math inline">\(f_{\mathbf{w}}\)</span>) does not use group information <span class="math inline">\(g\)</span> to make a prediction.</p>
</blockquote>
<p>And since this is linear regression, all models will be a line that passes through the origin, differing in the slope <span class="math inline">\(w\)</span>. Here is a plot of the training samples with the identity line:</p>
<p align="center">
<img alt="picture 3" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/f35e283529f869898fc49472936d28da35fd65511dd58be434ea94a901246faf.png" width="300">
</p>
<p>As we can see, certain values for <span class="math inline">\(w\)</span> will be more beneficial for the blue group than the orange group. There is a bit of tension.</p>
<p>The classic way to solve a linear regression problem here is to minimize the train loss which is an <strong>average</strong> over all sample losses:</p>
<p><span class="math display">\[
min \operatorname{TrainLoss}(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}} \operatorname{Loss}(x, y, \mathbf{w})
\]</span></p>
<p>with the above data points (and <span class="math inline">\(\operatorname{Loss}(x, y, \mathbf{w})=\left(f_{\mathbf{w}}(x)-y\right)^2\)</span>), we have:</p>
<p><span class="math display">\[
\operatorname{TrainLoss}(1)=\frac{1}{6}\left((1-4)^2+(2-8)^2+(5-5)^2+(6-6)^2+(7-7)^2+(8-8)^2\right)=7.5.
\]</span></p>
<p>doing some calculations, we see that <span class="math inline">\(\mathbf{w}=1.09\)</span> produces the minimum train loss:</p>
<p align="center">
<img alt="picture 5" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/8af2e036628b68f4d09559683fc2c6efeee466a62848b36b168d48ca5f7c3527.png" width="200">
</p>
</section>
<section id="using-groups" class="level3">
<h3 class="anchored" data-anchor-id="using-groups">Using groups</h3>
<p>Ok… let’s change one thing up (loss!) a couple of times.</p>
<blockquote class="blockquote">
<p><strong>Let’s calculate the loss for each group separately</strong>.</p>
</blockquote>
<section id="per-group-loss" class="level4">
<h4 class="anchored" data-anchor-id="per-group-loss">Per-group loss</h4>
<p>From above, we define the train loss for some general group <span class="math inline">\(g\)</span> as</p>
<p><span class="math display">\[
\operatorname{TrainLoss}_g(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}(g)\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}(g)} \operatorname{Loss}(x, y, \mathbf{w}).
\]</span></p>
<p>plugging in for the above dataset:</p>
<p><span class="math display">\[\operatorname{TrainLoss}_{\mathrm{A}}(1)=\frac{1}{2}\left((1-4)^2+(2-8)^2\right)=22.5\]</span></p>
<p><span class="math display">\[\operatorname{TrainLoss}_{\mathrm{B}}(1)=\frac{1}{4}\left((5-5)^2+(6-6)^2+(7-7)^2+(8-8)^2\right)=0\]</span></p>
<blockquote class="blockquote">
<p><strong>Conclusion</strong>: huge performance drop-off from 0 to 22.5 loss for the underrepresented group (A). So, although the average loss is 7.5, we see the disparity for ourselves here. Many people won’t go this deep and assume their “7.5” loss is good enough.</p>
</blockquote>
<p>Also, check this out:</p>
<p align="center">
<img alt="picture 6" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/82657168bc8bda0c533b977607a98e1548df8361a95946696ee0ea9f508d3b15.png" width="300">
</p>
<p>see how the minimum for group A and group B mean picking different values for <span class="math inline">\(\mathbf{w}\)</span>. But of course, we can only pick one <span class="math inline">\(\mathbf{w}\)</span>. Tension, tension!</p>
<p>Ok cool… but usually it is helpful to have just <em>one</em> value for the loss. I agree:</p>
</section>
<section id="maximum-group-loss" class="level4">
<h4 class="anchored" data-anchor-id="maximum-group-loss">Maximum group loss</h4>
<p>We can literally just perform per-group losses and just take the max of all of them (i.e., the worst case):</p>
<p><span class="math display">\[
\operatorname{TrainLoss}_{\max }(\mathbf{w})=\max _g \operatorname{TrainLoss}_g(\mathbf{w}).
\]</span></p>
<p align="center">
<img alt="picture 7" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/68003fb50fdb35b72c8b4e494e9bad33f352b4736f5e8b8720b51da77f32408a.png" width="300">
</p>
<p>This function is the pointwise maximum of the per-group losses, which you can see on the plot as taking the upper envelope of the two per-group losses (cs 221).</p>
<p>We call this group distributionally robust optimization.</p>
</section>
<section id="average-vs.-group-dro" class="level4">
<h4 class="anchored" data-anchor-id="average-vs.-group-dro">Average vs.&nbsp;group DRO</h4>
<p>Alright… here are the results:</p>
<p align="center">
<img alt="picture 8" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/c52ed0799b4138af07d7659332457f601131fc24b0e8d012a18c1e27d72aaabf.png" width="300">
</p>
<p>Here, a weight vector <span class="math inline">\(\mathbf{w}=1.09\)</span> minimizes the loss for the average (traditional) case. However, using group DRO, we get <span class="math inline">\(\mathbf{w}=1.58\)</span>. So using <span class="math inline">\(\mathbf{w}=1.58\)</span> might produce worse results but should be more equitable (see the learned purple line vs.&nbsp;the learning red line).</p>
<blockquote class="blockquote">
<p>“Intuitively, the average loss favors majority groups over minority groups, but the maximum group loss gives a stronger voice to the minority groups, and as we see here, their influence is felt to a greater extent.” - CS 221.</p>
</blockquote>
</section>
<section id="note-on-gradient-descent" class="level4">
<h4 class="anchored" data-anchor-id="note-on-gradient-descent">Note on gradient descent</h4>
<p>From the 221 slides:</p>
<ul>
<li>In general, we can minimize the maximum group loss by gradient descent, as usual.</li>
<li>We just have to be able to take the gradient of TrainLoss <span class="math inline">\(_{\max }\)</span> , which is a maximum over the per-group losses.</li>
<li>The gradient of a max is simply the gradient of the term that achieves the max.</li>
<li>So algorithmically, it’s very intuitive: you first compute whichever group (<span class="math inline">\(g^*\)</span>) has the highest loss, and then you just evaluate the gradient only on per-group loss of that group (<span class="math inline">\(g^*\)</span>).</li>
</ul>
<p>In math:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\operatorname{TrainLoss}_{\max }(\mathbf{w})=\max _g \operatorname{TrainLoss}_g(\mathbf{w}) \\
&amp;\nabla \operatorname{TrainLoss}_{\max }(\mathbf{w})=\nabla \operatorname{TrainLoss} g_{g^*}(\mathbf{w}) \\
&amp;\text { where } g^*=\arg \max _g \operatorname{TrainLoss}_g(\mathbf{w})
\end{aligned}.
\]</span></p>
<p>Note that SGD cannot be applied in vanilla form since its not an average, it is a max of an average<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>Some other notes to consider: - When we have multiple attributes (gender, race), intersectionality becomes difficult to incorporate. - For further reading, consider checking out the book <a href="https://fairmlbook.org/">Fairness and machine learning: Limitations and Opportunities</a>, and a <a href="https://arxiv.org/pdf/1911.08731.pdf">paper</a> that explores group DRO for neural networks.</p>
</section>
</section>
</section>
<section id="module-non-linear-features-offline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="module-non-linear-features-offline">Module: Non-linear features (offline)</h2>
<blockquote class="blockquote">
<p>How to get non-linear functions from linear machinery.</p>
</blockquote>
<ul>
<li>Linear regression/classification: fit some straight line to some data: <span class="math inline">\(\mathcal{F}=\left\{f_{\mathbf{w}}(x)=\mathbf{w} \cdot \phi(x): \mathbf{w} \in \mathbb{R}^d\right\}\)</span>.</li>
<li>Recall <strong>hypothesis class</strong>: what are the possible predictors that the learning algorithm can consider?</li>
<li>Works well for linear data.</li>
</ul>
<p>However… not all data is linear:</p>
<p align="center">
<img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/165d8f6e3179429b28c5c591e918d1d13756554f093e9306a4c8ca5651bd8e96.png" width="300">
</p>
<p><strong>how can we fit a non-linear predictor to generalize for this data?</strong></p>
<section id="quadratic-predictors" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-predictors">Quadratic predictors</h3>
<ul>
<li><strong>Change the feature extractor to include non-linear components/features</strong>. Example:</li>
</ul>
<p><span class="math display">\[\phi(x)=\left[1, x, x^2\right].\]</span></p>
<p>Example: <span class="math inline">\(\phi(3)=[1,3,9]\)</span>. With some various weight combinations, we’d get something like:</p>
<p align="center">
<img alt="picture 2" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/15b6b0118a82127331156e1ab10446f85bfd0fef443f22678dc21e4163808916.png" width="250">
</p>
<ul>
<li>Note that by setting the weight for feature <span class="math inline">\(x^2\)</span> to zero, we recover linear predictors.</li>
<li>Here, the hypothesis class is the set of all predictors <span class="math inline">\(f_{\mathbf{w}}\)</span> obtained by varying <span class="math inline">\(\mathbf{w}\)</span>.</li>
</ul>
</section>
<section id="piecewise-constant-predictors" class="level3">
<h3 class="anchored" data-anchor-id="piecewise-constant-predictors">Piecewise constant predictors</h3>
<ul>
<li>Quadratic only can fit smooth and linear curves. But we…</li>
<li>Can carry on this idea to other math forms to better capture the input space.</li>
<li>Piecewise: expressive non-linear predictors by <strong>partitioning the input space</strong>.</li>
</ul>
<p><span class="math display">\[
\phi(x)=[\mathbf{1}[0&lt;x \leq 1], \mathbf{1}[1&lt;x \leq 2], \mathbf{1}[2&lt;x \leq 3], \mathbf{1}[3&lt;x \leq 4], \mathbf{1}[4&lt;x \leq 5]]
\]</span></p>
<p>Example: <span class="math inline">\(\phi(2.3)=[0,0,1,0,0]\)</span>. Visual with:</p>
<p><span class="math display">\[f(x)=[1,2,4,4,3] \cdot \phi(x)\]</span> <span class="math display">\[f(x)=[4,3,3,2,1.5] \cdot \phi(x)\]</span> <span class="math display">\[\mathcal{F}=\left\{f_{\mathbf{w}}(x)=\mathbf{w} \cdot \phi(x): \mathbf{w} \in \mathbb{R}^5\right\}\]</span></p>
<p align="center">
<img alt="picture 4" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/3b9f107d941fdd2f998c174e57f686b5b7d815068b019e30577bb4d88c9661b8.png" width="300">
</p>
<ul>
<li>Each component of the feature vector corresponds to one region and is 1 if <span class="math inline">\(x\)</span> lies in that region and <span class="math inline">\(0\)</span> otherwise.</li>
<li>Predicted value is just the weight.</li>
<li>Can make regions smaller. In the limit, we’d be able to capture any predictor we want.</li>
<li>But if the input is multi-dimensional (<span class="math inline">\(d\)</span>), then we’d have <span class="math inline">\(B^{d}\)</span> features where <span class="math inline">\(B\)</span> is number of bins.</li>
</ul>
</section>
<section id="periodic-predictors" class="level3">
<h3 class="anchored" data-anchor-id="periodic-predictors">Periodic predictors</h3>
<ul>
<li>Add in a trigonometric function in the feature extractor.</li>
</ul>
<p><span class="math display">\[\phi(x)=\left[1, x, x^2, \cos (3 x)\right]\]</span></p>
<p align="center">
<img alt="picture 5" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/634e89712d73262b43f06c99bc92118b0a33c8e39921fb5e27511173e0753082.png" width="300">
</p>
</section>
<section id="the-general-idea" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-general-idea">The general idea</h3>
<ul>
<li>Key idea has been to <strong>incorporate a non-linear math function in the feature extractor</strong>.</li>
<li>Generally, <strong>the choice of features is informed by the prediction task</strong> that we wish to solve</li>
<li>For example, if <span class="math inline">\(x\)</span> represents time and we believe the true output <span class="math inline">\(y\)</span> varies according to some periodic structure, we can incorporate a periodic feature in the feature extractor.</li>
<li>Features represent what properties might be useful for prediction. If a feature is not useful, then the learning algorithm can assign a weight close to zero to that feature. Of course, the more features one has, the harder learning becomes.</li>
</ul>
<p>Note:</p>
<ul>
<li>We still have a linear classifier as the simple dot product <span class="math inline">\(\mathbf{w} \cdot \phi(x)\)</span> is indeed linear in <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\phi(x)\)</span>. However, the feature extractor components can mean that we aren’t linear in just <span class="math inline">\(x\)</span>.</li>
<li>The significance is that we can acheive non-linearity through the FE but the learning optimizer/algorithm thinks we just have a linear predictor (so optimizing the weights is still efficient!)</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p>Aside (<strong>convexity</strong>): if the score (logit) is linear in just <span class="math inline">\(\mathbf{W}\)</span> (i.e., <span class="math inline">\(\mathbf{W}\)</span> is a linear vector (as it usually is as its just scalar numbers) and <span class="math inline">\(\texttt{Loss}\)</span> is convex, then gradient descent is gaurenteed to converge to a global minimum.)</p>
</div></div><div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 </strong></span><strong>(Non-linearity)</strong>:</p>
<ul>
<li><em>Expressivity</em>: score w <span class="math inline">\(\boldsymbol{\phi}(\boldsymbol{x})\)</span> can be a non-linear function of <span class="math inline">\(x\)</span></li>
<li><em>Efficiency</em>: score <span class="math inline">\(\mathbf{w} \cdot \phi(x)\)</span> always a linear function of <span class="math inline">\(\mathbf{w}\)</span></li>
</ul>
</div>
</div>
</div>
</section>
<section id="classifiers" class="level3">
<h3 class="anchored" data-anchor-id="classifiers">Classifiers</h3>
<ul>
<li>Regression to classification: instead of using the predictor curve (decision boundary) to predict <span class="math inline">\(y\)</span> from new <span class="math inline">\(x\)</span>’s, let’s use the curve to classify <span class="math inline">\(x\)</span> and <code>positive</code> or <code>negative</code> (corresponding to the sides of the decision boundary). Recall <span class="math inline">\(f_{\mathbf{w}}(x)=\operatorname{sign}(\mathbf{w} \cdot \phi(x))\)</span>.</li>
<li>Non-linearity arises here then in that we can make the decision curve non-linear.</li>
</ul>
<p>From this:</p>
<p align="center">
<img alt="picture 6" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/4d7e60c2b7ca68a149e224469dc1e4903161f9550c4fa55deaad0eaf3fed7952.png" width="300">
</p>
<p>to this:</p>
<p align="center">
<img alt="picture 7" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/322376a519ec0156c9a36417c3bee50bfb95639c15f283522f0ec2ccaf721fe6.png" width="300">
</p>
<p>how? Introduce non-linear functions in feature extractor:</p>
<p><span class="math display">\[\phi(x)=\left[x_1, x_2, x_1^2+x_2^2\right],\]</span> <span class="math display">\[f(x)=\operatorname{sign}([2,2,-1] \cdot \phi(x))\]</span> or equivalently: <span class="math display">\[
f(x)= \begin{cases}1 &amp; \text { if }\left(x_1-1\right)^2+\left(x_2-1\right)^2 \leq 2 \\ -1 &amp; \text { otherwise }\end{cases}
\]</span></p>
<p>See <a href="https://stanford-cs221.github.io/autumn2022/modules/machine-learning/images/svm-polynomial-kernel.mp4">this</a> awesome video animation for understanding how we can still have a linearity in <span class="math inline">\(\phi(x)\)</span> but non-linearity in <span class="math inline">\(x\)</span>. Idea is that the raw <span class="math inline">\(x\)</span> isn’t linearly seperable, but we can project the raw <span class="math inline">\(x\)</span> into some different dimension (i.e., adding a feature changes the dimension of the vector) non-linearly (e.g., onto a polynomial in <span class="math inline">\(\mathbf{R}^{n + 1}\)</span>) which we can then have a linear hyperplane as the decision boundary (how SVMs work!).</p>
<p><strong>Question:</strong> how can we generalize a decision boundary for <span class="math inline">\(n\)</span>-way classification for <span class="math inline">\(n &gt; 2\)</span>? Answer: create <span class="math inline">\(n\)</span> binary classifiers!</p>
</section>
</section>
<section id="module-feature-templates-offline" class="level2">
<h2 class="anchored" data-anchor-id="module-feature-templates-offline">Module: Feature templates (offline)</h2>
<blockquote class="blockquote">
<p>How to design and organize features.</p>
</blockquote>
<p>Motivation: hypothesis class is too big because we let <span class="math inline">\(\mathbf{w}\)</span> vary freely.</p>
<p align="center">
<img alt="picture 9" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/c1c044d1d46866c6bce69bf922ffd8e39a0cf4df88e3e284351b703361fa1c64.png" width="300">
</p>
<p>We can narrow this down to the smaller circle by <strong>using a feature extractor</strong>. Then, choose <span class="math inline">\(f_{\mathrm{w}} \in \mathcal{F}\)</span> based on data. So how do we narrow this down and choose a good feature extractor?</p>
<p>In <span class="math inline">\(\mathbf{w} \cdot \phi(x)\)</span>, the weight corresponds to the magnitude of importance for the corresponding feature extraction component. The polarity is also assigned.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 </strong></span><strong>(Feature template)</strong>: A feature template is a group of features all computed in a similar way.</p>
</div>
</div>
</div>
<p>Example: say we have this task:</p>
<p align="center">
<img alt="picture 10" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/88074ffaf4936bdc57c1ed4d47ff77c27ab707465abe7fda5812892ae47b6180.png" width="300">
</p>
<p>given an input string, determine if this is a valid email address. We can try to hand-construct features such as “contains @”. But this isn’t a general framework. So instead, we can define feature templates where we say something like</p>
<ul>
<li>Template: “last three characters equals _ _ _”</li>
<li>Resultant features:</li>
</ul>
<p align="center">
<img alt="picture 11" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/e655a1e41948c3895c31290a99eff6af2a8780a8192fe0158273aff5d61d9355.png" width="5r00">
</p>
</section>
<section id="module-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="module-neural-networks">Module: Neural networks</h2>
<ul>
<li>In summary, acheive end-to-end non-linearity without needing to define a feature extractor.<br>
</li>
<li>Screw hand-configuring the feature extractor <span class="math inline">\(\phi(x)\)</span>. Throw this away. Then, define activation functions that take in the logits from each dot product. That is how we acheive non-linearity. And guess what?!: this is implicitly learned in gradient descent (why though?)!</li>
<li>Then, stack a bunch of these on top of each other to get a deep NN!</li>
</ul>
<p align="center">
<img alt="picture 12" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/a05747b399fef5cd060032555e79da0cc43738eb343fa8b6c729f76524eba58c.png" width="300">
</p>
<p>Notes:</p>
<ul>
<li>Can achieve non-linear classifier with feature extraction manipulation too. But…
<ul>
<li>Have to hand-learn features</li>
<li>as <span class="math inline">\(d\)</span>, dimension of vector increases, we need <span class="math inline">\(O\left(d^2\right)\)</span> features. Computationally hard.</li>
<li>neural networks yield non-linear predictors in a more compact way.</li>
</ul></li>
<li>Activation function avoids zero-gradients.</li>
</ul>
<p>Helpful slide:</p>
<p align="center">
<img alt="picture 13" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/3857d193e48b724af5564dffc6e454781d6b9cfcadcb397d9afe70742df86575.png" width="90%">
</p>
</section>
<section id="module-backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="module-backpropagation">Module: Backpropagation</h2>
<blockquote class="blockquote">
<p>Computation graphs and backpropagation algorithm for computing gradients to optimize the weights of neural networks.</p>
</blockquote>
<ul>
<li>For each time we want to update our weights in a neural network, we have to do so weight respect to each weight vector in the weight matrix. This wasn’t a problem for linear predictors because the gradient only deals with one parameter weight vector. Neural nets have many params we need to tune and calculate gradients with respect too!</li>
</ul>
<p><span class="math display">\[\operatorname{Loss}\left(x, y, \mathbf{V}_1, \mathbf{V}_2, \mathbf{V}_3, \mathbf{w}\right)=\left(\mathbf{w} \cdot \sigma\left(\mathbb{V}_3 \sigma\left(\mathbb{V}_2 \sigma\left(\mathbb{V}_1 \phi(x)\right)\right)\right)-y\right)^2,\]</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbf{V}_1 \leftarrow \mathbf{V}_1-\eta \nabla_{\mathbf{V}_1} \operatorname{Loss}\left(x, y, \mathbf{V}_1, \mathbf{V}_2, \mathbf{V}_3, \mathbf{w}\right) \\
&amp;\mathbf{V}_2 \leftarrow \mathbf{V}_2-\eta \nabla_{\mathbf{V}_2} \operatorname{Loss}\left(x, y, \mathbf{V}_1, \mathbf{V}_2, \mathbf{V}_3, \mathbf{w}\right) \\
&amp;\mathbf{V}_3 \leftarrow \mathbf{V}_3-\eta \nabla_{\mathbf{V}_3} \operatorname{Loss}\left(x, y, \mathbf{V}_1, \mathbf{V}_2, \mathbf{V}_3, \mathbf{w}\right) \\
&amp;\mathbf{w} \leftarrow \mathbf{w}-\eta \nabla_{\mathbf{w}} \operatorname{Loss}\left(x, y, \mathbf{V}_1, \mathbf{V}_2, \mathbf{V}_3, \mathbf{w}\right)
\end{aligned}
\]</span></p>
<ul>
<li>This is tedious and expensive. Instead, we can create a <strong>computation graph</strong> and see the relaton between gradients. This is the main idea behind the <strong>backpropogation algorithm</strong>.
<ul>
<li>We harness the idea that the gradients of e.g., <span class="math inline">\(\mathbf{V}_3\)</span> depend on <span class="math inline">\(\mathbf{V}_2\)</span> and <span class="math inline">\(\mathbf{V}_1\)</span> gradients.</li>
</ul></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 </strong></span><strong>(Computation graph)</strong>: A directed acyclic graph whose root node represents the final mathematical expression and each node represents intermediate subexpressions.</p>
</div>
</div>
</div>
<section id="functions-as-boxes" class="level3">
<h3 class="anchored" data-anchor-id="functions-as-boxes">Functions as boxes</h3>
<p>The main idea is to think of functions as boxes and calculate the gradient upwards with respect to each leave node. Example:</p>
<p align="center">
<img alt="picture 14" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/fb69b697a228508ef7a2665c45b4a09c87b88fc84e6f74566d8552988044d8be.png" width="600">
</p>
<p>we can define these for some basic, common functions (<strong>basic building blocks</strong>):</p>
<p align="center">
<img alt="picture 15" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/37c1741c3d36b91abeb982799ed93c0287b43fdcf51e39d7a1229b4b38698937.png" width="70%">
</p>
<p>we can now compose these building blocks to make more complicated functions. Example:</p>
<p align="center">
<img alt="picture 16" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/fdddb0958fe35f6e94be4e26465fff69748123e679c3d0f613da8fac7601602e.png" width="125">
</p>
<p>by chain rule (note <span class="math inline">\(b=a^2\)</span>), we have <span class="math inline">\(\frac{\partial c}{\partial a}=\frac{\partial c}{\partial b} \frac{\partial b}{\partial a}=(2 b)(2 a)=\left(2 a^2\right)(2 a)=4 a^3\)</span>. To obtain complete gradient, multiply up the graph. We can also hash into specific points to get the individual gradients with respect to the successor node (which is exactly what we need for autodiff’ing neural nets! because we can get gradients with respect to <span class="math inline">\(\mathbf{V}_3\)</span>, <span class="math inline">\(\mathbf{V}_2\)</span>, <span class="math inline">\(\mathbf{V}_1\)</span> just by hashing into a dict instead of re-calculating (kinda like memoization!)).</p>
</section>
</section>
<section id="module-differentiable-programming-optional" class="level2">
<h2 class="anchored" data-anchor-id="module-differentiable-programming-optional">Module: Differentiable programming (optional)</h2>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I don’t think I comprehend <a href="https://stanford-cs221.github.io/autumn2022/modules/module.html#include=machine-learning%2Fgroup-dro.js&amp;slideId=training-via-gradient-descent-prose&amp;level=0">this</a> well enough.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>