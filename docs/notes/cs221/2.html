<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Knowledge – quarto-input270acd6b</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Knowledge</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html">Stanford Notes</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../machine-learning.html">Machine Learning</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../concepts.html">Concepts</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#week-1-lecture-2---machine-learning-i" id="toc-week-1-lecture-2---machine-learning-i" class="nav-link active" data-scroll-target="#week-1-lecture-2---machine-learning-i">Week 1, Lecture 2 - Machine Learning I</a>
  <ul class="collapse">
  <li><a href="#module-overview" id="toc-module-overview" class="nav-link" data-scroll-target="#module-overview">Module: Overview</a>
  <ul class="collapse">
  <li><a href="#reflex-based-models" id="toc-reflex-based-models" class="nav-link" data-scroll-target="#reflex-based-models">Reflex-based models</a></li>
  <li><a href="#roadmap" id="toc-roadmap" class="nav-link" data-scroll-target="#roadmap">Roadmap</a></li>
  </ul></li>
  <li><a href="#module-linear-regression" id="toc-module-linear-regression" class="nav-link" data-scroll-target="#module-linear-regression">Module: linear regression</a>
  <ul class="collapse">
  <li><a href="#modeling" id="toc-modeling" class="nav-link" data-scroll-target="#modeling">Modeling</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss function</a></li>
  <li><a href="#optimization" id="toc-optimization" class="nav-link" data-scroll-target="#optimization">Optimization</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul></li>
  <li><a href="#module-classification" id="toc-module-classification" class="nav-link" data-scroll-target="#module-classification">Module: Classification</a>
  <ul class="collapse">
  <li><a href="#score-and-margin" id="toc-score-and-margin" class="nav-link" data-scroll-target="#score-and-margin">Score and margin</a></li>
  <li><a href="#introducing-hinge-loss" id="toc-introducing-hinge-loss" class="nav-link" data-scroll-target="#introducing-hinge-loss">Introducing hinge loss</a></li>
  </ul></li>
  <li><a href="#module-stochastic-gradient-descent-offline" id="toc-module-stochastic-gradient-descent-offline" class="nav-link" data-scroll-target="#module-stochastic-gradient-descent-offline">Module: Stochastic Gradient Descent (Offline)</a>
  <ul class="collapse">
  <li><a href="#step-size" id="toc-step-size" class="nav-link" data-scroll-target="#step-size">Step size</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="week-1-lecture-2---machine-learning-i" class="level1">
<h1>Week 1, Lecture 2 - Machine Learning I</h1>
<p><code>Date: 9/28/22</code></p>
<section id="module-overview" class="level2">
<h2 class="anchored" data-anchor-id="module-overview">Module: Overview</h2>
<p>We now begin our journey into ML. Next 3 lectures. Reflex-based models to logic.</p>
<section id="reflex-based-models" class="level3">
<h3 class="anchored" data-anchor-id="reflex-based-models">Reflex-based models</h3>
<ul>
<li><strong>Reflex-based models</strong>: a predictor <span class="math inline">\(f\)</span> takes some input <span class="math inline">\(x\)</span> and produces some output <span class="math inline">\(y\)</span>.
<ul>
<li>Input can usually be anything but output is usually restricted (output defines the task).</li>
</ul></li>
</ul>
<p align="center">
<img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/8f28e0641834c9a710da834d765664f93896856acafec0c44b71edda2cbfa6b1.png" width="300">
</p>
<p>First examples:</p>
<ul>
<li><strong>Binary classifier</strong>: restrict <span class="math inline">\(y\)</span>: <span class="math inline">\(x \longrightarrow f \rightarrow y \in\{+1,-1\} \text { label }\)</span>.</li>
</ul>
<p align="center">
<img alt="picture 2" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/c11c0df8a65845181572c1f5cc2b7704301c460cdd59e916523d73d8709654f5.png" width="300">
</p>
<ul>
<li><strong>Regression</strong>: <span class="math inline">\(x \longrightarrow f \rightarrow y \in \mathbb{R} \text { response }\)</span>.
<ul>
<li>Arbitrary input with output restricted to some real number.</li>
<li>Infinite continuous (whereas classification is finite discrete)</li>
<li>Output is called the <strong>response</strong>: <span class="math inline">\(y \in \mathbb{R}\)</span>.</li>
</ul></li>
</ul>
<p align="center">
<img alt="picture 4" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/32e37143b9c347739ecf229ad9493aeace4a380128815f77b0b9065e884f590a.png" width="300">
</p>
<ul>
<li><strong>Structured prediction</strong>: catch-all generalization for non-categoricalizable machine learning problems.
<ul>
<li>e.g., <em>image segmentation</em>. Most tasks reduce to series of multi-class categorization problems. For example, image segmentation reduces to <span class="math inline">\(m \times n\)</span> multi-class (or binary) classification problems where <span class="math inline">\(m, n\)</span> are the dimensions of the image in pixels.</li>
</ul></li>
</ul>
<p align="center">
<img alt="picture 3" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/37c9a1710b3a460ebdd850310a3accd04c45a4803bd0eab73007ee53a6ed06d6.png" width="300">
</p>
</section>
<section id="roadmap" class="level3">
<h3 class="anchored" data-anchor-id="roadmap">Roadmap</h3>
<ul>
<li><strong>Tasks</strong>:
<ul>
<li>classification</li>
<li>regression</li>
</ul></li>
<li><strong>Algorithms</strong>:
<ul>
<li>SGD</li>
<li>backpropogation</li>
</ul></li>
<li><strong>Models</strong>:
<ul>
<li>non-linear features</li>
<li>feature templates</li>
<li>neural networks</li>
<li>differentiable programming</li>
</ul></li>
<li><strong>Misc. considerations</strong>:
<ul>
<li>Group DRO (unbiasing objectives)</li>
<li>Generalization</li>
</ul></li>
</ul>
</section>
</section>
<section id="module-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="module-linear-regression">Module: linear regression</h2>
<p><strong>History</strong>: Ceres (asteroid) went behind the sun and scientists wanted to know when it would be observed again. Using data points gathered before it went behind the sun, Gauss used linear algebra to invent least squares linear regression to solve this task.</p>
<p><strong>General framework</strong>:</p>
<p align="center">
<img alt="picture 5" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/13980a77c09d5b32de8247244e965853018b722f8cffdc0d56561d6f38fc6ecf.png" width="500">
</p>
<ul>
<li><strong>Hypothesis</strong> class: which predictors (i.e.&nbsp;models) are possible?
<ul>
<li>Linear, quadratic, neural network, etc. These all belong to the hypothesis class of the process.</li>
</ul></li>
<li>How <strong>good</strong> is predictor: loss function</li>
<li>How to compute best fit curve: <strong>optimization</strong> <em>algorithm</em>
<ul>
<li>Though looping through all possible predictors is computationally impossible so we use optimization to numerically approximate best fit curve. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
</ul></li>
</ul>
<section id="modeling" class="level3">
<h3 class="anchored" data-anchor-id="modeling">Modeling</h3>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 </strong></span><strong>Hypothesis class</strong>: When we say hypothesis class, we mean the <em>range</em> of predictors that we can learn. For example, linear classifiers are of the form <span class="math inline">\(f(x)=w_1+w_2 x\)</span> for some <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>:</p>
<p align="center">
<img alt="picture 7" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/3ec8f8f0b52d958ff1ff565dfb7f4e06d0fa8183f3efc035549c132f67497c66.png" width="300">
</p>
</div>
</div>
</div>
<ul>
<li>This is cool and all, but if we want to add complexity, we should keep our notation simple. So here we will introduce the above with <strong>vector notation</strong>.</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 </strong></span><strong>Vector notation for linear regression</strong>:</p>
<ul>
<li><p><strong>Weight vector</strong>: <span class="math display">\[
\mathbf{w}=\left[w_1, w_2\right]
\]</span></p></li>
<li><p><strong>Feature extractor</strong>:</p></li>
</ul>
<p><span class="math display">\[
\phi(x)=[1, x]
\]</span></p>
<p>Takes in input vector <span class="math inline">\(x\)</span> and returns transformed version of <span class="math inline">\(x\)</span> to be fed into the model. We do this such that the input can be of the proper shape for the matrix multiplication that is to come or just oe extract general features that might lead to better performance. - <strong>Prediction (score)</strong>:</p>
<p><span class="math display">\[
f_{\mathbf{w}}(x)=\mathbf{w} \cdot \phi(x)
\]</span></p>
</div>
</div>
</div>
<p>Example: <span class="math inline">\(f_{\mathbf{w}}(3)=[1,0.57] \cdot[1,3]=2.71\)</span>. Hypothesis class: <span class="math inline">\(\mathcal{F}=\left\{f_{\mathbf{w}}: \mathbf{w} \in \mathbb{R}^2\right\}\)</span>. i.e., a set of weight real-valued functions.</p>
<p>Next let’s move onto the <strong>loss function</strong>.</p>
</section>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss function</h3>
<ul>
<li>How good is a predictor? Here we “define” good.</li>
<li>Example: <strong>least residual squares</strong>:</li>
</ul>
<p align="center">
<img alt="picture 8" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/2885d16c7368d927320cc967ae65fcca62a5f079e1732176407a4d9370c11226.png" width="300">
</p>
<p><span class="math display">\[\operatorname{Loss}(x, y, \mathbf{w})=\left(f_{\mathbf{w}}(x)-y\right)^2\]</span></p>
<p>The above equation defines the loss for <em>one data point</em>. During training, we’d average all of the individual losses on each data point and define the <strong>training loss</strong>:</p>
<p><span class="math display">\[\operatorname{TrainLoss}(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}}\]</span></p>
</section>
<section id="optimization" class="level3">
<h3 class="anchored" data-anchor-id="optimization">Optimization</h3>
<blockquote class="blockquote">
<p>How the heck do we find a good predictor? Optimization! Gradient descent algorithm.</p>
</blockquote>
<p>We want to <span class="math inline">\(\min _{\mathbf{w}} \operatorname{TrainLoss}(\mathbf{w})\)</span>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 </strong></span><strong>Gradient</strong>: The gradient <span class="math inline">\(\nabla_{\mathbf{w}} \operatorname{TrainLoss}(\mathbf{w})\)</span> is the direction that increases the training loss the most.</p>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-gd" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 </strong></span><strong>Gradient descent</strong>:</p>
<ul>
<li>Initialize <span class="math inline">\(\mathbf{w}=[0, \ldots, 0]\)</span></li>
<li>For <span class="math inline">\(t=1, \ldots, T\)</span> : epochs
<ul>
<li><span class="math inline">\(\mathbf{w} \leftarrow \mathbf{w}-\underbrace{\eta}_{\text {step size }} \underbrace{\nabla_{\mathbf{w}} \operatorname{TrainLoss}(\mathbf{w})}_{\text {gradient }}\)</span>.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<p>How does this relate back to lin. reg.? Well, really, we are just optimizing the <strong>parameters</strong> (<span class="math inline">\(\mathbf{w}\)</span>) of the <strong>linear regression model</strong>.</p>
<p>Ok the above is kind of abstract. But how do we actually compute the gradient for a full loss objective.</p>
<p>Here is an example with the previous objective function…</p>
<div id="exm-default" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 </strong></span><strong>Computing the gradient of objective functions</strong>:</p>
<p>Objective function:</p>
<p><span class="math display">\[\operatorname{TrainLoss}(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}}(\mathbf{w} \cdot \phi(x)-y)^2\]</span></p>
<p>Gradient (use the chain rule!) with respect to <span class="math inline">\(\mathbf{w}\)</span>:</p>
<p><span class="math display">\[\nabla_{\mathbf{w}} \operatorname{TrainLoss}(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}} 2(\underbrace{\mathbf{w} \cdot \phi(x)-y}_{\text {prediction-target }}) \phi(x)\]</span></p>
</div>
<p>A couple tips and points:</p>
<ul>
<li><p>gradient of the objective function is usually with respect to the weight vector <span class="math inline">\(\mathbf{w}\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p></li>
<li><blockquote class="blockquote">
<p><em>Percy</em>’s tip: for differentiating vector/matrix functions, just do the scalar case first and then see if they match up. In theory, they should because a vectorization is just a generalization of a scalar. Also <a href="https://www.youtube.com/watch?v=VQDtSZ5A0xM">this video</a> from Princeton’s COS 302 class should help with differentiating vector and matrix-valued functions.</p>
</blockquote></li>
</ul>
<p>Also, this can be done algebraically and we can hand-calculate <span class="math inline">\(\mathbf{w}\)</span> using gradient descent. <a href="https://stanford-cs221.github.io/autumn2022/modules/module.html#include=machine-learning%2Flinear-regression.js&amp;slideId=gradient-descent-example&amp;level=0">See lecture slides</a> for an example.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Gradient descent in code</strong>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimization problem</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>trainExamples <span class="op">=</span> [</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">2</span>, <span class="dv">3</span>),</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">3</span>),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> phi(x):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([<span class="dv">1</span>, x])</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> initialWeightVector():</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.zeros(<span class="dv">2</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trainLoss(w):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> <span class="bu">len</span>(trainExamples) <span class="op">*</span> <span class="bu">sum</span>((w.dot(phi(x)) <span class="op">-</span> y)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> x, y <span class="kw">in</span> trainExamples)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradientTrainLoss(w):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> <span class="bu">len</span>(trainExamples) <span class="op">*</span> <span class="bu">sum</span>(<span class="dv">2</span> <span class="op">*</span> (w.dot(phi(x)) <span class="op">-</span> y) <span class="op">*</span> phi(x) <span class="cf">for</span> x, y <span class="kw">in</span> trainExamples)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimization algorithm</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradientDescent(F, gradientF, initialWeightVector):</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> initialWeightVector()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> F(w)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        gradient <span class="op">=</span> gradientF(w)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> w <span class="op">-</span> eta <span class="op">*</span> gradient</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'epoch </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: w = </span><span class="sc">{</span>w<span class="sc">}</span><span class="ss">, F(w) = </span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">, gradientF = </span><span class="sc">{</span>gradient<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>gradientDescent(trainLoss, gradientTrainLoss, initialWeightVector)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p align="center">
<img alt="picture 9" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/bde600a095faaaf285a48cba57fc133d8db577585dc6eb470a1aebeda94a31c8.png" width="550">
</p>
</section>
</section>
<section id="module-classification" class="level2">
<h2 class="anchored" data-anchor-id="module-classification">Module: Classification</h2>
<ul>
<li>The setup and framework is essentially the same (model, loss, optimization), but the dimensionalities change.</li>
<li>Change is that we have a <strong>linear separator</strong> for the predictor which acts as a decision boundary. This is for binary problems.</li>
</ul>
<p><strong>Framework</strong>:</p>
<p align="center">
<img alt="picture 10" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/9c3c85a5d6dfc57bb32752b7ae48862a34c905c6f9f1d6d376f04c54df4a3fe7.png" width="550">
</p>
<ul>
<li><strong>Binary linear classifier</strong>: <span class="math inline">\(f_{\mathbf{w}}(x)=\operatorname{sign}(\mathbf{w} \cdot \phi(x))\)</span>.</li>
<li><strong>Hypothesis class</strong>: <span class="math inline">\(\mathcal{F}=\left\{f_{\mathbf{w}}: \mathbf{w} \in \mathbb{R}^2\right\}\)</span>.</li>
<li><strong>Loss</strong>:
<ul>
<li><em>zero-one</em>: <span class="math inline">\(\operatorname{Loss}_{0-1}(x, y, \mathbf{w})=\mathbf{1}\left[f_{\mathbf{w}}(x) \neq y\right]\)</span>
<ul>
<li>If correct classification, loss is 1. Else, loss is 0.</li>
</ul></li>
<li><em>hinge</em>: fixing the problems with the zero-one loss.</li>
</ul></li>
</ul>
<section id="score-and-margin" class="level3">
<h3 class="anchored" data-anchor-id="score-and-margin">Score and margin</h3>
<p>Detour. New concepts.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 </strong></span><strong>Score</strong>:</p>
<ul>
<li>In regression, this is just the <strong>prediction</strong> (model output).</li>
<li>In classification, <strong>confidence</strong> in which we are predicting (+1):
<ul>
<li>The score on an example <span class="math inline">\((x, y)\)</span> is <span class="math inline">\(\mathbf{w} \cdot \phi(x)\)</span>, how confident we are in predicting <span class="math inline">\(+1\)</span>.
<ul>
<li>This essentially is the number before taking the sign in the prediction from the model.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 </strong></span><strong>(Margin)</strong>:</p>
<ul>
<li>The margin on an example <span class="math inline">\((x, y)\)</span> is <span class="math inline">\((\mathbf{w} \cdot \phi(x)) y\)</span>, how correct we are. Score times <span class="math inline">\(y\)</span>.</li>
<li>More: larger the margin, the more correct. If <span class="math inline">\(y=1\)</span>, then score (raw prediction) needs to be very high to have good margin. Vise versa if <span class="math inline">\(y=-1\)</span>.</li>
</ul>
</div>
</div>
</div>
<p><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<blockquote class="blockquote">
<p>Note that these concepts help us delve deeper into the nuances of the classifer as opposed to just seeing whether the predictor is right or wrong. This will help us transform the zero-one loss into hinge.</p>
</blockquote>
<p><strong>Problems with zero-one loss</strong>:</p>
<p align="center">
<img alt="picture 12" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/903f88b55fce6930339f1882c3e0f7dbdda809d3a2b5133d8f82270a5d8fabb9.png" width="300">
</p>
<p><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<ol type="1">
<li>non-differentiable</li>
<li>flat curve means zero-gradients almost everywhere: thus, GD will get stuck!</li>
</ol>
</section>
<section id="introducing-hinge-loss" class="level3">
<h3 class="anchored" data-anchor-id="introducing-hinge-loss">Introducing hinge loss</h3>
<p>To resolve the above problems, we now introduce the <em>hinge loss</em>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 </strong></span><strong>(Hinge loss)</strong></p>
<ul>
<li>In words: the maximum over a descending line and the zero function. In other words,
<ul>
<li>If the margin is at least 1, then the hinge loss is zero.</li>
<li>If the margin is less than 1, then the hinge loss rises linearly.</li>
</ul></li>
<li>In math:</li>
</ul>
<p><span class="math display">\[\operatorname{Loss}_{\text {hinge }}(x, y, \mathbf{w})=\max \{1-(\mathbf{w} \cdot \phi(x)) y, 0\}\]</span></p>
<ul>
<li>Now visually:</li>
</ul>
<p align="center">
<img alt="picture 111" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/2b93e45e768a91ba8635188fd25077e95f59a69af933e9413f071041165ab690.png" width="300">
</p>
</div>
</div>
</div>
<p>Some intuitions:</p>
<ul>
<li>Hinge loss is upper bound of zero-one (see this visually). So if we drive down hinge loss, we must also be driving down zero-one.</li>
<li>Logistic is another common loss to use here.</li>
</ul>
</section>
</section>
<section id="module-stochastic-gradient-descent-offline" class="level2">
<h2 class="anchored" data-anchor-id="module-stochastic-gradient-descent-offline">Module: Stochastic Gradient Descent (Offline)</h2>
<p><strong>Problem with gradient descent (it is too computationally expensive and slow!)</strong>:</p>
<ul>
<li>In regular gradient descent (see <a href="#def-gd">Definition&nbsp;4</a>), we update the weights after every epoch by summing all of the losses for each datum in the dataset. This is expensive since, for each iteration, just to update the weights, we must traverse the <em>entire</em> dataset. And for large problems, we should we updating the weights quite often.</li>
<li>Also, “recall that the training loss is a sum over the training data. If we have one million training examples, then each gradient computation requires going through those one million examples, and this must happen before we can make any progress” (<a href="https://stanford-cs221.github.io/autumn2022/modules/module.html#include=machine-learning%2Fstochastic-gradient-descent.js&amp;slideId=gradient-descent-is-slow-prose&amp;level=0">CS 221 Slides</a>).</li>
</ul>
<p>Thus, we introduce:</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-sgd" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8 </strong></span><strong>(Stochastic Gradient descent)</strong>:</p>
<ul>
<li>Initialize <span class="math inline">\(\mathbf{w}=[0, \ldots, 0]\)</span></li>
<li>For <span class="math inline">\(t=1, \ldots, T\)</span> : epochs
<ul>
<li>For <span class="math inline">\((x, y) \in \mathcal{D}_{\text {train }}\)</span>:
<ul>
<li><span class="math inline">\(\mathbf{w} \leftarrow \mathbf{w}-\eta \nabla_{\mathbf{w}} \operatorname{Loss}(x, y, \mathbf{w})\)</span></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<p>Differences and notes:</p>
<ul>
<li>We aren’t differentiating <span class="math inline">\(\operatorname{TrainLoss}(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}} \operatorname{Loss}(x, y, \mathbf{w})\)</span>. Just <span class="math inline">\(\operatorname{Loss}(x, y, \mathbf{w})\)</span>.</li>
<li>We update the weights on a per-sample, rather than per-epoch basis.</li>
<li>Drawback: each update isn’t as good/general since we are only looking at one sample when updating. But more efficient this way.
<ul>
<li>Thus, can become unstable. Here is happy medium:</li>
</ul></li>
<li>Mini-batch SGD: per-batch basis instead of per-sample. i.e., take gradient averages over <span class="math inline">\(B\)</span> samples per weight update.</li>
<li>Can add randomization regularization: randomize the order of the dataset for each epoch.</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>(<strong>Stochastic gradient descent in python</strong>):</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stochasticGradientDescent(f, gradientf, n, initialWeightVector):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> initialWeightVector()</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    numUpdates <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>            value <span class="op">=</span> f(w, i)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            gradient <span class="op">=</span> gradientf(w, i)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            numUpdates <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            eta <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> math.sqrt(numUpdates)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            w <span class="op">=</span> w <span class="op">-</span> eta <span class="op">*</span> gradient</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'epoch </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: w = </span><span class="sc">{</span>w<span class="sc">}</span><span class="ss">, F(w) = </span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">, gradientF = </span><span class="sc">{</span>gradient<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Credit: CS 221.</p>
</div>
</div>
</div>
<section id="step-size" class="level3">
<h3 class="anchored" data-anchor-id="step-size">Step size</h3>
<p><span class="math display">\[\mathbf{w} \leftarrow \mathbf{w}-\underbrace{\eta}_{\text {step size }} \nabla_{\mathbf{w}} \operatorname{Loss}(x, y, \mathbf{w})\]</span></p>
<p>aka learning rate.</p>
<p align="center">
<img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/b6a2ad657da29261152e030782eca625a61e7aecd810f21472904e081f19b36b.png" width="300">
</p>
<ul>
<li>How do we choose this in practice?</li>
<li>Analogy: step sizes are like driving: larger step size might mean getting there faster but can also crash and burn.
<ul>
<li>On the other hand, smaller SS will increase stability but get us there much slower. Note that if <span class="math inline">\(\eta = 0\)</span>, then <span class="math inline">\(\mathbf{w}\)</span> is actually never updated.</li>
</ul></li>
<li><strong>Adaptive strategy</strong>:
<ul>
<li>set the initial step size to <span class="math inline">\(\mathbf{1}\)</span> and let the step size decrease as the inverse of the square root of the number of updates we’ve taken so far.</li>
</ul></li>
<li>Asides:
<ul>
<li>Adam and Adagrad are more advanced and tweak this based on your data.</li>
<li>Theoretical results prove SGD convergence given that gradients are bounded with your data.</li>
</ul></li>
</ul>


</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><em>Question</em>: when we say “learning” algorithm, what are we referring to? The whole framework? Backprop?<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>is this always the case though?<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Question: I am still confused on the difference between score and margin. Is margin measures correctness, what is the difference between margin and loss then?<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>see how plotting the loss against the margin allows us to discover the following pitfalls? (motivation for introducing margin).<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>