<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Knowledge – final-notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Knowledge</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html">Stanford Notes</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../machine-learning.html">Machine Learning</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../concepts.html">Concepts</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#final-review-document" id="toc-final-review-document" class="nav-link active" data-scroll-target="#final-review-document">Final Review Document</a>
  <ul class="collapse">
  <li><a href="#i.-constraint-satisfication-problems" id="toc-i.-constraint-satisfication-problems" class="nav-link" data-scroll-target="#i.-constraint-satisfication-problems">I. Constraint Satisfication Problems</a></li>
  <li><a href="#ii.-markov-networks" id="toc-ii.-markov-networks" class="nav-link" data-scroll-target="#ii.-markov-networks">II. Markov Networks</a>
  <ul class="collapse">
  <li><a href="#gibbs-sampling" id="toc-gibbs-sampling" class="nav-link" data-scroll-target="#gibbs-sampling">Gibbs Sampling</a></li>
  </ul></li>
  <li><a href="#iii.-bayesian-networks" id="toc-iii.-bayesian-networks" class="nav-link" data-scroll-target="#iii.-bayesian-networks">III. Bayesian Networks</a>
  <ul class="collapse">
  <li><a href="#important-points" id="toc-important-points" class="nav-link" data-scroll-target="#important-points">Important points</a></li>
  </ul></li>
  <li><a href="#iv.-logic" id="toc-iv.-logic" class="nav-link" data-scroll-target="#iv.-logic">IV. Logic</a></li>
  </ul></li>
  <li><a href="#lecture-notes" id="toc-lecture-notes" class="nav-link" data-scroll-target="#lecture-notes">Lecture notes</a>
  <ul class="collapse">
  <li><a href="#probablistic-programming" id="toc-probablistic-programming" class="nav-link" data-scroll-target="#probablistic-programming">Probablistic programming</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="final-review-document" class="level1">
<h1>Final Review Document</h1>
<section id="i.-constraint-satisfication-problems" class="level2">
<h2 class="anchored" data-anchor-id="i.-constraint-satisfication-problems">I. Constraint Satisfication Problems</h2>
<ul>
<li>If we take that the factors of the factor graph are constraints that result in 0 if not satisfied, then satisfiable assignments will have a weight &gt; 0 because if at least one constraint is not satisfied, then the whole multiplication goes to 0. So the objective of maximizing the weight holds here.</li>
</ul>
</section>
<section id="ii.-markov-networks" class="level2">
<h2 class="anchored" data-anchor-id="ii.-markov-networks">II. Markov Networks</h2>
<ul>
<li><strong>Factor graphs</strong>: basically bipartie undirected graphs where there exists nodes representing RV’s and nodes representing factors which are functions acting on the RV’s they are connected to. Factors are simply functions which spit out some scalar value given assignments to the RV’s they take in. Here is an example:</li>
</ul>
<p align="center">
<img alt="picture 2" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/90608e4ad3483f413ba61a67f8d4a9bbda61144c6bfde53e1729e5b0440d6e12.png" width="500">
</p>
<p>Some useful terms for factors:</p>
<p align="center">
<img alt="picture 3" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/62ea20613bc37dd5b53d921633646909e895be970457db2df2279208700f4be4.png" width="300">
</p>
<p>each assignment of values to the RV’s has a weight associated to it which involves multiplying the factors together:</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Assignment weights) </strong></span>Each assignment <span class="math inline">\(x=\left(x_1, \ldots, x_n\right)\)</span> has a weight: <span class="math display">\[
\operatorname{Weight}(x)=\prod_{j=1}^m f_j(x)
\]</span></p>
</div>
</div>
</div>
<section id="transitioning-to-markov-networks" class="level4">
<h4 class="anchored" data-anchor-id="transitioning-to-markov-networks">Transitioning to markov networks</h4>
<ul>
<li>Markov networks are just factor graph’s but with probability defined to each assignment of the variables rather than weight. Before, we’d have a table that looks like this which specifies a weight for each assignment:</li>
</ul>
<p align="center">
<img alt="picture 4" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/c1160710786b8c3db6c7308f43162a47337088b618bd994d37d0cbd462d00777.png" width="300">
</p>
<ul>
<li>Now, we can normalize it by dividing each weight by the sum of all weights:</li>
</ul>
<p><span class="math display">\[\mathbb{P}(X=x)=\frac{\text { Weight }(x)}{Z}\]</span></p>
<p>where <span class="math inline">\(Z=\sum_{x^{\prime}}\)</span> Weight <span class="math inline">\(\left(x^{\prime}\right)\)</span>. We now have:</p>
<p align="center">
<img alt="picture 5" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/ea0ff0d2541f63a7ae511a1425e77d18c83c32b8d9ac8a79200aa82cb013c291.png" width="300">
</p>
<p>this allows us to represent uncertainty. We can perform inference via taking marginals for example.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \mathbb{P}\left(X_2=1\right)=0.15+0.15+0.15+0.15=0.62 \\
&amp; \mathbb{P}\left(X_2=2\right)=0.08+0.31=0.38
\end{aligned}
\]</span></p>
<p>Note: different than max weight assignment!</p>
</section>
<section id="gibbs-sampling" class="level3">
<h3 class="anchored" data-anchor-id="gibbs-sampling">Gibbs Sampling</h3>
<ul>
<li>We are interested in computing marginal, conditional, and regular probabilities from the distribution that the markov network represents. Here, we discuss an inference algorithm to <em>approximate</em> the marginal calculations (e.g., what is the probability of some random variable <span class="math inline">\(X_2\)</span>?).</li>
<li><strong>Marginal probabilities are computed by summing the probabilities over all assignments satisfying the given condition</strong>. Gibbs sampling gives us a computationally efficient approximate.
<ul>
<li>Note: only given conditional probabilities rather than marginal in the graphs so we must approximate it.</li>
<li>Cond probs are represented by the factors which are often derived when converting bayesian to markov:</li>
</ul>
<p align="center">
<img alt="picture 6" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/9ab65e611e5dbb1e6f5f8afba4d75178ec382aa5a7af1785d90e1d51ced78f44.png" width="300">
</p></li>
</ul>
<p><em>Describe algorithm steps here</em>.</p>
</section>
</section>
<section id="iii.-bayesian-networks" class="level2">
<h2 class="anchored" data-anchor-id="iii.-bayesian-networks">III. Bayesian Networks</h2>
<ul>
<li>Form of modeling the world. A BN is simply a directed acyclic graph of random variables where directed edges between nodes represents conditional dependence and causation.</li>
<li>Joints are too big in the real world. Can’t:
<ul>
<li>Efficiently store all the combinations. We’d have <span class="math inline">\(2^k\)</span> parameters where <span class="math inline">\(k\)</span> is the number of RV’s.</li>
<li>Perform useful inference due to the global nature of the joint.</li>
</ul></li>
<li>Bayes nets allow us to compactly represent the joint via local conditional probability distributions for each RV. And it resolves the second problem by reasoning locally but conditioning globally. This relies on two concepts:
<ul>
<li>conditional independence</li>
<li>chain rule</li>
</ul></li>
</ul>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bayes net &lt;–&gt; Joint
</div>
</div>
<div class="callout-body-container callout-body">
<p>The full joint would be to multiply the RV’s out:</p>
<p><span class="math display">\[
P\left(x_1, x_2, \ldots x_n\right)=\prod_{i=1}^n P\left(x_i \mid x_1 \ldots x_{i-1}\right)
\]</span></p>
<p>this is true for <em>any</em> distribution (this is just the chain rule). However, in bayes nets, we assume conditional independence meaning each node is conditionally independent from every node buts its parents. So we reduce the above to:</p>
<p><span class="math display">\[
P\left(x_i \mid x_1, \ldots x_{i-1}\right)=P\left(x_i \mid \operatorname{parents}\left(X_i\right)\right)
\]</span></p>
<p>which still gives us a valid joint:</p>
<p><span class="math display">\[
P\left(x_1, x_2, \ldots x_n\right)=\prod_{i=1}^n P\left(x_i \mid\right. \text{parents}\left.\left(X_i\right)\right)
\]</span></p>
<p>See how much space we save by making the conditional independence assumption?</p>
</div>
</div>
<section id="conditional-independence" class="level4">
<h4 class="anchored" data-anchor-id="conditional-independence">Conditional Independence</h4>
<ul>
<li>Main characteristic that allows us to formulate joints via Bayes’ nets. Essentially, conditional independence allows us to condition only on the local parents for each node rather than the ancestor variables.</li>
</ul>
<p>Example:</p>
<p align="center">
<img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/4f6aa56ddd11ad81b38ed4c9fda2a56b59619fa8117396ddddfc272fadb2aeb1.png" width="300">
</p>
<p><span class="math inline">\(x_3\)</span> is independent of <span class="math inline">\(x_2\)</span> given <span class="math inline">\(x_1\)</span>.</p>
</section>
<section id="important-points" class="level3">
<h3 class="anchored" data-anchor-id="important-points">Important points</h3>
<ul>
<li>You can <strong>recover</strong> the full joint from a Bayes net via chain rule:</li>
</ul>
<p><span class="math display">\[
P\left(x_1, x_2, \ldots x_n\right)=\prod_{i=1}^n P\left(x_i \mid\right. \text{parents} \left.\left(X_i\right)\right)
\]</span></p>
<ul>
<li><strong>Inference</strong> in a Bayesian network involves computing the probability of a variable, or a set of variables, given some evidence. This involves combining the probabilities of the variables using the chain rule of probability and the conditional probabilities represented in the network.</li>
</ul>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Markov networks
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Factors in a Markov network may include variables beyond the cliques scope.</li>
<li>Markov assumption is that the rest of the graph not in the clique is conditionally independent given an observation. As such, the variable only depends on the other variables in its clique.
<ul>
<li><a href="https://www.youtube.com/watch?v=nBrFC0STApo">clique</a> means subgraph where each vertice is adjacent. That is, <strong>every markov inference is conditioned on its neighbors only</strong>.
<ul>
<li>In other words:
<ul>
<li>Any two non-adjacent variables are conditionally independent given all other variables… aka Any variable is conditionally independent of the other variables given its neighbors.</li>
</ul></li>
</ul></li>
</ul></li>
<li>The probability is <span class="math inline">\(p\left(x_1, \ldots, x_n\right)=\frac{1}{Z} \prod_{c \in C} \phi_c\left(x_c\right)\)</span> which means it sums <span class="math inline">\(\sum_{x_1, \ldots, x_n}\)</span> over all values in the given inference. For example, you have to set of assignments to the variables <span class="math inline">\(x_1, \ldots, x_n\)</span> and so you calculate the probability that this prediction is correct based on the given formula. But you sum the factors for normalization purposes.
<ul>
<li>Here, the probabilities are the factors.</li>
</ul></li>
<li>Can learn the factors (params) via MLE.</li>
<li><strong>Bayesian to Markov</strong>:
<ul>
<li>Draw edges between all parents for each given node (go one by one).</li>
<li>Remove directionality</li>
</ul></li>
<li>Factor graphs:
<ul>
<li>bipartite graph. One type of vertex is factor and another RV. Edges connect the factors to the variables such that the edges connect the variables that the factor depends on. Example:</li>
</ul></li>
</ul>
<p align="center">
<img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/c3266bd57d365ce4cba1bc7600071804fc9f9db37164b16d70742daf4a4c009e.png">
</p>
<ul>
<li><p><strong>Inference</strong>:</p>
<ul>
<li>Two types:
<ul>
<li>Marginalization: what is the probability of a given variable in our model after we sum everything else out (e.g., probability of spam vs.&nbsp;non-spam)?: <span class="math inline">\(p(y=1)=\sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} p\left(y=1, x_1, x_2, \ldots, x_n\right)\)</span>.</li>
<li>Maximum a posteriori (MAP) inference: what is the most likely assignment to the variables in the model (possibly conditioned on evidence)?: <span class="math inline">\(\max _{x_1, \ldots, x_n} p\left(y=1, x_1, \ldots, x_n\right)\)</span>.</li>
<li>Sometimes computationally intractable.</li>
</ul></li>
</ul></li>
<li><p><strong>Sampling-based inference</strong>:</p>
<ul>
<li>Exact inference is often too slow for large distributions. So we need to approximate using sampling methods.</li>
<li>variational: inference by solving some optimization problem.</li>
<li>Sampling: produce answers by repeatedly generating random numbers from a distribution of interest.</li>
<li>Sample meaning: for example, divide the space into areas with sizes representing the corresponding probabilities. Throw a dart randomly at it:</li>
</ul>
<p align="center">
</p><p><img alt="picture 2" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/8a14d5110a91c51d53caeedb5b77c76ace056d6af551efccfabc3245c2b9265a.png" width="300"></p>
<p></p>
<ul>
<li>Sampling from a Bayes net:
<ul>
<li>Sample each variable, conditioned on its parents, which will be observed if you go…, in topological order. Linear time. Topological order means start with node of no parents (i.e., conditioned on nothing), then sample on its children which will be sampled on itself the parent, then work your way down. See <a href="https://ermongroup.github.io/cs228-notes/inference/sampling/">forward sampling</a> section here.</li>
</ul></li>
<li>Gibbs sampling: say we want to compute assignments to the variables <span class="math inline">\(x_1, \ldots, x_n\)</span>. We are given a markov network with factors representing transition probabilities. E.g.:
<p align="center">
<img alt="picture 4" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/102ef4a5dc7fa82fe1c6a57bcea27b2dd423247f3e1c2e42ab9366eccf9d3fc6.png" width="300">
</p>
<ul>
<li>Set the variables to some random initial state for <span class="math inline">\(t=0\)</span>: <span class="math inline">\(x^0=\left(x_1^0, \ldots, x_n^0\right)\)</span>.</li>
<li>For <span class="math inline">\(t\)</span> time steps, we want to update these assignments. Repeat the following <span class="math inline">\(t\)</span> times:
<ul>
<li>Let <span class="math inline">\(x \leftarrow x^{t-1}\)</span> where <span class="math inline">\(x\)</span> represents <span class="math inline">\(x^t\)</span>, the assignment of variables that we are currently updating.</li>
<li>Loop through all variables:
<ul>
<li>Sample <span class="math inline">\(x_i^{\prime} \sim p\left(x_i \mid x_{-i}\right)\)</span> where <span class="math inline">\(x_{-i}\)</span> means all variables except <span class="math inline">\(x_i\)</span>.
<ul>
<li>IMPORTANT: due to the markov assumption, <span class="math inline">\(x_{-i}\)</span> reduces to <span class="math inline">\(x_i\)</span>’s neighbors (its clique)! So we only really condition on its neighbors rather than everything else.</li>
</ul></li>
<li>Update the global vector <span class="math inline">\(x \leftarrow\left(x_1, \ldots, x_i^{\prime}, \ldots, x_n\right)\)</span>.</li>
</ul></li>
<li>Set <span class="math inline">\(x^t \leftarrow x\)</span>.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p>Some notes:</p>
<ul>
<li>Sampling is only conditioned on neighbor nodes.</li>
<li>Note that when we update <span class="math inline">\(x_i\)</span>, we immediately use its new value for sampling other variables <span class="math inline">\(x_j\)</span>.</li>
<li>Often times, you are not given a conditional probability distribution exactly (cuz otherwise no need for sampling just use the joint) for each variable. Instead, you’ll be given factors. So to get probabilities, loop over all possible combinations of <span class="math inline">\(x_i\)</span> and assign each a weight given the factors (put assignment into factor). Then sample from that probability distribution:</li>
</ul>
<p align="center">
<img alt="picture 5" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/73874e05e50de53a63f2e5a32351455739d9f330e077079462477307bc1d68f3.png" width="300">
</p>
<ul>
<li>Only use factors in clique?</li>
</ul>
<p>This picture sums it up perfectly:</p>
<p align="center">
</p><p><img alt="picture 3" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/93967af70e30d6bee434467027b26bb7280d21d2ff7fc555d589c3a461354fc6.png" width="70%"></p>
<p></p>
<p>Note… the algo presented in class in slightly different and just counts the number of times the variable <span class="math inline">\(x_i\)</span> was set to some value and then normalizes:</p>
<p align="center">
</p><p><img alt="picture 6" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/f7fa2e9013efdfdffa35f1ae8bc38f9ce82dfc07ee401354cc9058cde8636e7b.png" width="300"></p>
<p></p>
<p>Note: it seems that for marginal we only compute the weights and probabilities for the clique and factors that touch the RV <span class="math inline">\(x_i\)</span>:</p>
<p align="center">
<img alt="picture 7" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/914925766e044136b93ec350c046445b00ae4896be81f1f3470c0f440783bf04.png" width="300">
</p>
<ul>
<li>Another note: you may think that when we loop through all possible values for <span class="math inline">\(x_i\)</span> to perform the inputs into the factors, we’d have to loop through all the other inputs variables in the factors’ scopes. This is not true because we are operating under the assumption that the neighboring variables <em>have</em> been observed already and thus have a concrete value (e.g., image denoising). Example:</li>
</ul>
<p align="center">
<img alt="picture 8" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/05e4083d88bae48d68eb67131fac722e1830407962db08f8ddc0f19641d28533.png" width="300">
</p>
<ul>
<li><strong>Parameter learning</strong>: where the graph structure is known and we want to estimate the factors (i.e., the probabilities) from data sets.</li>
</ul>
</div>
</div>
<section id="questions" class="level4">
<h4 class="anchored" data-anchor-id="questions">Questions</h4>
<ul>
<li>Why do we assume conditional independence in Bayesian networks? That is, why do we assume that the value of each node depends only on its direct parents rather than the rest of the network and/or its ancestors?
<ul>
<li>I think this is just the assumption we make when modeling the world using Bayes nets.</li>
<li>We assume the “markov property”.</li>
</ul></li>
<li></li>
</ul>
</section>
</section>
</section>
<section id="iv.-logic" class="level2">
<h2 class="anchored" data-anchor-id="iv.-logic">IV. Logic</h2>
<hr>
</section>
</section>
<section id="lecture-notes" class="level1">
<h1>Lecture notes</h1>
<section id="probablistic-programming" class="level2">
<h2 class="anchored" data-anchor-id="probablistic-programming">Probablistic programming</h2>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>