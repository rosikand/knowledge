{"title":"Week 1, Lecture 1 - Course Introduction","markdown":{"yaml":{"categories":["lecture"],"format":{"html":"default","pdf":"default","docx":"default"}},"headingText":"Week 1, Lecture 1 - Course Introduction","containsRefs":false,"markdown":"\n\n\n<!--  \n::: {#exm-default}\n\n## Beta \n\nHere $43$. \n:::\n\n \n:::{.callout-note appearance=\"simple\"}\nHello\n:::\n\n \n::: {#def-default}\n# s\nwow.\n:::\n\n\n \n:::{.callout-note collapse='true'}\nEven cooler bro. \n:::\n \n \n \n  -->\n\nNote: MEL = meta-learning. \n\n## Topics \n\n\n- Modern DL methods for learning across tasks \n- Implementing these methods (MT, TL) in PyTorch \n- Glimpse of building new algorithms \n\nlow-level descriptions: \n\n- MT, TL\n- Meta learning algos \n- Advanced meta learning topics\n- Unsupervised pre-training\n    - FS learning\n- Domain adaption \n- Lifelong learning \n- Open problems\n\nFocus on DL, with case studies in things like NLP. \n- No RL! (see CS 224R) \n\n## 1. Logistics \n- Lectures are live-streamed and recorded \n- two guest lectures\n- Prereqs:\n    - Sufficient background in ML (229) \n\n### Homeworks \n50% of grade. \n\n- 0: multi-task basics\n- 1: multi-task data processing and BB-ML\n- 2: gradient-based ML \n- 3: fine-tuning pre-trained language models \n- 4 (optional): Bayesian ML and meta overfitting \n    - Replace 15% of hw/project \n    - Not coding, all math \n- 6 late days \n\n### Project\n- Poster session, 50% of grade. \n- Idea: ... \n\nNow technical content... \n\n## 2. Why study multi-task learning and meta-learning?\n- How can we enable agents to learn a breadth of skills in the real world? \n    - Because each time we have to train a supervised signal \n        - So the goal is to learn representations across tasks \n- Aside (common paradigm to learn representations): initialize well (not randomly) --> fine-tune on new task. \n    - This is harder for RL than NLP because NLP has the entire wikipedia to use but robotic common sense representations are not as straightforward (maybe we need a common robot embedding?)\n\n**Evolution**:\n\n- Early in CV: hand-design features, train SVM on-top \n- Modern CV: end-to-end training, no hand-engineering\n    - Allows us to handle unstructured inputs without understanding it\n- Now why meta-learning? Three reasons... \n    - **Don't have large dataset** at the outset to pre-train on or use in end-to-end SL manner (med imaging, robotics, etc.)\n        - Even more so: **long-tail data** samples (e.g., self-driving won't catch all edge cases)\n            - MEL techniques can help with this (kinda... not the main focus tho)\n    - **Quickly learn something new** (few-shot learning)\n- Lots of open problems \n\n## Multi-task intro \n\n- What is a task?\n    - Dataset + loss objective --> model \n    - Objects as \"tasks\" \n    - Critical assumption: different tasks need to share some base structure (goal is to exploit shared structure)\n        - But lots of tasks share structure (even as upstream as sharing the laws of physics!)\n        - Question: can we learn a shared embedding space for e.g., text + images in one? \n- Does MT learning reduce to single-task SL learning?\n    - Somewhat (tho not for every problem)\n    - Idea: sum loss and data: \n    \n$$\n\\mathcal{D}=\\bigcup \\mathcal{D}_i \\quad \\mathcal{L}=\\sum \\mathcal{L}_i\n$$\n\n\n\nNext up: a technical dive into the **multi-task** learning framework. \n\n<p align=\"center\">\n<img alt=\"picture 1\" src=\"https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/c49969eb11cb825d222502ce7dba1133f0ab193896de5d7a97599632a20c990c.png\" width=\"300\" />  \n</p>\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"1.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":"cosmo","categories":["lecture"]},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"1.pdf"},"language":{},"metadata":{"block-headings":true,"categories":["lecture"]},"extensions":{"book":{}}},"docx":{"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"docx","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"page-width":6.5},"pandoc":{"default-image-extension":"png","to":"docx","output-file":"1.docx"},"language":{},"metadata":{"categories":["lecture"]},"extensions":{"book":{}}}}}