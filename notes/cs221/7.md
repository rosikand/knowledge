# Week 4, Lecture 1 - Markov Decision Processes I 

## I. Overview

- Search: model a problem from the world as a graph and find the minimum cost path to solve the problem.
- Search was *deterministic*.  

<p align='center'>
    <img alt="picture 2" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/8a4a700805b58d6a3d07f8cce78725e857bdcf9292f125a0ef19faa9f7063f52.png" width="300" />  
</p>

- That is, if we take $a$ from $s$, we will always end up in whatever is specified by $\operatorname{Succ}(s, a)$.
- However, the real world is filled with *uncertainty*. Take for example, robotic movement. When put in a new environment, there is new uncertainty that we have to tackle.

<p align='center'>
    <img alt="picture 3" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/bd10cc64fecc6fa91debc70dfbf7610eccc2c373750eea61522d6431dee4191f.png" width="300" />  
</p>

- How to handle such uncertainty? Search in it of itself is not enough. But it provides the basis for MDP‚Äôs which allow for uncertainty in $\operatorname{Succ}(s, a)$.
- MDPs: Mathematical Model for decision making under uncertainty.
- Roadmap:

<img alt="picture 4" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/2d9c303108479522a970d574bb21ea4decb53a2bc020cb02d33d0292cff29750.png" width="300" />  

## II. Modeling

- Chance nodes: state with action brings us to chance nodes which draws probabilities to the next potential state.
    - Over state and action
- Transition probability: state, action, new state tuple which says, ‚Äúgiven a specific state, and an action from that state, what is the probability that we end up in s‚Äô?‚Äù. Held in a matrix.
- In addition, we also define a reward with the same tuple inputs.
- Goal is to maximize reward (same as minimizing cost just reverse)

<p align='center'>
    <img alt="picture 5" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/e8c17cce37a925be063a6393da46581837f5d2cfabc27e3fc5d2302c9550a1c4.png" width="500" />  
</p>


More on the transition component:

<p align='center'>
    <img alt="picture 6" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/89c00e67f9af013bd999b16ad1880621eec37ef018a5235c2b59ef691b88cae8.png" width="500" />  
</p>

- This makes sense to think of it as a *distribution*. at each state, given an action from that state, we can end up in different states with different **probabilities**. 
- Instead of having some $Succ(s,a)$ function that *deterministically* picks a state given the $(s, a)$ pair, we are instead given a distribution of next possible states with associated probabilities. This distribution of course, must sum to 1.
- What changes from a search problem:

<p align='center'>
    <img alt="picture 7" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/c09159be93ad8c9beb0a8e4f1d0e450cd37608236fc408d836ca1cf516af5e93.png" width="300" />  
</p>

- Example:

<p align='center'>
    <img alt="picture 8" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/3c60622226f50d27f4b3bf45a1c0e5ad440e92eea801e2c3b37dd0bbbd28e6c1.png" width="300" />  
</p>


### In code:

- Change successors and costs function to `succProbReward`.
    - return list of (newState, prob, reward) triples given (state, action) as input.

```python
class TransportationMDP(object):
    walkCost = 1
    tramCost = 1
    failProb = 0.5

    def __init__(self, N):
        self.N = N

    def startState(self):
        return 1

    def isEnd(self, state):
        return state == self.N

    def actions(self, state):
        results = []
        if state + 1 <= self.N:
            results.append('walk')
        if 2 * state <= self.N:
            results.append('tram')
        return results

    def succProbReward(self, state, action):
        # Return a list of (newState, prob, reward) triples, where:
        # - newState: s' (where I might end up)
        # - prob: T(s, a, s')
        # - reward: Reward(s, a, s')
        results = []
        if action == 'walk':
            results.append((state + 1, 1, -self.walkCost))
        elif action == 'tram':
            results.append((state, self.failProb, -self.tramCost))
            results.append((2 * state, 1 - self.failProb, -self.tramCost))
        return results

    def discount(self):
        return 1.0

    def states(self):
        return list(range(1, self.N + 1))
```

### Policy (the solution)

ok cool‚Ä¶ what is a solution? In search, we just return the minimum cost path. But here, we must diverge‚Ä¶ cuz it isn‚Äôt deterministic. Thus, a solution is in the form of a *policy*.  

<p align='center'>
    <img alt="picture 9" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/7f74236800c356e347830a09e9a8047551ff5039bfd08c9f2493dbd1f3673453.png" width="300" />  
</p>

- IT IS A FUNCTION!
    - Takes input, returns output.
    - Takes in each state to an action
    - Lets take this deeper:
        - It may look like this is a path: from each state, we choose the next one. but the key is that they aren‚Äôt connected. the states aren‚Äôt connected.
        - It just takes in a state, and chooses the best action given that state. It doesn‚Äôt draw us to a next state. And this makes sense because we can‚Äôt reach the next state deterministically‚Ä¶ it happens probabilistically.
        
<p align='center'>
    <img alt="picture 10" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/611e3d32d2c34aab3d8e182d8970486d4f72610953e240a5398a14a8c772e464.png" />  
</p>


<p align='center'>
    <img alt="picture 11" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/8074aeada88a552791ca52b7628f87c4b3c0232b1e94db72cb599a8002f4dd8e.png" width="300" />  
</p>

<aside>
üí° **Summary**: MDP‚Äôs add probability to search problems. Imagine nodes in a graph where we start at one state and want to get to some end state. We can take actions at each state but we don‚Äôt know which state this will make us end up in. A transition distribution then comes in which specifies, given a state and a corresponding action, returns the corresponding potential new states and the probability that we end up at such a state. To solve an MDP, we can‚Äôt specify a path cuz it isn‚Äôt deterministic. As such, we specify a policy. A policy is a per-state mapping, which, from each state tells us what action to take (s‚Äô independent).

</aside>

Ok cool‚Ä¶. we got this gist‚Ä¶ now we will talk about how to evaluate policies (are they actually good or not?) 

## III. Policy Evaluation

- Since we are dealing with non-deterministic scenarios, a policy gives us an inherently ******random****** path. We can never be for certain what path a policy may give us. So to discern whether a policy is good or not, we must built up a framework of knowledge, rooted in probability.
- The goal of an MDP is to maximize the total rewards. To variabalize this, we define this notion of utility:

<p align='center'>
    <img alt="picture 12" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/d296f71484a1cfc2d3d9b7b6dd6cd2134576556bfe6dbaa4cafa7961b133f05d.png" width="300" />  
</p>

- Inherently, a utility is not deterministic. Thus, we deem it as a random variable and specify a distribution over it (i.e., probability of getting ****this**** reward given the policy).
- So our goal is to maximize the utility. But we can‚Äôt maximize a random variable. Thus, we define:

<p align='center'>
    <img alt="picture 13" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/399ca8d771269ba9cab48f4ba00dd5dc31611982047923e1a3c994bbd0977ff3.png" width="300" />  
</p>

- So really, we wan‚Äôt to maximize the ****************expected utility**************** (called the **********value**********).
- So for each policy, we now have a value (expected value for the utility RV) and utility (RV for a policy).
- Example of a utility result:

<p align='center'>
    <img alt="picture 14" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/20cf1d063dd0240796d0ef238807f7ba2a9f1562d70b574a40bdf85474be5c44.png" width="300" />  
</p>

- In sum:

> Recall that we'd like to maximize the total rewards (utility), but this is a random variable, so we can't quite do that. Instead, we will instead maximize the¬†**expected utility**, which we will refer to as¬†**value**¬†(of a policy).
> 
- Interestingly, the problem of computing the value of a policy is a complex one. Here, we will discover algorithms for it.

Formal definition: 

<p align='center'>
    <img alt="picture 15" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/018278e57130b9660b8ba0f5c570fb57c2baa433b8cba7d7afd257440798db65.png" width="300" />  
</p>

### Discount

What the hell is $\gamma$? 

- To ungreedy-ize this, we introduce discount.
- which captures the fact that a reward today might be worth more than the same reward tomorrow.
    - is discount is small, than favor present more than future. cuz it is between 0 and 1.
- ‚Ä¢ Note that the discounting parameter is applied exponentially to future rewards, so the distant future is always going to have a fairly small contribution to the utility (unless¬†).
- ‚Ä¢ The terminology, though standard, is slightly confusing: a larger value of the discount parameter¬†¬†actually means that the future is discounted less.

### Back to policy evaluation

- We are given a policy, how do we figure out the *****value***** of the policy?

<p align='center'>
    <img alt="picture 16" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/ab606cdff296b96637b758c84a6214eaeacaa467561cad05b52e9124e358097b.png" width="300" />  
</p>    
    
- Function above, given policy and following it from state s, what is the expected utility (value)?
- Let‚Äôs bring this up a notch:

<p align='center'>
    <img alt="picture 17" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/dcb2dc05fa1756201f700847da6fc771f65efc7c4b5bfe77ef6e6d5713602009.png" width="300" />  
</p>    
    
- So we see that v_pi(s) gives us value from state s itself.

<p align='center'>
    <img alt="picture 18" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/955f38885d5010341ffa8cc1eee014b43842a0c0738f298898f23def189cc17e.png" width="300" />  
</p>

- But what about after state s? state s, given a, will land us in some chance node that points us to different places. but what about chance node to next and so on? Q-value defines this. It is sequential whereas v_pi is one to one.
- It might be hard to understand the difference between $V_\pi(S)$ and $Q_\pi(s, a)$. Key differences:
    - $Q_\pi(s, a)$ takes in both a state and action and plays out the policy. v_pi just takes in state.
        - So as such, q plays out the policy, given that we are taking this next action.
            - See how the math will depend on Q?
- THE BIG RECURRENCE THAT EXPLAINS IT ALL:

<p align='center'>
    <img alt="picture 19" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/781d67f63e833ae6e047b2a6f8a120f9ea85f5a088275acbd61f1ad95f65ee86.png" width="500" />  
</p>

- That is it folks‚Ä¶ policy evaluation
- There you have it folks: plug and jug into the equation.
- End node is base case with $V_\pi( end )=0$.
- See dice game example for solving the closed form of v_pi(in).

### Policy evaluation algorithm through iteration 

- v_pi is a policy evaluation function: that is, create a mapping (a discrete function) that maps each state to its expected utility (value).
    - Can also do this iteratively with a loop:
        - ‚Ä¢ Policy iteration starts with a vector (python list) of all zeros for the initial values¬†. Each iteration, we loop over all the states and apply the two recurrences that we had before.
- Run until convergence which we define as:
- $\max {s \in \text { States }}\left|V\pi^{(t)}(s)-V_\pi^{(t-1)}(s)\right| \leq \epsilon$.

- How to know when to stop? Here is a heuristic:
	- $$\max _{s \in \text { States }}\left|V_\pi^{(t)}(s)-V_\pi^{(t-1)}(s)\right| \leq \epsilon$$
- Note that we only need to keep $V_\pi^{(t)}$ and $V_\pi^{(t-1)}$ in the array then because that is all we need to know when we converge. 

### Summary thus far: 
- **MDP**: graph with states, chance nodes, transition probabilities, rewards
- **Policy**: mapping from state to action (solution to MDP)
- **Value of policy**: expected utility over random paths
- **Policy evaluation**: iterative algorithm to compute value of policy

next... how to find that optimal policy $\pi$ via **value iteration**. 

## IV. Value Iteration 

- Given a policy, we know how to know its value $V_\pi\left(s_{\text {start }}\right)$. But there exists an exponential amount of possible policies ($A^S$). So how do we find the optimal one?
- Here, we introduce value iteration, an algorithm for finding the optimal policy. While it may seem that this will be very different from policy evaluation, it is quite similar! 

 
::: {#def-default}
**Optimal value**: The optimal value $V_{\text {opt }}(s)$ is the maximum value attained by any policy.
:::

The above definition should be self explanatory: the optimal policy is the one with the highest value. 

But it will help us set up the solution. Instead of having a fixed policy for the recurrence like in PE, we will now define a recurrence on top of $V_{\text {opt }}$ and $Q_{\text {opt }}$. 


 
:::{.callout-note}
**Value iteration recurrence:**

Optimal value from state $s$:

$$V_{\text {opt }}(s)= \begin{cases}0 & \text { if IsEnd }(s) \\ \max _{a \in \operatorname{Actions}(s)} Q_{\mathrm{opt}}(s, a) & \text { otherwise. }\end{cases},$$

where the optimal value if take action $a$ in state $s$: 

$$Q_{\mathrm{opt}}(s, a)=\sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[\operatorname{Reward}\left(s, a, s^{\prime}\right)+\gamma V_{\mathrm{opt}}\left(s^{\prime}\right)\right]$$. 

Visually: 

<p align='center'>
    <img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/db3ebfa3c6c97a640871341ac063273b6225d068f967e2c9fd60f6a70cdde6ab.png" width="300" />  
</p>
:::

and so then to find the optimal policy, we'd just take $Q_{opt}$ and do 

$$\pi_{\mathrm{opt}}(s)=\arg \max _{a \in \operatorname{Actions}(s)} Q_{\mathrm{opt}}(s, a).$$ 


 
:::{.callout-note}
::: {#def-default}
**(Value iteration algorithm)** 

1. Initialize $V_{\text {opt }}^{(0)}(s) \leftarrow 0$ for all states $s$. 
2. For iteration $t=1, \ldots, t_{\mathrm{VI}}$ :
   1. For each state $s$ :

$$
V_{\text {opt }}^{(t)}(s) \leftarrow \max _{a \in \text { Actions }(s)} \underbrace{\sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[\operatorname{Reward}\left(s, a, s^{\prime}\right)+\gamma V_{\text {opt }}^{(t-1)}\left(s^{\prime}\right)\right]}_{Q_{\text {oot }}^{(t-1)}(s, a)}
$$
:::
:::
 

- The keen eye will note that this just returns the optimal value and not actually the optimal policy. However, optimal policy comes as a byproduct. Just **take the argmax at each state**. 
- 


 
 
