---
toc: false
---

# Week 3, Lecture 1 - Search I

`Date: 10/10/22` 

## I. Overview 

We now move into the next unit of the course: **Search**. This is a reflex-based paradigm. We will still abide by the principles of the "model, learning, and inference" paradigm. To introduce search, take the following canonical problem: 

 
::: {#exm-default}
(**Route planning**)
Given as the input a map, a source point and a destination point, output a sequence of actions (e.g., go straight, turn left, or turn right) that will take us from the source to the destination. Evaluate based on custom objective (e.g., most scenic route, shortest path, etc.)
:::


 - There are many more examples that we can think of. Robot motion planning,solving puzzles, etc. Many traditional (non-ML) algorithmic paradigms (e.g., dynamic programming) make its way into AI through this medium. The main idea though, is that:
   - Search problems are defined by their **objective** and the **actions** it can take to get there. 

#### Beyond reflex 

<p align='center'>
    <img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/d927b01d4eb23e8f998111ea391c7df53b93c78cac3067fb65829aabaa6b0ed9.png" width="500" />  
</p>

- Reflex models are very high-level: e.g., classifiers which output a quick "yes" or "no". Not dependent on any future/past predictions. 
- However, some applications of AI (such as solving puzzles), demand more. 
- **Search problems** are an instance of **state-based models**. 
- We are still building a predictor $f$ which takes an input $x$, but will now return an entire **action sequence**, not just a single action (i.e., a "reflex"). 
- You can think of the difference as defined by the task. Solving a puzzle is not the same as classifying cats vs. monkeys. 
- Can't you solve a search problem just by iteratively multiplying the reflex-based predictions to get an action sequence? 
  - Not really... future actions should be conditioned on past actions! 
  - But... think of this: "if you're walking around Stanford for the first time, you might have to really plan things out, but eventually it kind of becomes reflex.". 
- We have looked at many real-world examples of this new "search" paradigm. For each example, **the key is to decompose the output solution into a sequence of primitive actions**. In addition, we need to think about **how to evaluate different possible outputs**. 

#### Roadmap 

- In the ML unit, we had the **"model, learning, and inference" paradigm**. We adapt this for search too. 
  - **Machine learning**:
    - **Modeling**: feature extractor + neural architecture 
    - **Inference**: model forward pass ($f(x)$)
    - **Learning**: algorithms such as stochastic gradient descent to find optimal model weights
  - **Search**: find out down below! We will start with modeling and inference, then learning next lecture. 

Here is a roadmap of what we will cover: 

<p align='center'>
    <img alt="picture 2" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/e0cf1e0851f3e28aeb23c7c6c58d0daa93a1f9f989f77e63f0c108bcd29692f9.png" width="500" />  
</p>

> The main idea is to cover how we define ("model") search problems, and come up with **algorithms to solve them**.  


 
:::{.callout-note}
(**State-based vs reflex-models**) The main TL;DR is to understand the following difference and what problems we can now solve with this new tool: 

**Reflex-based modeling**:

$$
x \rightarrow f \rightarrow \text { single action } y \in\{-1,+1\}
$$


**(Search) State-based modeling**:

$$
x \rightarrow f \rightarrow \text { action sequence }\left(a_1, a_2, a_3, a_4, \ldots\right)
$$

This allows us to not only solve things like classification problems with reflex-based modeling, but also now solve things like route planning with state-based modeling. 
:::
 


 
## II. Modeling 

Ok... cool! We now have a general intuition for what is search/state-based modeling. Let's get formal now with definitions. 

We will introduce modeling via the following example. 

 
:::{.callout-note appearance='simple'}
 
::: {#exm-default}
(**Farmer search problems**) A farmer wants to get his cabbage, goat, and wolf across a river. He has a boat that only holds two. He cannot leave the cabbage and goat alone or the goat and wolf alone. How many river crossings does he need?
:::
:::
 


- You probably thought of each scenario playing out in your head. How can we build a system to do this automatically. 
- Search **problems define the possibilities**, and search **algorithms explore these possibilities**.
- For this problem, we have eight possible actions, which will be denoted by a concise set of symbols.

<p align='center'>
    <img alt="picture 4" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/a8fcf66fd4dbf46005eecfc64fbbc6728de1852feb7638e41c3e5c6b7a1e7ac8.png" width="300" />  
</p>

- We can start to build a **search tree** of each "what-if" scenario. Playing out each scenario, we get the following: 

<p align='center'>
    <img alt="picture 5" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/e3c33ead58c5b6dab9fe89e299c9c05dd44700bd9b7e21af600ac2bb3584f259.png" width="500" />  
</p>

- **Each scenario is represented by a "branch"** of the search tree.
- We terminate the branch (leaf) once we reached a final state (win or lose). 

Let's formalize this. 

### Defining the search problem 

 
:::{.callout-note}
::: {#def-default}
**(Search Problem)** A search problem is built upon the following: 

- $s_{\text {start }}$ : starting state
- Actions $(s)$ : possible actions
- $\operatorname{Cost}(s, a)$ : action cost
- $\operatorname{Succ}(s, a)$ : successor
- IsEnd $(s)$ : reached end state?
:::
:::
 
This can be visually represented/solved with a search tree. The root of the tree is the **start state** $s_{\text {start }}$, and the leaves are the **end states** (IsEnd $(s)$ is true). Each edge leaving a node $s$ corresponds to a possible action $a \in \operatorname{Actions}(s)$ that could be performed in state $s$. The edge is labeled with the action and its cost, written $a: \operatorname{Cost}(s, a)$. The action leads deterministically to the successor state $\operatorname{Succ}(s, a)$, represented by the child node.


In the example, each root-to-leaf path represents a possible action sequence, and the sum of the costs of the edges is the cost of that path. The goal is to find the root-to-leaf path that ends in a valid end state with minimum cost. 

## III. Tree search 

Cool... we got the formalism and modeling down. Now let's build algorithms to solve them! We will discuss several tree-based algorithms here. 

**The idea**:

<p align='center'>
    <img alt="picture 6" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/840f2201d46e7a7f8ee5a4281a7eb6e4efb87556b3d54973f8cedec107121806.png" width="300" />  
</p>

- We have some **tree** (model) representing the **search problem**. We want to find paths that lead to successful leaves with the minimum-cost. There are several algorithms that can do this. We can keep track of them via the following factors:
  - Algorithm name
  - Cost
  - Time
  - Space 


### Backtracking 

- 106B! 
- Brute forces all paths. Basically, just complete [depth-first traversal](https://stackoverflow.com/questions/687731/breadth-first-vs-depth-first) of the search tree and update minimum-cost path along the way. 
- We consider all possible actions $a$ (edges) from state $s$ (node), and recursively search each of the possibilities, updating the minimum-cost and path along the way. 
- To get an idea of complexity, we define the following:
  - **branching factor** $b$: number of available actions at each state[^1]. [^1]: can't this vary though depending on the depth that we are at?  
  - **maximum depth** $D$: each path consists of $D$ or less actions/edges. 
- **Complexity**:
  - **Space**: $O(D)$
  - **Time**: $O\left(b^D\right)$
- Why $O(D)$ for space?
  - Each path is defined by the sequence of actions you take at each node in the branch. This can be, at the worst, $D$ long. Example: 
  <p align='center'>
    <img alt="picture 8" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/4b93fcef8293f4e07055ea28da74515fb119b0a5f4b06d42390b35b23de4111e.png" width="200" />  
  </p>
- Time also makes sense. We need to search, at worst, $D$ things for each total branch. But the branching factor $b$ (a scaling factor), exponentially grows this. 

Algorithm: 

<p align='center'>
    <img alt="picture 7" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/be8a914ff8fd7fe53cb9c47e0af34444864693f66a14c3485a4eeb467601f070.png" width="450" />  
</p>


### Depth-first search 

Before we go on, let's do a quick recap: 

- State-based modeling allows us to model and solve problems where the output is a sequence of actions to take between states. 
- We formalized a search problem via notations such as states and actions. 
- We can model a search problem via a tree. 
- We can solve a search problem via a traversal algorithm.
- There exists several such algorithms, each differing in performance slightly (some suited for tasks better than others). 

Onto DFS: 

- Ok... suppose we don't care about finding the *minimum*-cost path, but rather simply the first solution path. That is, make the cost $c = 0$. 
- Then, DFS works the best here. Just traverse each branch in a depth-fashion. Stop when you reach the first leaf. 
- Time and space complexity is still the same, because, in the worst case, we traverse the whole tree and don't end up with a solution. 

### Breadth-first search 

- Now suppose all costs of each action are constant. Every action costs $c$. 
- Then, do BFS. 
- We traverse the tree laterally.
- When we reach a solution, we know we are at the best one. 
  - Think about it. If we traverse from the left to the right and find a successful leaf node (successful end state), there can be no branch that is shorter in depth. 

<p align='center'>
    <img alt="picture 9" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/0741e8715aecd4f01f36474ef42fb7ba958bacc80b5aeaf9ae768a534902f3ad.png" width="490" />  
</p>

## IV. Dynamic Programming 

- Time-complexity was pretty bad for the tree traversal algorithms for solving search problems. Lots of it is simply too brute-force to be reasonable (at worst case). 
- Time-complexity was **growing exponentially** because we'd have to recurse through the whole maze. 
- Let's see how DP can speed this up. 
  
Here is the general idea: 


 
::: {#exm-default}
take the following graph:  
<p align='center'>
    <img alt="picture 11" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/ef9b19a1dc4cac31d0c2e3f2a560b28824b0b25b2aa3f48cc8aa5c0d300100fd.png" width="700" />  
</p>
in tree search (take backtracking), we'd have to recurse through each branch, for every branch. Exponential time complexity! However, notice that, for example, once we hit a $5$, the rest of the branch is the same. This is the key idea. We can store the subproblem path of starting from $5$ to the solution (leaf) in a lookup table and next time we are at a $5$, we just look it up! We save tons of time but sacrifice a bit of space while were at it (which is ok!). This concept is called **memoization**. 
:::

This slide formalizes the concept of dynamic programming. 

 
:::{.callout-note}
::: {#def-default}
(**Dynamic programming**) 
<p align='center'>
    <img alt="picture 12" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/338c7106f8492df7ebaf341d0f3b6f9796e3209c785d8894c8bd5fd1fdba689b.png" width="600" />  
</p>
:::
:::
 
In code, this would be something like: 

 
```python
if table[subproblem_solution] is not None:
    return table[subproblem_solution]
else:
    recurse as usual 
    ...
    base case 

table[subproblem_solution] = solution 
return solution 
```
 