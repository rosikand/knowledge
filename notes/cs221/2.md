# Week 1, Lecture 2 - Machine Learning Introduction  

`Date: 9/28/22` 

## Module: Overview 

We now begin our journey into ML. Next 3 lectures. Reflex-based models to logic. 

### Reflex-based models

- **Reflex-based models**: a predictor $f$ takes some input $x$ and produces some output $y$. 
  - Input can usually be anything but output is usually restricted (output defines the task). 

<p align='center'>
    <img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/8f28e0641834c9a710da834d765664f93896856acafec0c44b71edda2cbfa6b1.png" width="300" />  
</p>

First examples: 

- **Binary classifier**: restrict $y$: $x \longrightarrow f \rightarrow y \in\{+1,-1\} \text { label }$. 

<p align='center'>
    <img alt="picture 2" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/c11c0df8a65845181572c1f5cc2b7704301c460cdd59e916523d73d8709654f5.png" width="300" />  
</p>

- **Regression**: $x \longrightarrow f \rightarrow y \in \mathbb{R} \text { response }$. 
  - Arbitrary input with output restricted to some real number. 
  - Infinite continuous (whereas classification is finite discrete) 
  - Output is called the **response**: $y \in \mathbb{R}$. 

<p align='center'>
    <img alt="picture 4" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/32e37143b9c347739ecf229ad9493aeace4a380128815f77b0b9065e884f590a.png" width="300" />  
</p>

- **Structured prediction**: catch-all generalization for non-categoricalizable machine learning problems. 
  - e.g., *image segmentation*. Most tasks reduce to series of multi-class categorization problems. For example, image segmentation reduces to $m \times n$ multi-class (or binary) classification problems where $m, n$ are the dimensions of the image in pixels. 

<p align='center'>
    <img alt="picture 3" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/37c9a1710b3a460ebdd850310a3accd04c45a4803bd0eab73007ee53a6ed06d6.png" width="300" />  
</p>

### Roadmap 

- **Tasks**:
  - classification 
  - regression 
- **Algorithms**:
  - SGD
  - backpropogation 
- **Models**:
  - non-linear features
  - feature templates
  - neural networks
  - differentiable programming
- **Misc. considerations**:
  - Group DRO (unbiasing objectives)
  - Generalization 

## Module: linear regression 

**History**: Ceres (asteroid) went behind the sun and scientists wanted to know when it would be observed again. Using data points gathered before it went behind the sun, Gauss used linear algebra to invent least squares linear regression to solve this task. 

**General framework**: 

<p align='center'>
    <img alt="picture 5" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/13980a77c09d5b32de8247244e965853018b722f8cffdc0d56561d6f38fc6ecf.png" width="500" />  
</p>

- **Hypothesis** class: which predictors (i.e. models) are possible?
    - Linear, quadratic, neural network, etc. These all belong to the hypothesis class of the process.
- How **good** is predictor: loss function
- How to compute best fit curve: **optimization** *algorithm*
    - Though looping through all possible predictors is computationally impossible so we use optimization to numerically approximate best fit curve. 
[^1]

[^1]: *Question*: when we say "learning" algorithm, what are we referring to? The whole framework? Backprop? 


### Modeling 
 
:::{.callout-note}
::: {#def-default}

**Hypothesis class**: When we say hypothesis class, we mean the *range* of predictors that we can learn. For example, linear classifiers are of the form $f(x)=w_1+w_2 x$ for some $w_1$ and $w_2$: 

<p align='center'>
    <img alt="picture 7" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/3ec8f8f0b52d958ff1ff565dfb7f4e06d0fa8183f3efc035549c132f67497c66.png" width="300" />  
</p>

:::
:::


- This is cool and all, but if we want to add complexity, we should keep our notation simple. So here we will introduce the above with **vector notation**. 


:::{.callout-note}
::: {#def-default}
**Vector notation for linear regression**: 

- **Weight vector**: 
$$ 
\mathbf{w}=\left[w_1, w_2\right] 
$$

- **Feature extractor**: 

$$
\phi(x)=[1, x]
$$

Takes in input vector $x$ and returns transformed version of $x$ to be fed into the model. We do this such that the input can be of the proper shape for the matrix multiplication that is to come or just oe extract general features that might lead to better performance. 
- **Prediction (score)**: 

$$
f_{\mathbf{w}}(x)=\mathbf{w} \cdot \phi(x)
$$

:::
:::
 

Example: $f_{\mathbf{w}}(3)=[1,0.57] \cdot[1,3]=2.71$. Hypothesis class: $\mathcal{F}=\left\{f_{\mathbf{w}}: \mathbf{w} \in \mathbb{R}^2\right\}$. i.e., a set of weight real-valued functions. 

Next let's move onto the **loss function**.

### Loss function

- How good is a predictor? Here we "define" good. 
- Example: **least residual squares**: 

<p align='center'>
    <img alt="picture 8" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/2885d16c7368d927320cc967ae65fcca62a5f079e1732176407a4d9370c11226.png" width="300" />  
</p>

$$\operatorname{Loss}(x, y, \mathbf{w})=\left(f_{\mathbf{w}}(x)-y\right)^2$$


The above equation defines the loss for *one data point*. During training, we'd average all of the individual losses on each data point and define the **training loss**: 

$$\operatorname{TrainLoss}(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}}$$

### Optimization 

> How the heck do we find a good predictor? Optimization! Gradient descent algorithm. 

We want to $\min _{\mathbf{w}} \operatorname{TrainLoss}(\mathbf{w})$. 


:::{.callout-note}
::: {#def-default}
**Gradient**: The gradient $\nabla_{\mathbf{w}} \operatorname{TrainLoss}(\mathbf{w})$ is the direction that increases the training loss the most. 
:::
:::
 
 
:::{.callout-note}
::: {#def-default}
**Gradient descent**: 
- Initialize $\mathbf{w}=[0, \ldots, 0]$
- For $t=1, \ldots, T$ : epochs
  - $\mathbf{w} \leftarrow \mathbf{w}-\underbrace{\eta}_{\text {step size }} \underbrace{\nabla_{\mathbf{w}} \operatorname{TrainLoss}(\mathbf{w})}_{\text {gradient }}$. 
:::
:::

How does this relate back to lin. reg.? Well, really, we are just optimizing the **parameters** ($\mathbf{w}$) of the **linear regression model**. 

Ok the above is kind of abstract. But how do we actually compute the gradient for a full loss objective. 

Here is an example with the previous objective function... 

 
::: {#exm-default}
**Computing the gradient of objective functions**: 

Objective function: 

$$\operatorname{TrainLoss}(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}}(\mathbf{w} \cdot \phi(x)-y)^2$$

Gradient (use the chain rule!) with respect to $\mathbf{w}$: 

$$\nabla_{\mathbf{w}} \operatorname{TrainLoss}(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}} 2(\underbrace{\mathbf{w} \cdot \phi(x)-y}_{\text {prediction-target }}) \phi(x)$$

:::


A couple tips and points: 

- gradient of the objective function is usually with respect to the weight vector $\mathbf{w}$.[^2]
   
   [^2]: is this always the case though? 
- > *Percy*'s tip: for differentiating vector/matrix functions, just do the scalar case first and then see if they match up. In theory, they should because a vectorization is just a generalization of a scalar. Also [this video](https://www.youtube.com/watch?v=VQDtSZ5A0xM) from Princeton's COS 302 class should help with differentiating vector and matrix-valued functions. 

Also, this can be done algebraically and we can hand-calculate $\mathbf{w}$ using gradient descent. [See lecture slides](https://stanford-cs221.github.io/autumn2022/modules/module.html#include=machine-learning%2Flinear-regression.js&slideId=gradient-descent-example&level=0) for an example. 

 
:::{.callout-note collapse='true'}
**Gradient descent in code**: 

 
```python
import numpy as np

############################################################
# Optimization problem

trainExamples = [
    (1, 1),
    (2, 3),
    (4, 3),
]

def phi(x):
    return np.array([1, x])

def initialWeightVector():
    return np.zeros(2)

def trainLoss(w):
    return 1.0 / len(trainExamples) * sum((w.dot(phi(x)) - y)**2 for x, y in trainExamples)

def gradientTrainLoss(w):
    return 1.0 / len(trainExamples) * sum(2 * (w.dot(phi(x)) - y) * phi(x) for x, y in trainExamples)

############################################################
# Optimization algorithm

def gradientDescent(F, gradientF, initialWeightVector):
    w = initialWeightVector()
    eta = 0.1
    for t in range(500):
        value = F(w)
        gradient = gradientF(w)
        w = w - eta * gradient
        print(f'epoch {t}: w = {w}, F(w) = {value}, gradientF = {gradient}')

gradientDescent(trainLoss, gradientTrainLoss, initialWeightVector)
``` 

:::
 
### Summary 

<p align='center'>
    <img alt="picture 9" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/bde600a095faaaf285a48cba57fc133d8db577585dc6eb470a1aebeda94a31c8.png" width="550" />  
</p>


## Module: Classification 

- The setup and framework is essentially the same (model, loss, optimization), but the dimensionalities change. 
- Change is that we have a **linear separator** for the predictor which acts as a decision boundary. This is for binary problems. 
  
**Framework**: 

<p align='center'>
    <img alt="picture 10" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/9c3c85a5d6dfc57bb32752b7ae48862a34c905c6f9f1d6d376f04c54df4a3fe7.png" width="550" />  
</p>

- **Binary linear classifier**: $f_{\mathbf{w}}(x)=\operatorname{sign}(\mathbf{w} \cdot \phi(x))$. 
- **Hypothesis class**: $\mathcal{F}=\left\{f_{\mathbf{w}}: \mathbf{w} \in \mathbb{R}^2\right\}$. 
- **Loss**: 
  - *zero-one*: $\operatorname{Loss}_{0-1}(x, y, \mathbf{w})=\mathbf{1}\left[f_{\mathbf{w}}(x) \neq y\right]$
    - If correct classification, loss is 1. Else, loss is 0. 
  - *hinge*: fixing the problems with the zero-one loss. 

### Score and margin 

Detour. New concepts. 



 
:::{.callout-note}
::: {#def-default}
**Score**: 
- In regression, this is just the **prediction** (model output). 
- In classification, **confidence** in which we are predicting (+1):
  - The score on an example $(x, y)$ is $\mathbf{w} \cdot \phi(x)$, how confident we are in predicting $+1$. 
    - This essentially is the number before taking the sign in the prediction from the model. 
:::
:::


 
:::{.callout-note}
::: {#def-default}
**(Margin)**:

- The margin on an example $(x, y)$ is $(\mathbf{w} \cdot \phi(x)) y$, how correct we are. Score times $y$. 
- More: larger the margin, the more correct. If $y=1$, then score (raw prediction) needs to be very high to have good margin. Vise versa if $y=-1$. 
:::
:::

[^3]
 
[^3]: Question: I am still confused on the difference between score and margin. Is margin measures correctness, what is the difference between margin and loss then? 

> Note that these concepts help us delve deeper into the nuances of the classifer as opposed to just seeing whether the predictor is right or wrong. This will help us transform the zero-one loss into hinge. 

**Problems with zero-one loss**:

<p align='center'>
    <img alt="picture 12" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/903f88b55fce6930339f1882c3e0f7dbdda809d3a2b5133d8f82270a5d8fabb9.png" width="300" />  
</p> [^4]

[^4]: see how plotting the loss against the margin allows us to discover the following pitfalls? (motivation for introducing margin).  

1. non-differentiable 
2. flat curve means zero-gradients almost everywhere: thus, GD will get stuck! 

### Introducing hinge loss 

To resolve the above problems, we now introduce the *hinge loss*. 

 
:::{.callout-note}
::: {#def-default}
**(Hinge loss)**

- In words: the maximum over a descending line and the zero function. In other words, 
  - If the margin is at least 1, then the hinge loss is zero.
  - If the margin is less than 1, then the hinge loss rises linearly.
- In math: 

$$\operatorname{Loss}_{\text {hinge }}(x, y, \mathbf{w})=\max \{1-(\mathbf{w} \cdot \phi(x)) y, 0\}$$

- Now visually: 

<p align='center'>
    <img alt="picture 111" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/2b93e45e768a91ba8635188fd25077e95f59a69af933e9413f071041165ab690.png" width="300" />  
</p>

:::
:::
 
Some intuitions: 
- Hinge loss is upper bound of zero-one (see this visually). So if we drive down hinge loss, we must also be driving down zero-one. 
- Logistic is another common loss to use here. 

